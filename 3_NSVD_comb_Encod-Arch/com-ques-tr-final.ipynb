{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7618624,"sourceType":"datasetVersion","datasetId":4223935}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### for cleaning the CPU ram\nimport gc\ngc.collect()\n\n%reset -f","metadata":{"execution":{"iopub.execute_input":"2024-01-15T08:21:13.081056Z","iopub.status.busy":"2024-01-15T08:21:13.080540Z","iopub.status.idle":"2024-01-15T08:21:13.169248Z","shell.execute_reply":"2024-01-15T08:21:13.168532Z","shell.execute_reply.started":"2024-01-15T08:21:13.081031Z"}}},{"cell_type":"code","source":"#for cleaning the GPU ram\nimport torch \ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.224405Z","iopub.execute_input":"2024-02-13T15:59:16.224862Z","iopub.status.idle":"2024-02-13T15:59:16.301539Z","shell.execute_reply.started":"2024-02-13T15:59:16.224833Z","shell.execute_reply":"2024-02-13T15:59:16.300411Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"TOTAL_ITER = 1000\nVALID_EVE =100","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.303436Z","iopub.execute_input":"2024-02-13T15:59:16.303713Z","iopub.status.idle":"2024-02-13T15:59:16.317592Z","shell.execute_reply.started":"2024-02-13T15:59:16.303689Z","shell.execute_reply":"2024-02-13T15:59:16.316456Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport json\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport argparse#***","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.318943Z","iopub.execute_input":"2024-02-13T15:59:16.319243Z","iopub.status.idle":"2024-02-13T15:59:16.330286Z","shell.execute_reply.started":"2024-02-13T15:59:16.319219Z","shell.execute_reply":"2024-02-13T15:59:16.329119Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"def invertDict(_dict):\n    return {v: k for k, v in _dict.items()}","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.332391Z","iopub.execute_input":"2024-02-13T15:59:16.332772Z","iopub.status.idle":"2024-02-13T15:59:16.343339Z","shell.execute_reply.started":"2024-02-13T15:59:16.332736Z","shell.execute_reply":"2024-02-13T15:59:16.342368Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"class ClevrDialogDataset(Dataset):\n    def __init__(self, dataPath, vocabPath, split, indStart=0, indEnd=-1):\n        super(ClevrDialogDataset, self).__init__()\n        self.data = h5py.File(dataPath, \"r\")\n        with open(vocabPath, \"r\") as f:\n            self.vocab = json.load(f)\n        self.vocab[\"idx_text_to_token\"] = invertDict(self.vocab[\"text_token_to_idx\"])\n        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n        self.lenVocabText = len(self.vocab[\"text_token_to_idx\"])\n        self.lenVocabProg = len(self.vocab[\"prog_token_to_idx\"])\n\n        self.split = split\n        self.indStart = indStart\n        self.indEnd = indEnd\n        self.maxSamples = indEnd - indStart\n        self.maxLenProg = 6\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.344789Z","iopub.execute_input":"2024-02-13T15:59:16.345077Z","iopub.status.idle":"2024-02-13T15:59:16.360396Z","shell.execute_reply.started":"2024-02-13T15:59:16.345054Z","shell.execute_reply":"2024-02-13T15:59:16.359418Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"class ClevrDialogQuestionDataset(ClevrDialogDataset):\n    def __init__(self, dataPath, vocabPath, split, name, train=True, indStart=0, indEnd=-1):\n        super(ClevrDialogQuestionDataset, self).__init__(dataPath, vocabPath, split, indStart=indStart, indEnd=indEnd)\n        self.questions = torch.LongTensor(np.asarray(self.data[\"questions\"], dtype=np.int64)[indStart: indEnd])\n        self.quesProgs = torch.LongTensor(np.asarray(self.data[\"questionProgs\"], dtype=np.int64)[indStart: indEnd])\n        self.questionRounds = torch.LongTensor(np.asarray(self.data[\"questionRounds\"], dtype=np.int64)[indStart: indEnd])\n        self.questionImgIdx = torch.LongTensor(np.asarray(self.data[\"questionImgIdx\"], dtype=np.int64)[indStart: indEnd])\n        self.histories = torch.LongTensor(np.asarray(self.data[\"histories\"], dtype=np.int64)[indStart: indEnd])\n        self.historiesProgs = torch.LongTensor(np.asarray(self.data[\"historiesProg\"], dtype=np.int64)[indStart: indEnd])\n\n        self.answers = torch.LongTensor(np.asarray(self.data[\"answers\"], dtype=np.int64)[indStart: indEnd])\n        self.name = name\n        self.train = train\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        assert idx < len(self)\n        question = self.questions[idx]\n        questionPrg = self.quesProgs[idx]\n        questionImgIdx = self.questionImgIdx[idx]\n        questionRound = self.questionRounds[idx]\n\n        history = self.histories[idx]\n        historiesProg = self.historiesProgs[idx]\n\n        answer = self.answers[idx]\n        if self.train:\n            return question, history, questionPrg, questionRound, answer\n        else:\n            return question, questionPrg, questionImgIdx, questionRound, history, historiesProg, answer","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.361773Z","iopub.execute_input":"2024-02-13T15:59:16.362155Z","iopub.status.idle":"2024-02-13T15:59:16.378732Z","shell.execute_reply.started":"2024-02-13T15:59:16.362123Z","shell.execute_reply":"2024-02-13T15:59:16.377685Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"COLORS = [\"blue\", \"brown\", \"cyan\", \"gray\", \"green\", \"purple\", \"red\", \"yellow\"]\nMATERIALS = [\"rubber\", \"metal\"]\nSHAPES = [\"cube\", \"cylinder\", \"sphere\"]\nSIZES = [\"large\", \"small\"]\n\nATTRIBUTES_ALL = COLORS + MATERIALS + SHAPES + SIZES\n\nANSWER_CANDIDATES = {\n    # Count questions\n    \"count-all\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n    \"count-other\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n    \"count-all-group\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n    \"count-attribute\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n    \"count-attribure-group\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n    \"count-obj-rel-imm\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n    \"count-obj-rel-imm2\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n    \"count-obj-rel-early\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n    \"count-obj-exclude-imm\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n    \"count-obj-exclude-early\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n\n    # Existence questions\n    \"exist-other\": [\"yes\", \"no\"],\n    \"exist-attribute\": [\"yes\", \"no\"],\n    \"exist-attribute-group\": [\"yes\", \"no\"],\n    \"exist-obj-rel-imm\": [\"yes\", \"no\"],\n    \"exist-obj-rel-imm2\": [\"yes\", \"no\"],\n    \"exist-obj-rel-early\": [\"yes\", \"no\"],\n    \"exist-obj-exclude-imm\": [\"yes\", \"no\"],\n    \"exist-obj-exclude-early\": [\"yes\", \"no\"],\n\n    # Seek questions\n    \"seek-attr-imm\": ATTRIBUTES_ALL,\n    \"seek-attr-imm2\": ATTRIBUTES_ALL,\n    \"seek-attr-early\": ATTRIBUTES_ALL,\n    \"seek-attr-sim-early\": ATTRIBUTES_ALL,\n    \"seek-attr-rel-imm\": ATTRIBUTES_ALL,\n    \"seek-attr-rel-early\": ATTRIBUTES_ALL,\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.380019Z","iopub.execute_input":"2024-02-13T15:59:16.380360Z","iopub.status.idle":"2024-02-13T15:59:16.396639Z","shell.execute_reply.started":"2024-02-13T15:59:16.380323Z","shell.execute_reply":"2024-02-13T15:59:16.395722Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"import json\nimport numpy as np\n\n\ndef merge_captions_question_programs(path_cap, path_ques, caption_first=True):\n    with open(path_cap, \"r\"):\n        c_progs = path_cap.readlines()\n    with open(path_ques, \"r\"):\n        q_progs = path_ques.readlines()\n\n    all_merged_progs = []\n    i = 0\n    while i < len(q_progs):\n        cap_idx = i % 11 if caption_first else i % 10\n        start_idx_p = i + 1 if caption_first else i\n        end_idx_p = start_idx_p + 12 if caption_first else  start_idx_p + 11\n        temp = c_progs[cap_idx] + q_progs[start_idx_p, end_idx_p]\n        all_merged_progs.append(temp)\n        i = end_idx_p\n\n\ndef load_clevr_scenes(scenes_json):\n    with open(scenes_json) as f:\n        scenes_raw = json.load(f)\n    if type(scenes_raw) == dict:\n        scenes_raw = scenes_raw[\"scenes\"]\n\n    scenes = []\n    for s in scenes_raw:\n        table = []\n        for i, o in enumerate(s['objects']):\n            item = {}\n            item['id'] = '%d-%d' % (s['image_index'], i)\n            if '3d_coords' in o:\n                item['position'] = [np.dot(o['3d_coords'], s['directions']['right']),\n                                    np.dot(o['3d_coords'], s['directions']['front']),\n                                    o['3d_coords'][2]]\n            else:\n                item['position'] = o['position']\n            item['color'] = o['color']\n            item['material'] = o['material']\n            item['shape'] = o['shape']\n            item['size'] = o['size']\n            table.append(item)\n        scenes.append(table)\n    return scenes\n\n\ndef load_minecraft_scenes(scenes_json):\n    with open(scenes_json) as f:\n        scenes_raw = json.load(f)\n    if type(scenes_raw) == dict:\n        scenes_raw = scenes_raw[\"scenes\"]\n\n    scenes = []\n    for s in scenes_raw:\n        table = []\n        for i, o in enumerate(s['objects']):\n            item = {}\n            item['id'] = '%d-%d' % (s['image_index'], i)\n            if '3d_coords' in o:\n                item['position'] = [np.dot(o['3d_coords'], s['directions']['right']),\n                                    np.dot(o['3d_coords'], s['directions']['front']),\n                                    o['3d_coords'][2]]\n            else:\n                item['position'] = o['position']\n            item['nature'] = o['nature']\n            item['class'] = o['class']\n            item['direction'] = \"facing_\"\n            if o['direction'] == \"front\":\n                item['direction'] += \"forward\"\n            elif o['direction'] == \"back\":\n                item['direction'] += \"backward\"\n            elif o['direction'] == \"right\":\n                item['direction'] += \"right\"\n            elif o['direction'] == \"left\":\n                item['direction'] += \"left\"\n            table.append(item)\n        scenes.append(table)\n    return scenes","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.397911Z","iopub.execute_input":"2024-02-13T15:59:16.398246Z","iopub.status.idle":"2024-02-13T15:59:16.415755Z","shell.execute_reply.started":"2024-02-13T15:59:16.398221Z","shell.execute_reply":"2024-02-13T15:59:16.414766Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom copy import deepcopy\n\n\n#from executor.clevr_statics import COLORS, MATERIALS, SHAPES, SIZES\n#from executor.clevr_statics import ANSWER_CANDIDATES as ANSWER_CANDIDATES_CLEVR\n#from executor.clevr_statics import ATTRIBUTES_ALL as ATTRIBUTES_ALL_CLEVR\n\n#from utils_m import load_clevr_scenes\n\n\nclass SymbolicExecutorClevr(object):\n    \"\"\"Symbolic executor for clevr-dialog\n    \"\"\"\n    def __init__(self, scenesPath):\n        super(SymbolicExecutorClevr, self).__init__()\n        self.functions = {}\n        self.registerFunctions()\n        self.uniqueObjFlag = False\n        self.colors = COLORS\n        self.materials = MATERIALS\n        self.shapes = SHAPES\n        self.sizes = SIZES\n        self.answer_candidates = ANSWER_CANDIDATES#***\n        self.attribute_all = ATTRIBUTES_ALL#***\n        self.scenes = load_clevr_scenes(scenesPath)\n\n    def reset(self, sceneIdx):\n        \"\"\"Resets the scene\n\n        Args:\n            sceneIdx: The index of the new scene\n        \"\"\"\n        self.scene = self.scenes[sceneIdx]\n        for _obj in self.scene:\n            _obj[\"identifier\"] = None\n        # store previous objects in a list to better answer\n        # xxx-imm, xxx-imm2, xxx-group and xxx-early questions.\n        self.objs = []\n        self.groups = []\n        self.visited = []\n        self.currentObj = None\n        self.currentGrp = []\n        self.uniqueObjFlag = False\n\n    def registerFunctions(self):\n        \"\"\"Registers the available functions of the executor.\n        \"\"\"\n        # Captions - extreme location\n        self.functions[\"extreme-right\"] = self.extremeRight\n        self.functions[\"extreme-left\"] = self.extremeLeft\n        self.functions[\"extreme-behind\"] = self.extremeBehind\n        self.functions[\"extreme-front\"] = self.extremeFront\n        self.functions[\"extreme-center\"] = self.extremeCenter\n\n        # Captions - multiple objects\n        self.functions[\"count-att\"] = self.countAttributeCaption\n\n        # Captions - object relations\n        self.functions[\"obj-relation\"] = self.objRelation\n\n        # Captions - unique object\n        self.functions[\"unique-obj\"] = self.uniqueObject\n\n        # Questions - Count\n        self.functions[\"count-all\"] = self.countAll\n        self.functions[\"count-other\"] = self.countOther\n        self.functions[\"count-all-group\"] = self.countAllGroup\n        self.functions[\"count-attribute\"] = self.countAttribute\n        self.functions[\"count-attribute-group\"] = self.countAttributeGroup\n        self.functions[\"count-obj-rel-imm\"] = self.countObjRelImm\n        self.functions[\"count-obj-rel-imm2\"] = self.countObjRelImm2\n        self.functions[\"count-obj-rel-early\"] = self.countObjRelEarly\n        self.functions[\"count-obj-exclude-imm\"] = self.countObjExcludeImm\n        self.functions[\"count-obj-exclude-early\"] = self.countObjExcludeEarly\n\n        # Questions - Exist\n        self.functions[\"exist-other\"] = self.existOther\n        self.functions[\"exist-attribute\"] = self.existAttribute\n        self.functions[\"exist-attribute-group\"] = self.existAttributeGroup\n        self.functions[\"exist-obj-rel-imm\"] = self.existObjRelImm\n        self.functions[\"exist-obj-rel-imm2\"] = self.existObjRelImm\n        self.functions[\"exist-obj-rel-early\"] = self.existObjRelEarly\n        self.functions[\"exist-obj-exclude-imm\"] = self.existObjExcludeImm\n        self.functions[\"exist-obj-exclude-early\"] = self.existObjExcludeEarly\n\n        # Questions - Seek\n        self.functions[\"seek-attr-imm\"] = self.seekAttrImm\n        self.functions[\"seek-attr-imm2\"] = self.seekAttrImm\n        self.functions[\"seek-attr-early\"] = self.seekAttributeEarly\n        self.functions[\"seek-attr-rel-imm\"] = self.seekAttributeRelImm\n        self.functions[\"seek-attr-rel-early\"] = self.seekAttributeRelEarly\n\n\n    ########################################################\n    #                   Helper functions                   #\n    ########################################################\n    def getAttributeType(self, attribute):\n        assert attribute in self.attribute_all, \"The attribute {} is unkown\".format(\n            attribute)\n        if attribute in self.colors:\n            return \"color\"\n        elif attribute in self.materials:\n            return \"material\"\n        elif attribute in self.shapes:\n            return \"shape\"\n        elif attribute in self.sizes:\n            return \"size\"\n\n    def execute(self, functionLabel, functionArgs):\n        assert functionLabel in self.functions, \"{} is not a valid function\".format(\n            functionLabel)\n        function = self.functions[functionLabel]\n        answer = function(*functionArgs)\n        return answer\n\n    def updateCurrentObj(self, obj):\n        self.currentObj = obj\n        objsCopy = deepcopy(self.objs)\n        for i, _obj in enumerate(objsCopy):\n            if _obj[\"id\"] == obj[\"id\"]:\n                del self.objs[i]\n        # Current obj is always kept at the end of the visited objs\n        self.objs.append(obj)\n\n    def updateVisited(self, obj):\n        if len(self.visited) == 0:\n            self.visited.append(obj)\n        else:\n            newObjFlag = True\n            for _obj in self.visited:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    newObjFlag = False\n                    break\n            if newObjFlag:\n                self.visited.append(obj)\n\n    def getOther(self):\n        others = []\n        if len(self.visited) < len(self.scene):\n            for _obj in self.scene:\n                notExisting = True\n                for __obj in self.visited:\n                    if __obj[\"id\"] == _obj[\"id\"]:\n                        notExisting = False\n                        break\n                if notExisting:\n                    others.append(_obj)\n        return others\n\n    def updateIdentifier(self, obj, attribute):\n        if obj[\"identifier\"] is None:\n            obj[\"identifier\"] = attribute\n        else:\n            identifiers = obj[\"identifier\"].split(\"-\")\n            if attribute not in identifiers:\n                identifiers.append(attribute)\n                obj[\"identifier\"] = \"-\".join(identifiers)\n\n\n    ########################################################\n    #                   Caption programs                   #\n    ########################################################\n\n    def extremeRight(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n\n        leftToRight = deepcopy(self.scene)\n        leftToRight.sort(key=lambda o: o[\"position\"][0])\n        extremeRightObj = leftToRight[-1]\n        for attributeType, attribute in zip(attributeTypes, attributes):\n            assert extremeRightObj[attributeType] == attribute\n            self.updateIdentifier(extremeRightObj, attribute)\n\n        self.updateCurrentObj(extremeRightObj)\n        self.updateVisited(extremeRightObj)\n        del leftToRight\n\n    def extremeLeft(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n\n        leftToRight = deepcopy(self.scene)\n        leftToRight.sort(key=lambda o: o[\"position\"][0])\n        extremeLeftObj = leftToRight[0]\n        for attributeType, attribute in zip(attributeTypes, attributes):\n            assert extremeLeftObj[attributeType] == attribute\n            self.updateIdentifier(extremeLeftObj, attribute)\n\n        self.updateCurrentObj(extremeLeftObj)\n        self.updateVisited(extremeLeftObj)\n        del leftToRight\n\n    def extremeFront(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n\n        backToFront = deepcopy(self.scene)\n        backToFront.sort(key=lambda o: o[\"position\"][1])\n        extremeFrontObj = backToFront[-1]\n        for attributeType, attribute in zip(attributeTypes, attributes):\n            assert extremeFrontObj[attributeType] == attribute\n            self.updateIdentifier(extremeFrontObj, attribute)\n\n        self.updateCurrentObj(extremeFrontObj)\n        self.updateVisited(extremeFrontObj)\n        del backToFront\n\n    def extremeBehind(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n\n        backToFront = deepcopy(self.scene)\n        backToFront.sort(key=lambda o: o[\"position\"][1])\n        extremeBehindObj = backToFront[0]\n        for attributeType, attribute in zip(attributeTypes, attributes):\n            assert extremeBehindObj[attributeType] == attribute\n            self.updateIdentifier(extremeBehindObj, attribute)\n\n        self.updateCurrentObj(extremeBehindObj)\n        self.updateVisited(extremeBehindObj)\n        del backToFront\n\n    def extremeCenter(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n        numObjs = len(self.scene)\n\n        frontToBack = deepcopy(self.scene)\n        frontToBack.sort(key=lambda o: o[\"position\"][1], reverse=True)\n\n        rightToLeft = deepcopy(self.scene)\n        rightToLeft.sort(key=lambda o: o[\"position\"][0], reverse=True)\n\n        prelimenaryCandidates = []\n\n        for i, objFrontToBack in enumerate(frontToBack):\n            numObjsInFront = i\n            numObjsBehind = len(rightToLeft) - i - 1\n            if numObjsInFront <= numObjs / 2 and numObjsBehind <= numObjs / 2:\n                prelimenaryCandidates.append(objFrontToBack)\n        foundCenter = False\n        for _obj in prelimenaryCandidates:\n            for i, objRightToLeft in enumerate(rightToLeft):\n                if _obj[\"id\"] == objRightToLeft[\"id\"]:\n                    numObjsToTheRight = i\n                    numObjsToTheLeft = len(frontToBack) - i - 1\n                    if numObjsToTheRight <= numObjs / 2 and numObjsToTheLeft <= numObjs / 2:\n                        foundCenter = True\n                        for attributeType, attribute in zip(attributeTypes, attributes):\n                            if _obj[attributeType] != attribute:\n                                foundCenter = False\n                                break\n                        break\n            if foundCenter:\n                break\n        for attributeType, attribute in zip(attributeTypes, attributes):\n            self.updateIdentifier(_obj, attribute)\n        self.updateCurrentObj(_obj)\n        self.updateVisited(_obj)\n        del rightToLeft, frontToBack\n\n    def countAttributeCaption(self, attribute):\n        attributeType = self.getAttributeType(attribute)\n        objs = []\n        for _obj in self.scene:\n            if _obj[attributeType] == attribute:\n                objs.append(deepcopy(_obj))\n        for _obj in objs:\n            self.updateIdentifier(_obj, attribute)\n        # update the current group\n        self.currentGrp = objs\n\n        # update the visited objects list\n        for _obj in objs:\n            self.updateVisited(_obj)\n\n    def getAnchorAttribute(self, attribute_1, attribute_2, scene):\n        # The anchor object is unique. If we filter the object list\n        # based on the attribute anchor, we must find only one object.\n        filterAttribute_1 = self.filterAttribute(scene, attribute_1)\n        if len(filterAttribute_1) == 1:\n            return attribute_1\n        else:\n            return attribute_2\n\n    def objRelation(self, attribute, attributeAnchor, relation):\n        assert relation in [\"left\", \"right\", \"front\", \"behind\"]\n        # find the anchor object\n        if attributeAnchor != self.getAnchorAttribute(attribute, attributeAnchor, self.scene):\n            temp = deepcopy(attribute)\n            attribute = deepcopy(attributeAnchor)\n            attributeAnchor = temp\n            if relation == \"left\":\n                relation = \"right\"\n            elif relation == \"right\":\n                relation = \"left\"\n            elif relation == \"behind\":\n                relation = \"front\"\n            elif relation == \"front\":\n                relation = \"behind\"\n\n        # Order the objects in the scene w.r.t. the relation\n        sceneCopy = deepcopy(self.scene)\n\n        if relation in [\"left\", \"right\"]:\n            sceneCopy.sort(key=lambda o: o[\"position\"][0])\n        else:\n            sceneCopy.sort(key=lambda o: o[\"position\"][1])\n\n        # get the anchor object\n        attributeTypeAnchor = self.getAttributeType(attributeAnchor)\n        for i, _obj in enumerate(sceneCopy):\n            if _obj[attributeTypeAnchor] == attributeAnchor:\n                break\n        # save the anchor object before the main object\n        anchorObj = _obj\n        self.updateIdentifier(anchorObj, attributeAnchor)\n        self.updateCurrentObj(anchorObj)\n        self.updateVisited(anchorObj)\n\n        if relation in [\"left\", \"behind\"]:\n            sceneCopy = list(reversed(sceneCopy[:i]))\n        else:\n            sceneCopy = sceneCopy[i+1:]\n\n        attributeType = self.getAttributeType(attribute)\n        # get the main object\n        for _obj in sceneCopy:\n            # and not equalDicts(_obj, anchorObj):\n            if _obj[attributeType] == attribute:\n                break\n        self.updateIdentifier(_obj, attribute)\n        self.updateCurrentObj(_obj)\n        self.updateVisited(_obj)\n        del sceneCopy\n\n    def uniqueObject(self, *attributes):\n        attributes = list(attributes)\n        attributeTypes = list(\n            map(lambda att: self.getAttributeType(att), attributes))\n\n        for _obj in self.scene:\n            found = True\n            for attributeType, attribute in zip(attributeTypes, attributes):\n                if _obj[attributeType] != attribute:\n                    found = False\n                    break\n\n            if found:\n                break\n        for att in attributes:\n            self.updateIdentifier(_obj, att)\n\n        self.updateCurrentObj(_obj)\n        self.updateVisited(_obj)\n\n    ######################################## Question Programs ########################################\n    def filterOutObj(self, scene, obj):\n        sceneCopy = deepcopy(scene)\n        for i, _obj in enumerate(scene):\n            if obj[\"id\"] == _obj[\"id\"]:\n                break\n        del sceneCopy[i]\n        return sceneCopy\n\n    def filterAttribute(self, scene, attribute):\n        attributeType = self.getAttributeType(attribute)\n        filtered = []\n        if len(scene) == 0:\n            return filtered\n\n        for _obj in scene:\n            if _obj[attributeType] == attribute:\n                filtered.append(_obj)\n        return filtered\n\n    def excludeAttribute(self, scene, obj, attributeType):\n        filtered = []\n        if len(scene) == 0:\n            return filtered\n        for _obj in scene:\n            if _obj[\"id\"] != obj[\"id\"] and obj[attributeType] == _obj[attributeType]:\n                filtered.append(_obj)\n\n        # Update the visited objects list\n        if len(filtered) > 0:\n            for _obj in filtered:\n                self.updateVisited(_obj)\n        return filtered\n\n    def filterLeft(self, scene, obj):\n        filtered = []\n        if len(scene) == 0:\n            return filtered\n\n        for _obj in self.scene:\n            # if the x-coordinate of _obj is smaller than the x-coordinate of slef.currentObj,\n            # then _obj is located to the left of self.currentObj\n            if _obj[\"position\"][0] < obj[\"position\"][0] and _obj[\"id\"] != obj[\"id\"]:\n                filtered.append(_obj)\n        return filtered\n\n    def filterRight(self, scene, obj):\n        filtered = []\n        for _obj in self.scene:\n            # if the x-coordinate of _obj is bigger than the x-coordinate of slef.currentObj,\n            # then _obj is located to the right of self.currentObj\n            if _obj[\"position\"][0] > obj[\"position\"][0] and _obj[\"id\"] != obj[\"id\"]:\n                filtered.append(_obj)\n        return filtered\n\n    def filterFront(self, scene, obj):\n        filtered = []\n        if len(scene) == 0:\n            return filtered\n\n        for _obj in self.scene:\n            # if the y-coordinate of _obj is smaller than the y-coordinate of slef.currentObj,\n            # then _obj is located in front of self.currentObj\n            if _obj[\"position\"][1] > obj[\"position\"][1] and _obj[\"id\"] != obj[\"id\"]:\n                filtered.append(_obj)\n        return filtered\n\n    def filterBehind(self, scene, obj):\n        # assert type(scene) == list, \"Excpected type list got {} instead\".format(type(scene))\n        filtered = []\n        if len(scene) == 0:\n            return filtered\n\n        for _obj in scene:\n            # if the y-coordinate of _obj is bigger than the y-coordinate of slef.currentObj,\n            # then _obj is located behind self.currentObj\n            if _obj[\"position\"][1] < obj[\"position\"][1] and _obj[\"id\"] != obj[\"id\"]:\n                filtered.append(_obj)\n        return filtered\n\n    def filterPosition(self, scene, obj, pos):\n        # assert type(scene) == list, \"Excpected type list got {} instead\".format(type(scene))\n        assert pos in [\"left\", \"right\", \"front\", \"behind\"]\n        if pos == \"left\":\n            filtered = self.filterLeft(scene, obj)\n        elif pos == \"right\":\n            filtered = self.filterRight(scene, obj)\n        elif pos == \"front\":\n            filtered = self.filterFront(scene, obj)\n        elif pos == \"behind\":\n            filtered = self.filterBehind(scene, obj)\n\n        return filtered\n\n    ###########################################################################\n    #                           Counting questions                            #\n    ###########################################################################\n    def countAll(self):\n        self.currentGrp = deepcopy(self.scene)\n        self.groups.append(deepcopy(self.scene))\n        return len(self.scene)\n\n    def countOther(self):\n        others = self.getOther()\n        if len(others) > 0:\n            self.currentGrp = others\n            self.groups.append(others)\n        if len(others) == 1:\n            obj = others[0]\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    break\n            self.updateCurrentObj(obj)\n\n            self.updateVisited(obj)\n        return len(others)\n\n    def countAllGroup(self):\n        return len(self.currentGrp)\n\n    def countAttribute(self, attribute, updateCurrentObj=True):\n        filtered = self.filterAttribute(self.scene, attribute)\n        if len(filtered) == 0:\n            return 0\n        # Update the visited objects list\n        for _obj in filtered:\n            self.updateVisited(_obj)\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            self.updateIdentifier(obj, attribute)\n            self.updateVisited(obj)\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n            else:\n                if new:\n                    self.objs.append(obj)\n\n        self.groups.append(filtered)\n        self.currentGrp = filtered\n        return len(filtered)\n\n    def countAttributeGroup(self, attribute, updateCurrentObj=True):\n        filtered = self.filterAttribute(self.currentGrp, attribute)\n        if len(filtered) == 0:\n            return 0\n        # Update the visited objects list\n        for _obj in filtered:\n            self.updateVisited(_obj)\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            self.updateIdentifier(obj, attribute)\n            self.updateVisited(obj)\n\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n            else:\n                if new:\n                    self.objs.append(obj)\n\n        self.groups.append(filtered)\n        self.currentGrp = filtered\n        return len(filtered)\n\n    def countObjRelImm(self, pos, updateCurrentObj=True):\n        filtered = self.filterPosition(self.scene, self.currentObj, pos)\n        if len(filtered) == 0:\n            return 0\n        # Update the visited objects list\n        for _obj in filtered:\n            self.updateVisited(_obj)\n\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n                self.uniqueObjFlag = True\n            else:\n                if new:\n                    self.objs.append(obj)\n        return len(filtered)\n\n    def countObjRelImm2(self, pos):\n        if self.uniqueObjFlag:\n            # del self.objs[-1]\n            self.updateCurrentObj(self.objs[-2])\n            self.uniqueObjFlag = False\n        return self.countObjRelImm(pos)\n\n    def countObjRelEarly(self, pos, earlyObjAttribute, updateCurrentObj=True):\n        for objEarly in reversed(self.objs):\n            if objEarly[\"identifier\"] is not None:\n                identifiers = objEarly[\"identifier\"].split(\"-\")\n                if earlyObjAttribute in identifiers:\n                    break\n            else:\n                continue\n        filtered = self.filterPosition(self.scene, objEarly, pos)\n        if len(filtered) == 0:\n            return 0\n        # Update the visited objects list\n        for _obj in filtered:\n            self.updateVisited(_obj)\n\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n            else:\n                if new:\n                    self.objs.append(obj)\n        else:\n            self.updateCurrentObj(objEarly)\n\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n        return len(filtered)\n\n    def countObjExcludeImm(self, attributeType, updateCurrentObj=True):\n        filtered = self.excludeAttribute(\n            self.scene, self.currentObj, attributeType)\n        if len(filtered) == 0:\n            return 0\n\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n            else:\n                if new:\n                    self.objs.append(obj)\n\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n        return len(filtered)\n\n    def countObjExcludeEarly(self, attributeType, earlyObjAttribute, updateCurrentObj=True):\n        for objEarly in reversed(self.objs):\n            if objEarly[\"identifier\"] is not None:\n                identifiers = objEarly[\"identifier\"].split(\"-\")\n                if earlyObjAttribute in identifiers:\n                    break\n            else:\n                continue\n\n        filtered = self.excludeAttribute(self.scene, objEarly, attributeType)\n        if len(filtered) == 0:\n            return 0\n\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj = _obj\n                    new = False\n                    break\n            if updateCurrentObj:\n                self.updateCurrentObj(obj)\n            else:\n                if new:\n                    self.objs.append(obj)\n        else:\n            self.updateCurrentObj(objEarly)\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n        return len(filtered)\n\n    ###########################################################################\n    #                           Existence questions                           #\n    ###########################################################################\n\n    def existOther(self):\n        others = self.getOther()\n        numOther = len(others)\n        if numOther > 0:\n            self.currentGrp = others\n            self.groups.append(others)\n            for _obj in others:\n                self.updateVisited(_obj)\n        return \"yes\" if numOther > 0 else \"no\"\n\n    def existAttribute(self, attribute):\n        filtered = self.filterAttribute(self.scene, attribute)\n        numAttribute = len(filtered)\n        if numAttribute == 0:\n            return \"no\"\n\n        # Update the visited objects list\n        for _obj in filtered:\n            self.updateVisited(_obj)\n        if len(filtered) == 1:\n            obj = filtered[0]\n            new = True\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    self.updateIdentifier(_obj, attribute)\n                    new = False\n                    break\n            if new:\n                self.updateIdentifier(obj, attribute)\n                self.objs.append(obj)\n                # self.updateCurrentObj(obj)\n\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n        return \"yes\"\n\n    def existAttributeGroup(self, attribute):\n        numAttributeGrp = self.countAttributeGroup(\n            attribute, updateCurrentObj=False)\n        return \"yes\" if numAttributeGrp > 0 else \"no\"\n\n    def existObjRelImm(self, pos):\n        numObjs = self.countObjRelImm(pos, updateCurrentObj=False)\n        return \"yes\" if numObjs > 0 else \"no\"\n\n    def existObjRelEarly(self, pos, earlyObjAttribute):\n        numObjs = self.countObjRelEarly(\n            pos, earlyObjAttribute, updateCurrentObj=False)\n        return \"yes\" if numObjs > 0 else \"no\"\n\n    def existObjExcludeImm(self, attributeType):\n        numObjs = self.countObjExcludeImm(\n            attributeType, updateCurrentObj=False)\n        return \"yes\" if numObjs > 0 else \"no\"\n\n    def existObjExcludeEarly(self, attributeType, earlyObjAttribute):\n        for objEarly in reversed(self.objs):\n            if objEarly[\"identifier\"] is not None:\n                identifiers = objEarly[\"identifier\"].split(\"-\")\n                if earlyObjAttribute in identifiers:\n                    break\n            else:\n                continue\n\n        filtered = self.excludeAttribute(self.scene, objEarly, attributeType)\n        numObjs = len(filtered)\n        if numObjs == 0:\n            return \"no\"\n        self.currentGrp = filtered\n        self.groups.append(filtered)\n        return \"yes\"\n\n    ###########################################################################\n    #                             Seek questions                              #\n    ###########################################################################\n\n    def seekAttrImm(self, attributeType):\n        assert attributeType in self.currentObj, \"Attributre <{}> is not valid\"\n        self.updateIdentifier(self.currentObj, self.currentObj[attributeType])\n        return self.currentObj[attributeType]\n\n    def seekAttributeEarly(self, attributeType, earlyObjAttribute):\n        for objEarly in reversed(self.objs):\n            if objEarly[\"identifier\"] is not None:\n                identifiers = objEarly[\"identifier\"].split(\"-\")\n                if earlyObjAttribute in identifiers:\n                    break\n            else:\n                continue\n        self.updateIdentifier(objEarly, objEarly[attributeType])\n        self.updateCurrentObj(objEarly)\n        self.updateVisited(objEarly)\n        return objEarly[attributeType]\n\n    def seekAttributeRelImm(self, attributeType, pos):\n        filtered = self.filterPosition(self.scene, self.currentObj, pos)\n        if len(filtered) == 0:\n            return \"none\"\n        else:\n            # Get the closest object to slef.obj\n            if pos == \"left\":\n                filtered.sort(key=lambda x: x[\"position\"][0])\n                obj = filtered[-1]\n            elif pos == \"right\":\n                filtered.sort(key=lambda x: x[\"position\"][0])\n                obj = filtered[0]\n            elif pos == \"front\":\n                filtered.sort(key=lambda x: x[\"position\"][1])\n                obj = filtered[0]\n            elif pos == \"behind\":\n                filtered.sort(key=lambda x: x[\"position\"][1])\n                obj = filtered[-1]\n\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj[\"identifier\"] = _obj[\"identifier\"]\n                    break\n            self.updateIdentifier(obj, obj[attributeType])\n            self.updateCurrentObj(obj)\n            self.updateVisited(obj)\n            return obj[attributeType]\n\n    def seekAttributeRelEarly(self, attributeType, pos, earlyObjAttribute):\n        for objEarly in reversed(self.objs):\n            if objEarly[\"identifier\"] is not None:\n                identifiers = objEarly[\"identifier\"].split(\"-\")\n                if earlyObjAttribute in identifiers:\n                    break\n            else:\n                continue\n\n        filtered = self.filterPosition(self.scene, objEarly, pos)\n        if len(filtered) == 0:\n            return \"none\"\n        else:\n            # Get the closest object to slef.obj\n            if pos == \"left\":\n                filtered.sort(key=lambda x: x[\"position\"][0])\n                obj = filtered[-1]\n            elif pos == \"right\":\n                filtered.sort(key=lambda x: x[\"position\"][0])\n                obj = filtered[0]\n            elif pos == \"front\":\n                filtered.sort(key=lambda x: x[\"position\"][1])\n                obj = filtered[0]\n            elif pos == \"behind\":\n                filtered.sort(key=lambda x: x[\"position\"][1])\n                obj = filtered[-1]\n            for _obj in self.objs:\n                if _obj[\"id\"] == obj[\"id\"]:\n                    obj[\"identifier\"] = _obj[\"identifier\"]\n                    break\n            self.updateIdentifier(obj, obj[attributeType])\n            self.updateCurrentObj(obj)\n            self.updateVisited(obj)\n            return obj[attributeType]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.439194Z","iopub.execute_input":"2024-02-13T15:59:16.439489Z","iopub.status.idle":"2024-02-13T15:59:16.562600Z","shell.execute_reply.started":"2024-02-13T15:59:16.439465Z","shell.execute_reply":"2024-02-13T15:59:16.561711Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\nimport torch\n#import utils_m","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.564393Z","iopub.execute_input":"2024-02-13T15:59:16.564696Z","iopub.status.idle":"2024-02-13T15:59:16.570607Z","shell.execute_reply.started":"2024-02-13T15:59:16.564670Z","shell.execute_reply":"2024-02-13T15:59:16.569668Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"#!touch \"pkl-small/quastion-train\" res.txt","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.571848Z","iopub.execute_input":"2024-02-13T15:59:16.572164Z","iopub.status.idle":"2024-02-13T15:59:16.580902Z","shell.execute_reply.started":"2024-02-13T15:59:16.572139Z","shell.execute_reply":"2024-02-13T15:59:16.579949Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"class OptionsC():#changed optiopn class as Option_c to differentiate it with the one belong to question\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument(\n            '--mode',\n            default=\"train\",\n            # required=True,\n            type=str,\n            choices=['train', 'test'],\n            help='The mode of the experiment')\n\n        self.parser.add_argument(\n            '--run_dir',\n            #default=\"pkl-small/quastion-train\",#for saving the results of the small dataset\n            default= \"/kaggle/working/\",\n            # required=True,\n            type=str,\n            help='The experiment directory')\n\n        self.parser.add_argument(\n            '--load_checkpoint_path',\n            default='None',#*** training mode\n            type=str,\n            help='The path the the pretrained CaptionNet')\n\n        self.parser.add_argument(\n            '--res_path',\n            default = \"/kaggle/working\",\n            # required=True,\n            type=str,\n            help='Path where to log the predicted caption programs')\n\n        self.parser.add_argument(\n            '--gpu_ids',\n            default='0',\n            type=str,\n            help='Id of the gpu to be used')\n\n        self.parser.add_argument(\n            '--seed',\n            default=42,\n            type=int,\n            help='The seed used in training')\n\n        self.parser.add_argument(\n            '--dataPathTr',\n            default = \"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_tr_half.h5\",\n            # required=True,\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed training data')\n\n        self.parser.add_argument(\n            '--dataPathVal',\n            default = \"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_val_half.h5\",\n            # required=True,\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed validation data')\n\n        self.parser.add_argument(\n            '--dataPathTest',\n            # required=True,\n            default = \"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_test_75000.h5\",\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed test data')\n\n        self.parser.add_argument(\n            '--vocabPath',\n            default = \"/kaggle/input/nsvd-dataset/caption/vocab_output_caption.json\",\n            # required=True,\n            type=str,\n            help='Path to the generated vocabulary')\n\n        self.parser.add_argument(\n            '--batch_size',\n            default=64,\n            type=int,\n            help='Batch size')\n\n        self.parser.add_argument(\n            '--num_workers',\n            default=0,\n            type=int,\n            help='Number of workers for loading')\n\n        self.parser.add_argument(\n            '--num_iters',\n            #default=5000,\n            default=TOTAL_ITER,\n            type=int,\n            help='Total number of iterations')\n\n        self.parser.add_argument(\n            '--display_every',\n            default=5,\n            type=int,\n            help='Display training information every N iterations')\n\n        self.parser.add_argument(\n            '--debug_every',\n            default=100,\n            type=int,\n            help='Display debug message every N iterations')\n\n        self.parser.add_argument(\n            '--validate_every',\n            default=VALID_EVE,\n            #default=200,\n            type=int,\n            help='Validate every N iterations')\n\n        self.parser.add_argument(\n            '--shuffle_data',\n            default=1,\n            type=int,\n            help='Activate to shuffle the training data')\n\n        self.parser.add_argument(\n            '--optim',\n            default='adam',\n            type=str,\n            help='The name of the optimizer to be used')\n\n        self.parser.add_argument(\n            '--lr',\n            default=1e-3,\n            type=float,\n            help='Base learning rate')\n\n        self.parser.add_argument(\n            '--betas',\n            default='0.9, 0.98',\n            type=str,\n            help='Adam optimizer\\'s betas')\n\n        self.parser.add_argument(\n            '--eps',\n            default='1e-9',\n            type=float,\n            help='Adam optimizer\\'s epsilon')\n\n        self.parser.add_argument(\n            '--lr_decay_marks',\n            default='50000, 55000',\n            type=str,\n            help='Learing rate decay marks')\n\n        self.parser.add_argument(\n            '--lr_decay_factor',\n            default=0.5,\n            type=float,\n            help='Learning rate decay factor')\n\n        self.parser.add_argument(\n            '--weight_decay',\n            default=1e-6,\n            type=float,\n            help='Weight decay')\n\n        self.parser.add_argument(\n            '--embedDim',\n            default=300,\n            type=int,\n            help='Embedding dimension')\n\n        self.parser.add_argument(\n            '--hiddenDim',\n            default=512,\n            type=int,\n            help='LSTM hidden dimension')\n\n        self.parser.add_argument(\n            '--numLayers',\n            #default=2,\n            default=1,#to sync with the ques train\n            type=int,\n            help='Number of hidden LSTM layers')\n\n        self.parser.add_argument(\n            '--dropout',\n            #default=0.1,\n            default=0.2, # to make it sync with ques_train\n            type=float,\n            help='Dropout value')\n\n        self.parser.add_argument(\n            '--multiHead',\n            default=8,\n            type=int,\n            help='Number of attention heads')\n\n        self.parser.add_argument(\n            '--hiddenSizeHead',\n            default=64,\n            type=int,\n            help='Dimension of each attention head')\n\n        self.parser.add_argument(\n            '--FeedForwardSize',\n            default=2048,\n            type=int,\n            help='Dimension of the feed forward layer')\n\n        self.parser.add_argument(\n            '--FlatMLPSize',\n            default=512,\n            type=int,\n            help='MLP flatten size')\n\n        self.parser.add_argument(\n            '--FlatGlimpses',\n            default=1,\n            type=int,\n            help='Number of flatten glimpses')\n\n        self.parser.add_argument(\n            '--FlatOutSize',\n            default=512,\n            type=int,\n            help='Final attention reduction dimension')\n\n        self.parser.add_argument(\n            '--layers',\n            #default=6,\n            default=4,#to sync with questrain\n            type=int,\n            help='Number of self attention layers')\n\n        self.parser.add_argument(\n            '--bidirectional',\n            default=1,\n            type=int,\n            help='Activate to use bidirectional LSTMs')\n\n        self.initialized = True\n\n    def parse(self):\n        # initialize parser\n        if not self.initialized:\n            self.initialize()\n       # self.opts = self.parser.parse_args()\n        self.opts, unknown = self.parser.parse_known_args()#this is added by me to fix the error of command line arguments.\n\n        # parse gpu id list\n        str_gpu_ids = self.opts.gpu_ids.split(',')\n        self.opts.gpu_ids = []\n        for str_id in str_gpu_ids:\n            if str_id.isdigit() and int(str_id) >= 0:\n                self.opts.gpu_ids.append(int(str_id))\n        if len(self.opts.gpu_ids) > 0 and torch.cuda.is_available():\n            print('\\n[INFO] Using {} CUDA device(s) ...'.format(len(self.opts.gpu_ids)))\n        else:\n            print('\\n[INFO] Using cpu ...')\n            self.opts.gpu_ids = []\n\n        # parse the optimizer's betas and lr decay marks\n        self.opts.betas = [float(beta) for beta in self.opts.betas.split(',')]\n        lr_decay_marks = [int(m) for m in self.opts.lr_decay_marks.split(',')]\n        for i in range(1, len(lr_decay_marks)):\n            assert lr_decay_marks[i] > lr_decay_marks[i-1]\n        self.opts.lr_decay_marks = lr_decay_marks\n\n        # print and save options\n        args = vars(self.opts)\n        print('\\n ' + 30*'-' + 'Opts' + 30*'-')\n        for k, v in args.items():\n            print('%s: %s' % (str(k), str(v)))\n\n        if not os.path.isdir(self.opts.run_dir):\n            os.makedirs(self.opts.run_dir)\n        filename = 'opts_c.txt'\n        file_path = os.path.join(self.opts.run_dir, filename)\n        with open(file_path, 'wt') as fout:\n            fout.write('| options\\n')\n            for k, v in sorted(args.items()):\n                fout.write('%s: %s\\n' % (str(k), str(v)))\n        return self.opts\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.582455Z","iopub.execute_input":"2024-02-13T15:59:16.582745Z","iopub.status.idle":"2024-02-13T15:59:16.615072Z","shell.execute_reply.started":"2024-02-13T15:59:16.582720Z","shell.execute_reply":"2024-02-13T15:59:16.614128Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"import torch\nimport math\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.618049Z","iopub.execute_input":"2024-02-13T15:59:16.618339Z","iopub.status.idle":"2024-02-13T15:59:16.631278Z","shell.execute_reply.started":"2024-02-13T15:59:16.618309Z","shell.execute_reply":"2024-02-13T15:59:16.630340Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"class FC(nn.Module):\n    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n        super(FC, self).__init__()\n        self.dropout_r = dropout_r\n        self.use_relu = use_relu\n\n        self.linear = nn.Linear(in_size, out_size)\n\n        if use_relu:\n            self.relu = nn.ReLU(inplace=True)\n\n        if dropout_r > 0:\n            self.dropout = nn.Dropout(dropout_r)\n\n    def forward(self, x):\n        x = self.linear(x)\n\n        if self.use_relu:\n            x = self.relu(x)\n\n        if self.dropout_r > 0:\n            x = self.dropout(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.632530Z","iopub.execute_input":"2024-02-13T15:59:16.633094Z","iopub.status.idle":"2024-02-13T15:59:16.644390Z","shell.execute_reply.started":"2024-02-13T15:59:16.633048Z","shell.execute_reply":"2024-02-13T15:59:16.643412Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n        super(MLP, self).__init__()\n\n        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n        self.linear = nn.Linear(mid_size, out_size)\n\n    def forward(self, x):\n        return self.linear(self.fc(x))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.645551Z","iopub.execute_input":"2024-02-13T15:59:16.645871Z","iopub.status.idle":"2024-02-13T15:59:16.654431Z","shell.execute_reply.started":"2024-02-13T15:59:16.645846Z","shell.execute_reply":"2024-02-13T15:59:16.653512Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, size, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.eps = eps\n\n        self.a_2 = nn.Parameter(torch.ones(size))\n        self.b_2 = nn.Parameter(torch.zeros(size))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.655716Z","iopub.execute_input":"2024-02-13T15:59:16.656388Z","iopub.status.idle":"2024-02-13T15:59:16.666277Z","shell.execute_reply.started":"2024-02-13T15:59:16.656355Z","shell.execute_reply":"2024-02-13T15:59:16.665330Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"class MHAtt(nn.Module):\n    def __init__(self, opts):\n        super(MHAtt, self).__init__()\n        self.opts = opts\n\n        self.linear_v = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_k = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_q = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_merge = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n\n        self.dropout = nn.Dropout(opts.dropout)\n\n    def forward(self, v, k, q, mask):\n        n_batches = q.size(0)\n\n        v = self.linear_v(v).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        k = self.linear_k(k).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        q = self.linear_q(q).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        atted = self.att(v, k, q, mask)\n        atted = atted.transpose(1, 2).contiguous().view(\n            n_batches,\n            -1,\n            self.opts.hiddenDim\n        )\n\n        atted = self.linear_merge(atted)\n\n        return atted\n\n    def att(self, value, key, query, mask):\n        d_k = query.size(-1)\n\n        scores = torch.matmul(\n            query, key.transpose(-2, -1)\n        ) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask, -1e9)\n\n        att_map = F.softmax(scores, dim=-1)\n        att_map = self.dropout(att_map)\n\n        return torch.matmul(att_map, value)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.667543Z","iopub.execute_input":"2024-02-13T15:59:16.667851Z","iopub.status.idle":"2024-02-13T15:59:16.683199Z","shell.execute_reply.started":"2024-02-13T15:59:16.667826Z","shell.execute_reply":"2024-02-13T15:59:16.682174Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, opts):\n        super(FFN, self).__init__()\n\n        self.mlp = MLP(\n            in_size=opts.hiddenDim,\n            mid_size=opts.FeedForwardSize,\n            out_size=opts.hiddenDim,\n            dropout_r=opts.dropout,\n            use_relu=True\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.684433Z","iopub.execute_input":"2024-02-13T15:59:16.684772Z","iopub.status.idle":"2024-02-13T15:59:16.698237Z","shell.execute_reply.started":"2024-02-13T15:59:16.684741Z","shell.execute_reply":"2024-02-13T15:59:16.697370Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"class SA(nn.Module):\n    def __init__(self, opts):\n        super(SA, self).__init__()\n        self.mhatt = MHAtt(opts)\n        self.ffn = FFN(opts)\n\n        self.dropout1 = nn.Dropout(opts.dropout)\n        self.norm1 = LayerNorm(opts.hiddenDim)\n\n        self.dropout2 = nn.Dropout(opts.dropout)\n        self.norm2 = LayerNorm(opts.hiddenDim)\n\n    def forward(self, x, x_mask):\n        x = self.norm1(x + self.dropout1(\n            self.mhatt(x, x, x, x_mask)\n        ))\n\n        x = self.norm2(x + self.dropout2(\n            self.ffn(x)\n        ))\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.699619Z","iopub.execute_input":"2024-02-13T15:59:16.699891Z","iopub.status.idle":"2024-02-13T15:59:16.710649Z","shell.execute_reply.started":"2024-02-13T15:59:16.699870Z","shell.execute_reply":"2024-02-13T15:59:16.709711Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"class AttFlat(nn.Module):\n    def __init__(self, opts):\n        super(AttFlat, self).__init__()\n        self.opts = opts\n\n        self.mlp = MLP(\n            in_size=opts.hiddenDim,\n            mid_size=opts.FlatMLPSize,\n            out_size=opts.FlatGlimpses,\n            dropout_r=opts.dropout,\n            use_relu=True\n        )\n        # FLAT_GLIMPSES = 1\n        self.linear_merge = nn.Linear(\n            opts.hiddenDim * opts.FlatGlimpses,\n            opts.FlatOutSize\n        )\n\n    def forward(self, x, x_mask):\n        att = self.mlp(x)\n        att = att.masked_fill(\n            x_mask.squeeze(1).squeeze(1).unsqueeze(2),\n            -1e9\n        )\n        att = F.softmax(att, dim=1)\n\n        att_list = []\n        for i in range(self.opts.FlatGlimpses):\n            att_list.append(\n                torch.sum(att[:, :, i: i + 1] * x, dim=1)\n            )\n\n        x_atted = torch.cat(att_list, dim=1)\n        x_atted = self.linear_merge(x_atted)\n\n        return x_atted","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.711837Z","iopub.execute_input":"2024-02-13T15:59:16.712138Z","iopub.status.idle":"2024-02-13T15:59:16.726507Z","shell.execute_reply.started":"2024-02-13T15:59:16.712109Z","shell.execute_reply":"2024-02-13T15:59:16.725645Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, opts, textVocabSize):\n        super(Encoder, self).__init__()\n        self.bidirectional = opts.bidirectional > 0\n\n        self.embedding = nn.Embedding(textVocabSize, opts.embedDim)\n        self.lstm = nn.LSTM(\n            input_size=opts.embedDim,\n            hidden_size=opts.hiddenDim,\n            num_layers=opts.numLayers,\n            bidirectional=self.bidirectional,\n            batch_first=True\n        )\n        if self.bidirectional:\n            opts.hiddenDim *= 2\n            opts.hiddenSizeHead *= 2\n            opts.FlatOutSize *= 2\n        self.attCurr = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n        self.attHist = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n        self.attFlat = AttFlat(opts)\n        self.fc_1 = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.fc_2 = nn.Linear(2 * opts.hiddenDim, opts.hiddenDim)\n\n\n    def forward(self, curr, hist=None):\n        \n        if hist is None:\n            hist = torch.zeros_like(curr)\n        curr_mask = self.make_mask(curr.unsqueeze(2))\n        hist_Mask = self.make_mask(hist.unsqueeze(2))\n            \n            \n        curr = self.embedding(curr)\n        hist = self.embedding(hist)\n            # Get the batch size and sequence length\n        batch_size, sequence_length, _ = curr.size()\n\n    # Initialize LSTM hidden states\n        h0 = torch.zeros(self.lstm.num_layers * (2 if self.bidirectional else 1), batch_size, self.lstm.hidden_size)\n        c0 = torch.zeros(self.lstm.num_layers * (2 if self.bidirectional else 1), batch_size, self.lstm.hidden_size)\n\n        if torch.cuda.is_available():\n            h0 = h0.cuda()\n            c0 = c0.cuda()\n\n        combined_input = torch.cat([curr, hist], dim=1)\n            # Apply LSTM layer\n        combined_input,(_, _) = self.lstm(combined_input, (h0, c0))\n       \n        currO = combined_input.detach().clone()\n        curr, hist = torch.split(combined_input, [curr.size(1), hist.size(1)], dim=1)\n\n            \n        for attC, attH in zip(self.attCurr, self.attHist):\n            curr = attC(curr, curr_mask)\n            hist = attH(hist, hist_Mask)#***\n        curr = self.attFlat(curr, curr_mask)\n\n        if(not(hist == None)):\n            attWeights = torch.sum(torch.mul(hist, curr.unsqueeze(1)), -1)\n            attWeights = torch.softmax(attWeights, -1)\n            hist = torch.sum(torch.mul(hist, attWeights.unsqueeze(2)), 1)\n            encOut = self.fc_2(torch.cat([curr, hist], -1))\n        else:\n            encOut = self.fc_1(curr)\n\n        return encOut, currO\n\n    # Masking\n    def make_mask(self, feature):\n        return (torch.sum(\n            torch.abs(feature),\n            dim=-1\n        ) == 0).unsqueeze(1).unsqueeze(2)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.727733Z","iopub.execute_input":"2024-02-13T15:59:16.727992Z","iopub.status.idle":"2024-02-13T15:59:16.746335Z","shell.execute_reply.started":"2024-02-13T15:59:16.727969Z","shell.execute_reply":"2024-02-13T15:59:16.745345Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"#*** ->for qiansu: copy and paste the whole class\nclass Decoder(nn.Module):\n    def __init__(self, opts, progVocabSize, maxLen, startID=1, endID=2):\n        super(Decoder, self).__init__()\n        self.numLayers = opts.numLayers\n        self.bidirectional = opts.bidirectional > 0\n        self.maxLen = maxLen\n        self.startID = startID\n        self.endID = endID\n\n        self.embedding = nn.Embedding(progVocabSize, opts.embedDim)\n        self.lstmProg = nn.LSTM(\n            input_size=opts.embedDim,\n            hidden_size=2*opts.hiddenDim if self.bidirectional else opts.hiddenDim,\n            num_layers=opts.numLayers,\n            batch_first=True,\n            #bidirectional=self.bidirectional,#???????\n        )\n        hiddenDim = opts.hiddenDim\n        if self.bidirectional:\n            hiddenDim *= 2\n\n        self.fcAtt = nn.Linear(2*hiddenDim, hiddenDim)\n        self.fcOut = nn.Linear(hiddenDim, progVocabSize)\n\n    def initPrgHidden(self, encOut):\n        hidden = [encOut for _ in range(self.numLayers)]\n        hidden = torch.stack(hidden, 0).contiguous()\n        return hidden, hidden\n\n    def forwardStep(self, prog, progH, questO):\n        #**********************************************our error relates to this prog cause in our case it is not acting as tensor anymore.\n        batchSize = prog.size(0)\n        inputDim = questO.size(1)\n        prog = self.embedding(prog)\n        outProg, progH = self.lstmProg(prog, progH)\n\n        att = torch.bmm(outProg, questO.transpose(1, 2))\n        att = F.softmax(att.view(-1, inputDim), 1).view(batchSize, -1, inputDim)\n        context = torch.bmm(att, questO)\n        # (batchSize, progLength, hiddenDim)\n        out = F.tanh(self.fcAtt(torch.cat([outProg, context], dim=-1)))\n\n        # (batchSize, progLength, progVocabSize)\n        out = self.fcOut(out)\n        predSoftmax = F.log_softmax(out, 2)\n        return predSoftmax, progH\n\n    def forward(self, prog, encOut, questO):\n        progH = self.initPrgHidden(encOut)\n        predSoftmax, progH = self.forwardStep(prog, progH, questO)\n\n        return predSoftmax, progH\n\n    def sample(self, encOut, questO):\n        batchSize = encOut.size(0)\n        cudaFlag = encOut.is_cuda\n        progH = self.initPrgHidden(encOut)\n        # prog = progCopy[:, 0:3]\n        prog = torch.LongTensor(batchSize, 1).fill_(self.startID)\n        # prog = torch.cat((progStart, progEnd), -1)\n        if cudaFlag:\n            prog = prog.cuda()\n        outputLogProbs = []\n        outputTokens = []\n     \n\n        def decode(i, output):\n            tokens = output.topk(1, dim=-1)[1].view(batchSize, -1)\n            #print(\"This is inside of the decode local function and this is the tockens=\", tokens)\n            return tokens\n\n        for i in range(self.maxLen):\n            predSoftmax, progH = self.forwardStep(prog, progH, questO)\n            prog = decode(i, predSoftmax)\n            prog_flat = list(chain(*prog))\n            flat_list = [item.item() for item in prog_flat]\n\n        #****************************************my modification\n            outputTokens.append(flat_list)#new\n       #print(\"lets check what is inside outputTocken\", outputTokens)    \n       # print(\"-----------------------------------------\")\n        return outputTokens, outputLogProbs\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.751726Z","iopub.execute_input":"2024-02-13T15:59:16.752013Z","iopub.status.idle":"2024-02-13T15:59:16.769938Z","shell.execute_reply.started":"2024-02-13T15:59:16.751988Z","shell.execute_reply":"2024-02-13T15:59:16.768799Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"class SeqToSeqC(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(SeqToSeqC, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, cap, prog):\n        encOut, capO = self.encoder(cap)\n        predSoftmax, progHC = self.decoder(prog, encOut, capO)\n        return predSoftmax, progHC\n   \n    def sample(self, cap):\n        with torch.no_grad():\n            encOut, capO = self.encoder(cap)\n        outputTokens, outputLogProbs = self.decoder.sample(encOut, capO)\n        #***************************************************************** Added by Sepi to avoid returning an empty torchlist.\n        #if not outputTokens:\n          #  print(\"***\")\n        # Handle the case where outputTokens is empty, for example, return a placeholder tensor\n           # return torch.tensor([])\n        #***************************************************************** \n        #outputTokens = torch.stack(outputTokens, 0).transpose(0, 1)\n        #outputTokens = torch.stack(outputTokens, dim=0).transpose(0, 1)\n        outputTokens_t = [[row[i] for row in outputTokens] for i in range(len(outputTokens[0]))]\n        return outputTokens_t\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.771257Z","iopub.execute_input":"2024-02-13T15:59:16.771613Z","iopub.status.idle":"2024-02-13T15:59:16.788239Z","shell.execute_reply.started":"2024-02-13T15:59:16.771582Z","shell.execute_reply":"2024-02-13T15:59:16.787351Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"class SeqToSeqQ(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(SeqToSeqQ, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, quest, hist, prog):\n        encOut, questO = self.encoder(quest, hist)\n        predSoftmax, progHC = self.decoder(prog, encOut, questO)\n        return predSoftmax, progHC\n\n    def sample(self, quest, hist):\n        with torch.no_grad():\n            encOut, questO = self.encoder(quest, hist)\n            outputTokens, outputLogProbs = self.decoder.sample(encOut, questO)\n            \n        #outputTokens = torch.stack(outputTokens, 0).transpose(0, 1)#*** comment this line\n        #print(\"lets check what is inside final outputTocken IN THE SeqToSeqQ:\", outputTokens) \n        outputTokens_t = [[row[i] for row in outputTokens] for i in range(len(outputTokens[0]))]#***transpose \n        return outputTokens_t","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.789466Z","iopub.execute_input":"2024-02-13T15:59:16.789736Z","iopub.status.idle":"2024-02-13T15:59:16.805366Z","shell.execute_reply.started":"2024-02-13T15:59:16.789712Z","shell.execute_reply":"2024-02-13T15:59:16.804249Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as Optim\nfrom itertools import chain #***","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.806704Z","iopub.execute_input":"2024-02-13T15:59:16.807049Z","iopub.status.idle":"2024-02-13T15:59:16.822927Z","shell.execute_reply.started":"2024-02-13T15:59:16.807017Z","shell.execute_reply":"2024-02-13T15:59:16.821964Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"class WarmupOptimizer(object):\n    def __init__(self, lr_base, optimizer, data_size, batch_size):\n        self.optimizer = optimizer\n        self._step = 0\n        self.lr_base = lr_base\n        self._rate = 0\n        self.data_size = data_size\n        self.batch_size = batch_size\n\n    def step(self):\n        self._step += 1\n\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n\n        self.optimizer.step()\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def rate(self, step=None):\n        if step is None:\n            step = self._step\n\n        if step <= int(self.data_size / self.batch_size * 1):\n            r = self.lr_base * 1/2.\n        else:\n            r = self.lr_base\n\n        return r\n\n\ndef get_optim(opts, model, data_size, lr_base=None):\n    if lr_base is None:\n        lr_base = opts.lr\n\n    if opts.optim == 'adam':\n        optim = Optim.Adam(\n                filter(lambda p: p.requires_grad, model.parameters()),\n                lr=0,\n                betas=opts.betas,\n                eps=opts.eps,\n\n            )\n    elif opts.optim == 'rmsprop':\n        optim = Optim.RMSprop(\n                filter(lambda p: p.requires_grad, model.parameters()),\n                lr=0,\n                eps=opts.eps,\n                weight_decay=opts.weight_decay\n            )\n    else:\n        raise ValueError('{} optimizer is not supported'.fromat(opts.optim))\n    return WarmupOptimizer(\n        lr_base,\n        optim,\n        data_size,\n        opts.batch_size\n    )\n\ndef adjust_lr(optim, decay_r):\n    optim.lr_base *= decay_r\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.824405Z","iopub.execute_input":"2024-02-13T15:59:16.824785Z","iopub.status.idle":"2024-02-13T15:59:16.839164Z","shell.execute_reply.started":"2024-02-13T15:59:16.824759Z","shell.execute_reply":"2024-02-13T15:59:16.837985Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\n#import utils_m\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.840713Z","iopub.execute_input":"2024-02-13T15:59:16.841343Z","iopub.status.idle":"2024-02-13T15:59:16.855479Z","shell.execute_reply.started":"2024-02-13T15:59:16.841273Z","shell.execute_reply":"2024-02-13T15:59:16.854512Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"class OptionsQ():#changed optiopn class as Option_q to differentiate it with the one belong to caption\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n        \n\n    def initialize(self):\n        self.parser.add_argument(\n            '--mode',\n            #required=True,\n            default='train',\n            type=str,\n            help='The mode of the experiment')\n        self.parser.add_argument(\n            '--type',\n            default= 'q',\n            help='The encoder type is caption encoder.'\n        )\n\n        self.parser.add_argument(\n            '--run_dir',\n            #default=\"pkl-small/quastion-train\",#for saving the results of the large dataset\n            #default=\"pkl-small/quastion-train\",#for saving the results of the small dataset\n            default = \"/kaggle/working\",\n            type=str,\n            help='The experiment directory')\n        #***\n        self.parser.add_argument(\n            '--useCuda',\n            default=1,\n            type=int,\n            help='hahahaha')\n        #***\n       \n\n        # self.parser.add_argument('--dataset', default='clevr', type=str, help='dataset')\n         #***\n        self.parser.add_argument(\n            '--text_log_dir',\n            #default=\"pkl-small/quastion-train\",#for saving the results of the large dataset\n            #default=\"pkl-small/quastion-train\",#for saving the results of the small dataset\n            default = \"/kaggle/working\",\n            type=str,\n            help='File to save the logged text')\n\n        self.parser.add_argument(\n            '--questionNetPath',\n            #default=\"/kaggle/working\",\n            default=\"None\",#fixed this it should have not been None.\n            type=str,\n            help='Path to the pretrained QuestionNet that will be used for testing.')\n\n        self.parser.add_argument(\n            '--captionNetPath',\n            #default=\"/kaggle/input/nsvd-dataset/ckpt_iter150.pkl\",#***just created with small caption dataset and added here.\n            default= 'None',\n            type=str,\n            help='Path to the pretrained CaptionNet that will be used for testing.')\n\n        self.parser.add_argument(\n            '--dialogLen',\n            default=10,\n            type=int,\n            help='Length of the dialogs to be used for testing. We used 10, 15, and 20 in our experiments.')\n\n        self.parser.add_argument(\n            '--last_n_rounds',\n            default=10,\n            type=int,\n            help='Number of the last rounds to consider in the history. We used 1, 2, 3, 4, and 10 in our experiments. ')\n\n        self.parser.add_argument(\n            '--encoderType',\n            #required=True,\n            default=1,#***-> this is for question-concat\n            #default= 2, #*** -> this one is for question-stack\n            type=int,\n            choices=[1, 2],\n            help='Type of the encoder: 1 --> Concat, 2 --> Stack')\n\n        self.parser.add_argument(\n            '--load_checkpoint_path',\n            default='None',\n            type=str,\n            help='Path to a QestionNet checkpoint path to resume training')\n\n        self.parser.add_argument(\n            '--gpu_ids',\n            default='0',\n            type=str,\n            help='Id of the gpu to be used')\n\n        self.parser.add_argument(\n            '--seed',\n            default=42,\n            type=int,\n            help='The seed used in training')\n\n        self.parser.add_argument(\n            '--dataPathTr',\n            #required=True,\n            #default=\"train_concat/output_h5.h5\",#***-> for the large dataset\n            #default=os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"concat_small/train_concat_half.h5\"),#***-> for the small dataset\n            default =\"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/train_concat_half.h5\",\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed training data')\n\n        self.parser.add_argument(\n            '--dataPathVal',\n            #required=True,\n            #default=\"val_concat/output_h5.h5\",#***-> for the large dataset\n            #default=os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"concat_small/val_concat_half.h5\"),\n            #default=\"/kaggle/input/nsvd-dataset/stack_small/val_stack_s.h5\",#***\n            default = \"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/val_concat_half.h5\",\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed validation data')\n\n        self.parser.add_argument(\n            '--dataPathTest',\n            #required=True,\n            #default=\"test_concat/output_h5.h5\",#***-> for the large dataset\n            # default=\"concat_small/test_conc_s.h5\",#***-> for the train-concat\n            #default = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"concat_small/test_conc_s.h5\"),\n            default = \"/kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/test_concat_75000.h5\",\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed test data')\n\n        self.parser.add_argument(\n            '--scenesPath',\n            #required=True,\n            # default=\"data/CLEVR_train_scenes.json\",#***in the training mode\n            #default= \"/kaggle/input/nsvd-dataset/data/CLEVR_scenes_test.json\",#***-> in the testing mode\n            #default=os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)),\n                                # \"data/CLEVR_train_scenes.json\"),#***in the training mode\n            default= \"/kaggle/input/nsvd-dataset/data/CLEVR_train_scenes.json\",\n            type=str,\n            help='Path to the derendered clevr-dialog scenes')\n\n        self.parser.add_argument(\n            '--vocabPath',\n            #required=True,\n            # default=\"train_concat/vocab_output.json\",#***-> for the train-concat\n            #default= \"/kaggle/input/nsvd-dataset/train_stack/vocab_output.json\",#***for the train-mode stack\n            #default= \"/kaggle/input/nsvd-dataset/test_stack/vocab_output.json\",#***for testing mode\n            #default = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"train_concat/vocab_output.json\"),\n            default = \"/kaggle/input/nsvd-dataset/train_concat/vocab_output.json\",\n            type=str,\n            help='Path to the generated vocabulary')\n\n        self.parser.add_argument(\n            '--batch_size',\n            default=64,\n            type=int,\n            help='Batch size')\n\n        self.parser.add_argument(\n            '--countFirstFailueRound',\n            default=0,\n            type=int,\n            help='If activated, we count the first failure round')\n\n        self.parser.add_argument(\n            '--maxSamples',\n            default=-1,\n            type=int,\n            help='Maximum number of training samples')\n\n        self.parser.add_argument(\n            '--num_workers',\n            default=0,\n            type=int,\n            help='Number of workers for loading')\n\n        self.parser.add_argument(\n            '--num_iters',\n            default=TOTAL_ITER,\n            type=int,\n            help='Total number of iterations')\n\n        self.parser.add_argument(\n            '--display_every',\n            default=5,\n            type=int,\n            help='Display training information every N iterations')\n\n        self.parser.add_argument(\n            '--validate_every',\n            default=VALID_EVE,\n            #default=200,\n            type=int,\n            help='Validate every N iterations')\n\n        self.parser.add_argument(\n            '--shuffle_data',\n            default=1,\n            type=int,\n            help='Activate to shuffle the training data')\n\n        self.parser.add_argument(\n            '--optim',\n            default='adam',\n            type=str,\n            help='The name of the optimizer to be used')\n\n        self.parser.add_argument(\n            '--lr',\n            #default=1e-3,\n            default=1e-4, # to reach 51\n            type=float,\n            help='Base learning rate')\n\n        self.parser.add_argument(\n            '--betas',\n            default='0.9, 0.98',\n            type=str,\n            help='Adam optimizer\\'s betas')\n\n        self.parser.add_argument(\n            '--eps',\n            default='1e-9',\n            type=float,\n            help='Adam optimizer\\'s epsilon')\n\n        self.parser.add_argument(\n            '--lr_decay_marks',\n            default='50000, 55000',\n            type=str,\n            help='Learing rate decay marks')\n\n        self.parser.add_argument(\n            '--lr_decay_factor',\n            default=0.5,\n            type=float,\n            help='Learning rate decay factor')\n\n        self.parser.add_argument(\n            '--weight_decay',\n            #default=1e-6,#original\n            default=1e-3,#for reaching more than 51.72 pr:1e-5\n            type=float,\n            help='Weight decay')\n\n        self.parser.add_argument(\n            '--embedDim',\n            default=300,\n            type=int,\n            help='Embedding dimension')\n\n        self.parser.add_argument(\n            '--hiddenDim',\n            default=512,\n            type=int,\n            help='LSTM hidden dimension')\n\n        self.parser.add_argument(\n            '--numLayers',\n           # default=2,\n            default=1,\n            type=int,\n            help='Number of hidden LSTM layers')\n\n        self.parser.add_argument(\n            '--dropout',\n            #default=0.1,\n            default=0.2,\n            type=float,\n            help='Dropout value')\n\n        self.parser.add_argument(\n            '--multiHead',\n            default=8,\n            type=int,\n            help='Number of attention heads')\n\n        self.parser.add_argument(\n            '--hiddenSizeHead',\n            default=64,\n            type=int,\n            help='Dimension of each attention head')\n\n        self.parser.add_argument(\n            '--FeedForwardSize',\n            default=2048,\n            type=int,\n            help='Dimension of the feed forward layer')\n\n        self.parser.add_argument(\n            '--FlatMLPSize',\n            default=512,\n            type=int,\n            help='MLP flatten size')\n\n        self.parser.add_argument(\n            '--FlatGlimpses',\n            default=1,\n            type=int,\n            help='Number of flatten glimpses')\n\n        self.parser.add_argument(\n            '--FlatOutSize',\n            default=512,\n            type=int,\n            help='Final attention reduction dimension')\n\n        self.parser.add_argument(\n            '--layers',\n            #default=6,\n            default=4, #to reach more than 51\n            type=int,\n            help='Number of self attention layers')\n\n        self.parser.add_argument(\n            '--bidirectional',\n            default=1,\n            type=int,\n            help='Activate to use bidirectional LSTMs')\n\n        self.initialized = True\n\n    def parse(self):\n        # initialize parser\n        if not self.initialized:\n            self.initialize()\n        #self.opts = self.parser.parse_args()#***\n        self.opts, unknown = self.parser.parse_known_args()#this is added by me to fix the error of command line arguments.\n        # parse gpu id list\n        str_gpu_ids = self.opts.gpu_ids.split(',')\n        self.opts.gpu_ids = []\n        for str_id in str_gpu_ids:\n            if str_id.isdigit() and int(str_id) >= 0:\n                self.opts.gpu_ids.append(int(str_id))\n        if len(self.opts.gpu_ids) > 0 and torch.cuda.is_available():\n            print('\\n[INFO] Using {} CUDA device(s) ...'.format(\n                len(self.opts.gpu_ids)))\n        else:\n            print('\\n[INFO] Using cpu ...')\n            self.opts.gpu_ids = []\n\n        # parse the optimizer's betas and lr decay marks\n        self.opts.betas = [float(beta) for beta in self.opts.betas.split(',')]\n        lr_decay_marks = [int(m) for m in self.opts.lr_decay_marks.split(',')]\n        for i in range(1, len(lr_decay_marks)):\n            assert lr_decay_marks[i] > lr_decay_marks[i-1]\n        self.opts.lr_decay_marks = lr_decay_marks\n\n        # print and save options\n        args = vars(self.opts)\n        print('\\n ' + 30*'-' + 'Opts' + 30*'-')\n        for k, v in args.items():\n            print('%s: %s' % (str(k), str(v)))\n\n        if not os.path.isdir(self.opts.run_dir):\n            os.makedirs(self.opts.run_dir)\n        filename = 'opts.txt'\n        file_path = os.path.join(self.opts.run_dir, filename)\n        with open(file_path, 'wt') as fout:\n            fout.write('| options\\n')\n            for k, v in sorted(args.items()):\n                fout.write('%s: %s\\n' % (str(k), str(v)))\n        return self.opts\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.856809Z","iopub.execute_input":"2024-02-13T15:59:16.857096Z","iopub.status.idle":"2024-02-13T15:59:16.898254Z","shell.execute_reply.started":"2024-02-13T15:59:16.857065Z","shell.execute_reply":"2024-02-13T15:59:16.897273Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport json, torch, pickle, copy, time\nimport numpy as np\nimport argparse\nimport torch.nn as nn\nimport torch.utils.data as Data\nfrom tensorboardX import SummaryWriter\nfrom copy import deepcopy\nimport pickle\nfrom tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.899776Z","iopub.execute_input":"2024-02-13T15:59:16.900066Z","iopub.status.idle":"2024-02-13T15:59:16.914354Z","shell.execute_reply.started":"2024-02-13T15:59:16.900043Z","shell.execute_reply":"2024-02-13T15:59:16.913419Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"class Execution:\n    def __init__(self, optsQ, optsC):\n        self.opts = deepcopy(optsQ)\n        if self.opts.useCuda > 0 and torch.cuda.is_available():\n            self.device = torch.device(\"cuda:0\")\n            print(\"[INFO] Using GPU {} ...\".format(torch.cuda.get_device_name(0)))\n        else:\n            print(\"[INFO] Using CPU ...\")\n            self.device = torch.device(\"cpu\")\n\n        self.loss_fn = torch.nn.NLLLoss().to(self.device)\n\n        print(\"[INFO] Loading dataset ...\")\n\n        self.datasetTr = ClevrDialogQuestionDataset(\n            self.opts.dataPathTr, self.opts.vocabPath, \"train\", \"All tr data\")\n\n        self.datasetVal = ClevrDialogQuestionDataset(\n            self.opts.dataPathVal, self.opts.vocabPath, \"val\", \"All val data\", train=False)\n\n        self.datasetTest = ClevrDialogQuestionDataset(\n            self.opts.dataPathTest, self.opts.vocabPath, \"test\", \"All val data\", train=False)\n\n        self.QuestionNet = constructQuestionNet(\n            self.opts,\n            self.datasetTr.lenVocabText,\n            self.datasetTr.lenVocabProg,\n            self.datasetTr.maxLenProg,\n            #self.datasetTest.lenVocabText,#*** to solve mismatch problems\n            #self.datasetTest.lenVocabProg,#***\n            #self.datasetTest.maxLenProg#***\n            )\n\n        if os.path.isfile(self.opts.captionNetPath):\n            self.CaptionNet = constructCaptionNet(\n                optsC,\n                self.datasetTr.lenVocabText,\n                self.datasetTr.lenVocabProg,\n                self.datasetTr.maxLenProg,\n                #self.datasetTest.lenVocabText,#*** \n                #self.datasetTest.lenVocabProg,#***\n                #self.datasetTest.maxLenProg#***\n                )\n            print('Loading CaptionNet from {}'.format(self.opts.captionNetPath))\n            state_dict = torch.load(self.opts.captionNetPath)['state_dict']\n            self.CaptionNet.load_state_dict(state_dict)\n            self.CaptionNet.to(self.device)\n            total_params_cap = sum(p.numel() for p in self.CaptionNet.parameters() if p.requires_grad)\n            print(\"The caption encoder has {} trainable parameters\".format(total_params_cap))\n\n        self.QuestionNet.to(self.device)\n        # if os.path.isfile(self.opts.load_checkpoint_path):\n        #     print('Loading QuestionNet from {}'.format(optsQ.load_checkpoint_path))\n        #     state_dict = torch.load(self.opts.load_checkpoint_path)['state_dict']\n        #     self.QuestionNet.load_state_dict(state_dict)\n        total_params_quest = sum(p.numel() for p in self.QuestionNet.parameters() if p.requires_grad)\n        print(\"The question encoder has {} trainable parameters\".format(total_params_quest))\n\n        if \"minecraft\" in self.opts.scenesPath:\n            self.symbolicExecutor = SymbolicExecutorMinecraft(self.opts.scenesPath)\n        else:\n            self.symbolicExecutor = SymbolicExecutorClevr(self.opts.scenesPath)\n\n        tb_path = os.path.join(self.opts.run_dir, \"tb_logdir\")\n        if not os.path.isdir(tb_path):\n            os.makedirs(tb_path)\n\n        self.ckpt_path = os.path.join(self.opts.run_dir, \"ckpt_dir\")\n        if not os.path.isdir(self.ckpt_path):\n            os.makedirs(self.ckpt_path)\n        if not os.path.isdir(self.opts.text_log_dir):\n            os.makedirs(self.opts.text_log_dir)\n\n        self.writer = SummaryWriter(tb_path)\n        self.iter_val = 0\n#***\n        #if os.path.isfile(self.opts.dependenciesPath):\n            #with open(self.opts.dependenciesPath, \"rb\") as f:\n                #self.dependencies = pickle.load(f)\n\n    def train(self):\n        self.QuestionNet.train()\n\n        # Define the multi-gpu training if needed\n        if len(self.opts.gpu_ids) > 1:\n            self.QuestionNet = nn.DataParallel(self.QuestionNet, device_ids=self.opts.gpu_ids)\n\n        # Load checkpoint if resume training\n        if os.path.isfile(self.opts.load_checkpoint_path):\n            print(\"[INFO] Resume trainig from ckpt {} ...\".format(\n                self.opts.load_checkpoint_path\n            ))\n\n            # Load the network parameters\n            ckpt = torch.load(self.opts.load_checkpoint_path)\n            print(\"[INFO] Checkpoint successfully loaded ...\")\n            self.QuestionNet.load_state_dict(ckpt['state_dict'])\n\n            # Load the optimizer paramters\n            optim = get_optim(self.opts, self.QuestionNet, len(self.datasetTr))  # , ckpt['optim'], lr_base=ckpt['lr_base'])\n            # optim._step = int(data_size / self.__C.BATCH_SIZE * self.__C.CKPT_EPOCH)\n            optim.optimizer.load_state_dict(ckpt['optimizer'])\n            _iter = 0  #  ckpt['last_iter']\n            epoch = 0  # ckpt['last_epoch']\n\n        else:\n            optim = get_optim(self.opts, self.QuestionNet, len(self.datasetTr))\n            _iter = 0\n            epoch = 0\n\n        trainTime = 0\n        bestValAcc = float(\"-inf\")\n        bestCkp = 0\n        # Training loop\n        while _iter < self.opts.num_iters:\n\n            # Learning Rate Decay\n            if _iter in self.opts.lr_decay_marks:\n                adjust_lr(optim, self.opts.lr_decay_factor)\n\n            # Define multi-thread dataloader\n            dataloader = Data.DataLoader(\n                self.datasetTr,\n                batch_size=self.opts.batch_size,\n                shuffle=self.opts.shuffle_data,\n                num_workers=self.opts.num_workers,\n            )\n\n            # Iteration\n            time_start = 0\n            time_end = 0\n            \n            for batch_iter, (quest, hist, prog, questionRound, _) in enumerate(dataloader):\n            #for batch_iter, (quest, prog, questionRound, hist,  _) in enumerate(dataloader):\n                time_start = time.time()\n                if _iter >= self.opts.num_iters:\n                    break\n                quest = quest.to(self.device)\n                if self.opts.last_n_rounds < 10:\n                    last_n_rounds_batch = []\n                    for i, r in enumerate(questionRound.tolist()):\n                        startIdx = max(r - self.opts.last_n_rounds, 0)\n                        endIdx = max(r, self.opts.last_n_rounds)\n                        if hist.dim() == 3:\n                            assert endIdx - startIdx == self.opts.last_n_rounds\n                            histBatch = hist[i, :, :]\n                            last_n_rounds_batch.append(histBatch[startIdx:endIdx, :])\n                        elif hist.dim() == 2:\n                            startIdx *= 20\n                            endIdx *= 20\n                            histBatch = hist[i, :]\n                            temp = histBatch[startIdx:endIdx].cpu()\n                            if r > self.opts.last_n_rounds:\n                                last_n_rounds_batch.append(torch.cat([torch.tensor([1]), temp, torch.tensor([2])], 0))\n                            else:\n                                last_n_rounds_batch.append(torch.cat([temp, torch.tensor([2, 0])], 0))\n                    hist = torch.stack(last_n_rounds_batch, dim=0)\n                hist = hist.to(self.device)\n                prog = prog.to(self.device)\n                progTarget = prog.clone()\n                optim.zero_grad()\n\n                predSoftmax, _ = self.QuestionNet(quest, hist, prog[:, :-1])\n                loss = self.loss_fn(\n                    # predSoftmax[:, :-1, :].contiguous().view(-1, predSoftmax.size(2)),\n                    predSoftmax.contiguous().view(-1, predSoftmax.size(2)),\n                    progTarget[:, 1:].contiguous().view(-1))\n                loss.backward()\n\n                if _iter % self.opts.validate_every == 0 and _iter > 0:\n                    valAcc = self.val()\n                    if valAcc > bestValAcc:\n                        bestValAcc = valAcc\n                        bestCkp = _iter\n                        print(\"\\n[INFO] Checkpointing model @ iter {} with val accuracy {}\\n\".format(_iter, valAcc))\n                        state = {\n                            'state_dict': self.QuestionNet.state_dict(),\n                            'optimizer': optim.optimizer.state_dict(),\n                            'lr_base': optim.lr_base,\n                            'optim': optim.lr_base,\n                            'last_iter': _iter,\n                            'last_epoch': epoch,\n                        }\n                        # checkpointing\n                        torch.save(\n                            state,\n                            #os.path.join(self.ckpt_path, 'ckpt_iter' + str(_iter) + '.pkl')\n                            os.path.join(self.ckpt_path, 'QU_ckpt_iter' + '_(' +str(valAcc)+')_' +str(_iter) + '.pkl')\n                        )\n\n                # logging\n                self.writer.add_scalar(\n                    'train/loss',\n                    loss.cpu().data.numpy(),\n                    global_step=_iter)\n\n                self.writer.add_scalar(\n                    'train/lr',\n                    optim._rate,\n                    global_step=_iter)\n                if _iter % self.opts.display_every == 0:\n                    time_end = time.time()\n                    trainTime += time_end-time_start\n\n                    print(\"\\r[CLEVR-Dialog - %s (%d | %d)][epoch %2d][iter %4d/%4d][runtime %4f] loss: %.4f, lr: %.2e\" % (\n                        self.datasetTr.name,\n                        batch_iter,\n                        len(dataloader),\n                        epoch,\n                        _iter,\n                        self.opts.num_iters,\n                        trainTime,\n                        loss.cpu().data.numpy(),\n                        optim._rate,\n                    ), end='          ')\n\n                optim.step()\n                _iter += 1\n\n            epoch += 1\n        print(\"[INFO] Avg. epoch time: {} s\".format(trainTime / epoch))\n        print(\"[INFO] Best model achieved val acc. {} @ iter {}\".format(bestValAcc, bestCkp))\n\n    def val(self):\n        self.QuestionNet.eval()\n\n        total_correct = 0\n        total = 0\n\n        if len(self.opts.gpu_ids) > 1:\n            self.QuestionNet = nn.DataParallel(self.QuestionNet, device_ids=self.opts.gpu_ids)\n        self.QuestionNet = self.QuestionNet.eval()\n        dataloader = Data.DataLoader(\n            self.datasetVal,\n            batch_size=self.opts.batch_size,\n            shuffle=True,\n            num_workers=self.opts.num_workers,\n            pin_memory=False\n        )\n        _iterCur = 0\n        _totalCur = len(dataloader)\n\n        for step, (question, questionPrg, questionImgIdx, questionRounds, history, historiesProg, answer) in enumerate(dataloader):\n            # print(\"\\rEvaluation: [step %4d/%4d]\" % (\n            print(\"\\rEvaluation: [step %4d/%4d]\" % (\n                step,\n                int(len(dataloader)),\n            ), end='          ')\n\n            question = question.to(self.device)\n            if history.dim() == 3:\n                caption = history.detach()\n                caption = caption[:, 0, :]\n                caption = caption[:, :16].to(self.device)\n            elif history.dim() == 2:\n                caption = history.detach()\n                caption = caption[:, :16].to(self.device)\n            if self.opts.last_n_rounds is not None:\n                last_n_rounds_batch = []\n                for i, r in enumerate(questionRounds.tolist()):\n                    startIdx = max(r - self.opts.last_n_rounds, 0)\n                    endIdx = max(r, self.opts.last_n_rounds)\n                    if history.dim() == 3:\n                        assert endIdx - startIdx == self.opts.last_n_rounds\n                        histBatch = history[i, :, :]\n                        last_n_rounds_batch.append(histBatch[startIdx:endIdx, :])\n                    elif history.dim() == 2:\n                        startIdx *= 20\n                        endIdx *= 20\n                        histBatch = history[i, :]\n                        temp = histBatch[startIdx:endIdx]\n                        if r > self.opts.last_n_rounds:\n                            last_n_rounds_batch.append(torch.cat([torch.tensor([1]), temp, torch.tensor([2])], 0))\n                        else:\n                            last_n_rounds_batch.append(torch.cat([temp, torch.tensor([2, 0])], 0))\n                history = torch.stack(last_n_rounds_batch, dim=0)\n            history = history.to(self.device)\n            questionPrg = questionPrg.to(self.device)\n            #print(\"****************************\")\n            #print(\"This is the ground-truth:\", questionPrg)\n            #print(\"****************************\")\n            questProgsToksPred = self.QuestionNet.sample(question, history)\n            #print(\"****************************\")\n            #print(\"This is the prediction:\", questProgsToksPred)\n            #print(\"****************************\")\n            questProgsPred = decodeProg(questProgsToksPred, self.datasetVal.vocab[\"idx_prog_to_token\"])\n            ##print(\"****************************\")\n            targetProgs = decodeProg(questionPrg, self.datasetVal.vocab[\"idx_prog_to_token\"], target=True)\n            correct = [1 if pred == gt else 0 for (pred, gt) in zip(questProgsPred, targetProgs)]\n            correct = sum(correct)\n            total_correct += correct\n            total += len(targetProgs)\n            self.QuestionNet.train()\n\n        return 100.0 * (total_correct / total)\n\n\n\n    def getPrediction(self, questProgPred, capProgPred, historyProg, imgIndex):\n        self.symbolicExecutor.reset(imgIndex)\n        # if round one, execute the predicted caption program first then answer the question\n        if len(historyProg) == 1:\n            captionFuncLabel = capProgPred[0]\n            captionFuncArgs = capProgPred[1:]\n\n            questionFuncLabel = questProgPred[0]\n            questionFuncArgs = questProgPred[1:]\n\n            try:\n                _ = self.symbolicExecutor.execute(captionFuncLabel, captionFuncArgs)\n            except:\n                return \"Error\"\n\n            try:\n                predAnswer = self.symbolicExecutor.execute(questionFuncLabel, questionFuncArgs)\n            except:\n                return \"Error\"\n\n        # If it is not the first round, we have to execute the program history and\n        # then answer the question.\n        else:\n            questionFuncLabel = questProgPred[0]\n            questionFuncArgs = questProgPred[1:]\n            for prg in historyProg:\n                # prg = prg.split(\" \")\n                FuncLabel = prg[0]\n                FuncArgs = prg[1:]\n                try:\n                    _ = self.symbolicExecutor.execute(FuncLabel, FuncArgs)\n                except:\n                    return \"Error\"\n\n            try:\n                predAnswer = self.symbolicExecutor.execute(questionFuncLabel, questionFuncArgs)\n            except:\n                return \"Error\"\n        return str(predAnswer)\n\n    def run(self, run_mode, epoch=None):\n        self.set_seed(self.opts.seed)\n        if run_mode == 'train':\n            self.train()\n    \n        else:\n            exit(-1)\n\n    def set_seed(self, seed):\n        \"\"\"Sets the seed for reproducibility.\n        Args:\n            seed (int): The seed used\n        \"\"\"\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        np.random.seed(seed)\n        print('[INFO] Seed set to {}...'.format(seed))\n\n\ndef constructQuestionNet(opts, lenVocabText, lenVocabProg, maxLenProg):\n    decoder = Decoder(opts, lenVocabProg, maxLenProg)\n    if opts.encoderType == 1:\n        #encoder = QuestEncoder_1(opts, lenVocabText)\n        encoder = Encoder(opts, lenVocabText)\n    elif opts.encoderType == 2:\n        encoder = QuestEncoder_2(opts, lenVocabText)\n\n    net = SeqToSeqQ(encoder, decoder)\n    return net\n\n\ndef constructCaptionNet(opts, lenVocabText, lenVocabProg, maxLenProg):\n    decoder = Decoder(opts, lenVocabProg, maxLenProg)\n    #encoder = CaptionEncoder(opts, lenVocabText)\n    encoder = Encoder(opts, lenVocabText)\n    net = SeqToSeqC(encoder, decoder)\n    return net\n\n\ndef getProgHistories(progHistToks, prgIdxToToken):\n    progHist = []\n    temp = []\n    for tok in progHistToks:\n        if tok not in [0, 1, 2]:\n            temp.append(prgIdxToToken[tok])\n            # del progHistToks[i]\n        if tok == 2:\n            # del progHistToks[i]\n            # progHist.append(\" \".join(temp))\n            progHist.append(temp)\n            temp = []\n    return progHist\n\n\ndef getHistoriesFromStack(histToks, textIdxToToken):\n    histories = \"\\n\"\n    temp = []\n    for i, roundToks in enumerate(histToks):\n        for tok in roundToks:\n            if tok not in [0, 1, 2]:\n                temp.append(textIdxToToken[tok])\n                # del progHistToks[i]\n            if tok == 2:\n                # del progHistToks[i]\n                if i == 0:\n                    histories += \" \".join(temp) + \".\\n\"\n                else:\n                    histories += \" \".join(temp[:-1]) + \"? | {}.\\n\".format(temp[-1])\n                # histories.append(temp)\n                temp = []\n                break\n    return histories\n\n\ndef getHistoriesFromConcat(histToks, textIdxToToken):\n    histories = []\n    temp = []\n    for tok in histToks:\n        if tok not in [0, 1, 2]:\n            temp.append(textIdxToToken[tok])\n            # del progHistToks[i]\n        if tok == 2:\n            # del progHistToks[i]\n            histories.append(\" \".join(temp[:-1]) + \"? | {}\".format(temp[-1]))\n            # histories.append(temp)\n            temp = []\n    return histories\n\n\ndef decodeProg(tokens, prgIdxToToken, target=False):\n    if (target == True):#***\n        tokensBatch = tokens.tolist()\n    else:#***\n        tokensBatch = tokens\n    progsBatch = []\n    for tokens in tokensBatch:\n        prog = []\n        for tok in tokens:\n            if tok == 2:  # <END> has index 2\n                break\n            prog.append(prgIdxToToken.get(tok))\n        \n        if target:\n            prog = prog[1:]\n        progsBatch.append(prog)\n    return progsBatch\n\n\ndef printPred(predSoftmax, gts, prgIdxToToken):\n    assert predSoftmax.size(0) == gts.size(0)\n    tokens = predSoftmax.topk(1)[1].squeeze(-1)\n    tokens = tokens.tolist()\n    gts = gts.tolist()\n    message = \"\\n ------------------------ \\n\"\n    for token, gt in zip(tokens, gts):\n        message += \"Prediction: \"\n        for tok in token:\n            message += prgIdxToToken.get(tok) + \" \"\n        message += \"\\n Target   : \"\n        for tok in gt:\n            message += prgIdxToToken.get(tok) + \" \"\n        message += \"\\n ------------------------ \\n\"\n    return message\n\n\ndef get_per_round_acc(preds, gts, penalties):\n    res = {}\n    for img_preds, img_gt, img_pen in zip(preds, gts, penalties):\n        img_preds = list(img_preds)\n        img_gt = list(img_gt)\n        img_pen = list(img_pen)\n        for i, (pred, gt, pen) in enumerate(zip(img_preds, img_gt, img_pen)):\n            _round = str(i + 1)\n            if _round not in res:\n                res[_round] = {\n                    \"correct\": 0,\n                    \"all\": 0\n                }\n            res[_round][\"all\"] += 1\n            if pred == gt:\n                res[_round][\"correct\"] += 0.5**pen\n\n    textOut = \"\\n --------------- Per round Acc --------------- \\n\"\n    for k in res:\n        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res[k][\"correct\"]/res[k][\"all\"]))\n    return textOut\n\n\ndef get_per_question_type_acc(preds, gts, qtypes, penalties):\n    res1 = {}\n    res2 = {}\n\n    for img_preds, img_gt, img_qtypes, img_pen in zip(preds, gts, qtypes, penalties):\n        # img_preds = list(img_preds)\n        # img_gt = list(img_gt)\n        img_pen = list(img_pen)\n        for pred, gt, temp, pen in zip(img_preds, img_gt, img_qtypes, img_pen):\n            if temp not in res1:\n                res1[temp] = {\n                    \"correct\": 0,\n                    \"all\": 0\n                }\n            temp_cat = temp.split(\"-\")[0]\n            if temp_cat not in res2:\n                res2[temp_cat] = {\n                    \"correct\": 0,\n                    \"all\": 0\n                }\n            res1[temp][\"all\"] += 1\n            res2[temp_cat][\"all\"] += 1\n\n            if pred == gt:\n                res1[temp][\"correct\"] += 0.5**pen\n                res2[temp_cat][\"correct\"] += 0.5**pen\n\n    textOut = \"\\n --------------- Per question Type Acc --------------- \\n\"\n    for k in res1:\n        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res1[k][\"correct\"]/res1[k][\"all\"]))\n\n    textOut += \"\\n --------------- Per question Category Acc --------------- \\n\"\n    for k in res2:\n        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res2[k][\"correct\"]/res2[k][\"all\"]))\n    return textOut\n\n\ndef decode(tokens, prgIdxToToken, target=False):\n    if type(tokens) != list:\n        tokens = tokens.tolist()\n\n    progsBatch = []\n    for token in tokens:\n        prog = []\n        for tok in token:\n            if tok == 2:  # <END> has index 2\n                break\n            prog.append(prgIdxToToken.get(tok))\n        if target:\n            prog = prog[1:]\n        # progsBatch.append(\" \".join(prog))\n        progsBatch.append(prog)\n    return progsBatch\n\n#if __name__ == \"__main__\":#***","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:16.915886Z","iopub.execute_input":"2024-02-13T15:59:16.916172Z","iopub.status.idle":"2024-02-13T15:59:17.003557Z","shell.execute_reply.started":"2024-02-13T15:59:16.916144Z","shell.execute_reply":"2024-02-13T15:59:17.002706Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"optsC = OptionsC().parse()#***","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:17.004804Z","iopub.execute_input":"2024-02-13T15:59:17.005082Z","iopub.status.idle":"2024-02-13T15:59:17.024355Z","shell.execute_reply.started":"2024-02-13T15:59:17.005059Z","shell.execute_reply":"2024-02-13T15:59:17.023252Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"\n[INFO] Using 1 CUDA device(s) ...\n\n ------------------------------Opts------------------------------\nmode: train\nrun_dir: /kaggle/working/\nload_checkpoint_path: None\nres_path: /kaggle/working\ngpu_ids: [0]\nseed: 42\ndataPathTr: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_tr_half.h5\ndataPathVal: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_val_half.h5\ndataPathTest: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/cap_test_75000.h5\nvocabPath: /kaggle/input/nsvd-dataset/caption/vocab_output_caption.json\nbatch_size: 64\nnum_workers: 0\nnum_iters: 1000\ndisplay_every: 5\ndebug_every: 100\nvalidate_every: 100\nshuffle_data: 1\noptim: adam\nlr: 0.001\nbetas: [0.9, 0.98]\neps: 1e-09\nlr_decay_marks: [50000, 55000]\nlr_decay_factor: 0.5\nweight_decay: 1e-06\nembedDim: 300\nhiddenDim: 512\nnumLayers: 1\ndropout: 0.2\nmultiHead: 8\nhiddenSizeHead: 64\nFeedForwardSize: 2048\nFlatMLPSize: 512\nFlatGlimpses: 1\nFlatOutSize: 512\nlayers: 4\nbidirectional: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"optsQ = OptionsQ().parse()#***","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:17.025483Z","iopub.execute_input":"2024-02-13T15:59:17.025784Z","iopub.status.idle":"2024-02-13T15:59:17.040508Z","shell.execute_reply.started":"2024-02-13T15:59:17.025759Z","shell.execute_reply":"2024-02-13T15:59:17.039472Z"},"trusted":true},"execution_count":134,"outputs":[{"name":"stdout","text":"\n[INFO] Using 1 CUDA device(s) ...\n\n ------------------------------Opts------------------------------\nmode: train\ntype: q\nrun_dir: /kaggle/working\nuseCuda: 1\ntext_log_dir: /kaggle/working\nquestionNetPath: None\ncaptionNetPath: None\ndialogLen: 10\nlast_n_rounds: 10\nencoderType: 1\nload_checkpoint_path: None\ngpu_ids: [0]\nseed: 42\ndataPathTr: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/train_concat_half.h5\ndataPathVal: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/val_concat_half.h5\ndataPathTest: /kaggle/input/nsvd-dataset/Small_Tr_Val_Test_Final/test_concat_75000.h5\nscenesPath: /kaggle/input/nsvd-dataset/data/CLEVR_train_scenes.json\nvocabPath: /kaggle/input/nsvd-dataset/train_concat/vocab_output.json\nbatch_size: 64\ncountFirstFailueRound: 0\nmaxSamples: -1\nnum_workers: 0\nnum_iters: 1000\ndisplay_every: 5\nvalidate_every: 100\nshuffle_data: 1\noptim: adam\nlr: 0.0001\nbetas: [0.9, 0.98]\neps: 1e-09\nlr_decay_marks: [50000, 55000]\nlr_decay_factor: 0.5\nweight_decay: 0.001\nembedDim: 300\nhiddenDim: 512\nnumLayers: 1\ndropout: 0.2\nmultiHead: 8\nhiddenSizeHead: 64\nFeedForwardSize: 2048\nFlatMLPSize: 512\nFlatGlimpses: 1\nFlatOutSize: 512\nlayers: 4\nbidirectional: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"\nexe = Execution(optsQ, optsC)#***\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:59:17.041632Z","iopub.execute_input":"2024-02-13T15:59:17.041901Z","iopub.status.idle":"2024-02-13T15:59:38.158191Z","shell.execute_reply.started":"2024-02-13T15:59:17.041872Z","shell.execute_reply":"2024-02-13T15:59:38.157152Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"[INFO] Using GPU Tesla T4 ...\n[INFO] Loading dataset ...\nThe question encoder has 82904006 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"exe.run('train')\nprint(\"[INFO] Done ...\")#***","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-02-13T15:59:38.159512Z","iopub.execute_input":"2024-02-13T15:59:38.159890Z","iopub.status.idle":"2024-02-13T16:17:50.625970Z","shell.execute_reply.started":"2024-02-13T15:59:38.159854Z","shell.execute_reply":"2024-02-13T16:17:50.624814Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"[INFO] Seed set to 42...\nEvaluation: [step   19/  20]          5)][epoch  0][iter   95/1000][runtime 20.189049] loss: 0.6816, lr: 5.00e-05          \n[INFO] Checkpointing model @ iter 100 with val accuracy 21.377101681345074\n\nEvaluation: [step   19/  20]          25)][epoch  0][iter  195/1000][runtime 49.490485] loss: 0.3243, lr: 5.00e-05          \n[INFO] Checkpointing model @ iter 200 with val accuracy 28.903122497998403\n\nEvaluation: [step   19/  20]          25)][epoch  0][iter  295/1000][runtime 78.834692] loss: 0.2003, lr: 5.00e-05          \n[INFO] Checkpointing model @ iter 300 with val accuracy 36.50920736589271\n\nEvaluation: [step   19/  20]          25)][epoch  0][iter  395/1000][runtime 108.153006] loss: 0.0764, lr: 5.00e-05          \n[INFO] Checkpointing model @ iter 400 with val accuracy 49.55964771817454\n\nEvaluation: [step   19/  20]          25)][epoch  0][iter  495/1000][runtime 137.620934] loss: 0.0253, lr: 5.00e-05          \n[INFO] Checkpointing model @ iter 500 with val accuracy 51.72137710168134\n\n[CLEVR-Dialog - All tr data (995 | 27325)][epoch  0][iter  995/1000][runtime 279.910933] loss: 0.0001, lr: 5.00e-05          [INFO] Avg. epoch time: 279.91093254089355 s\n[INFO] Best model achieved val acc. 51.72137710168134 @ iter 500\n[INFO] Done ...\n","output_type":"stream"}]}]}