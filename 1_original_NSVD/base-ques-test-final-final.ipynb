{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-12-26T14:40:29.325315Z","iopub.status.busy":"2023-12-26T14:40:29.324431Z","iopub.status.idle":"2023-12-26T14:40:29.352608Z","shell.execute_reply":"2023-12-26T14:40:29.351221Z","shell.execute_reply.started":"2023-12-26T14:40:29.325278Z"}},"source":["## **clevrDialog_dataset.py**\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:31.926465Z","iopub.status.busy":"2024-02-14T17:58:31.926082Z","iopub.status.idle":"2024-02-14T17:58:32.655836Z","shell.execute_reply":"2024-02-14T17:58:32.654852Z","shell.execute_reply.started":"2024-02-14T17:58:31.926434Z"},"trusted":true},"outputs":[],"source":["#for cleaning the CPU ram\n","import gc\n","gc.collect()\n","\n","%reset -f"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.658491Z","iopub.status.busy":"2024-02-14T17:58:32.658108Z","iopub.status.idle":"2024-02-14T17:58:32.665274Z","shell.execute_reply":"2024-02-14T17:58:32.664407Z","shell.execute_reply.started":"2024-02-14T17:58:32.658455Z"},"trusted":true},"outputs":[],"source":["#Here we set variables for number of iterations and validation\n","TOTAL_ITER = 5000\n","VALID_EVE =1000"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.666615Z","iopub.status.busy":"2024-02-14T17:58:32.666297Z","iopub.status.idle":"2024-02-14T17:58:32.742366Z","shell.execute_reply":"2024-02-14T17:58:32.741541Z","shell.execute_reply.started":"2024-02-14T17:58:32.666589Z"},"trusted":true},"outputs":[],"source":["#for cleaning the GPU ram\n","import torch \n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.743811Z","iopub.status.busy":"2024-02-14T17:58:32.743482Z","iopub.status.idle":"2024-02-14T17:58:32.751872Z","shell.execute_reply":"2024-02-14T17:58:32.750861Z","shell.execute_reply.started":"2024-02-14T17:58:32.743785Z"},"trusted":true},"outputs":[],"source":["import h5py\n","import json\n","import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","import argparse#***"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.754856Z","iopub.status.busy":"2024-02-14T17:58:32.754562Z","iopub.status.idle":"2024-02-14T17:58:32.761423Z","shell.execute_reply":"2024-02-14T17:58:32.760622Z","shell.execute_reply.started":"2024-02-14T17:58:32.754833Z"},"trusted":true},"outputs":[],"source":["def invertDict(_dict):\n","    return {v: k for k, v in _dict.items()}"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.763310Z","iopub.status.busy":"2024-02-14T17:58:32.762487Z","iopub.status.idle":"2024-02-14T17:58:32.771754Z","shell.execute_reply":"2024-02-14T17:58:32.770800Z","shell.execute_reply.started":"2024-02-14T17:58:32.763284Z"},"trusted":true},"outputs":[],"source":["class ClevrDialogDataset(Dataset):\n","    def __init__(self, dataPath, vocabPath, split, indStart=0, indEnd=-1):\n","        super(ClevrDialogDataset, self).__init__()\n","        self.data = h5py.File(dataPath, \"r\")\n","        with open(vocabPath, \"r\") as f:\n","            self.vocab = json.load(f)\n","        self.vocab[\"idx_text_to_token\"] = invertDict(self.vocab[\"text_token_to_idx\"])\n","        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n","        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n","        self.lenVocabText = len(self.vocab[\"text_token_to_idx\"])\n","        self.lenVocabProg = len(self.vocab[\"prog_token_to_idx\"])\n","\n","        self.split = split\n","        self.indStart = indStart\n","        self.indEnd = indEnd\n","        self.maxSamples = indEnd - indStart\n","        self.maxLenProg = 6\n","\n","    def __len__(self):\n","        raise NotImplementedError\n","\n","    def __getitem__(self, index):\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.773205Z","iopub.status.busy":"2024-02-14T17:58:32.772944Z","iopub.status.idle":"2024-02-14T17:58:32.786409Z","shell.execute_reply":"2024-02-14T17:58:32.785667Z","shell.execute_reply.started":"2024-02-14T17:58:32.773182Z"},"trusted":true},"outputs":[],"source":["class ClevrDialogQuestionDataset(ClevrDialogDataset):\n","    def __init__(self, dataPath, vocabPath, split, name, train=True, indStart=0, indEnd=-1):\n","        super(ClevrDialogQuestionDataset, self).__init__(dataPath, vocabPath, split, indStart=indStart, indEnd=indEnd)\n","        self.questions = torch.LongTensor(np.asarray(self.data[\"questions\"], dtype=np.int64)[indStart: indEnd])\n","        self.quesProgs = torch.LongTensor(np.asarray(self.data[\"questionProgs\"], dtype=np.int64)[indStart: indEnd])\n","        self.questionRounds = torch.LongTensor(np.asarray(self.data[\"questionRounds\"], dtype=np.int64)[indStart: indEnd])\n","        self.questionImgIdx = torch.LongTensor(np.asarray(self.data[\"questionImgIdx\"], dtype=np.int64)[indStart: indEnd])\n","        self.histories = torch.LongTensor(np.asarray(self.data[\"histories\"], dtype=np.int64)[indStart: indEnd])\n","        self.historiesProgs = torch.LongTensor(np.asarray(self.data[\"historiesProg\"], dtype=np.int64)[indStart: indEnd])\n","\n","        self.answers = torch.LongTensor(np.asarray(self.data[\"answers\"], dtype=np.int64)[indStart: indEnd])\n","        self.name = name\n","        self.train = train\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        assert idx < len(self)\n","        question = self.questions[idx]\n","        questionPrg = self.quesProgs[idx]\n","        questionImgIdx = self.questionImgIdx[idx]\n","        questionRound = self.questionRounds[idx]\n","\n","        history = self.histories[idx]\n","        historiesProg = self.historiesProgs[idx]\n","\n","        answer = self.answers[idx]\n","        if self.train:\n","            return question, history, questionPrg, questionRound, answer\n","        else:\n","            return question, questionPrg, questionImgIdx, questionRound, history, historiesProg, answer"]},{"cell_type":"markdown","metadata":{},"source":["## **Clevr_statistics**"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.787659Z","iopub.status.busy":"2024-02-14T17:58:32.787362Z","iopub.status.idle":"2024-02-14T17:58:32.800526Z","shell.execute_reply":"2024-02-14T17:58:32.799626Z","shell.execute_reply.started":"2024-02-14T17:58:32.787635Z"},"trusted":true},"outputs":[],"source":["COLORS = [\"blue\", \"brown\", \"cyan\", \"gray\", \"green\", \"purple\", \"red\", \"yellow\"]\n","MATERIALS = [\"rubber\", \"metal\"]\n","SHAPES = [\"cube\", \"cylinder\", \"sphere\"]\n","SIZES = [\"large\", \"small\"]\n","\n","ATTRIBUTES_ALL = COLORS + MATERIALS + SHAPES + SIZES\n","\n","ANSWER_CANDIDATES = {\n","    # Count questions\n","    \"count-all\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n","    \"count-other\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","    \"count-all-group\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n","    \"count-attribute\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n","    \"count-attribure-group\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"],\n","    \"count-obj-rel-imm\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","    \"count-obj-rel-imm2\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","    \"count-obj-rel-early\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","    \"count-obj-exclude-imm\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","    \"count-obj-exclude-early\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n","\n","    # Existence questions\n","    \"exist-other\": [\"yes\", \"no\"],\n","    \"exist-attribute\": [\"yes\", \"no\"],\n","    \"exist-attribute-group\": [\"yes\", \"no\"],\n","    \"exist-obj-rel-imm\": [\"yes\", \"no\"],\n","    \"exist-obj-rel-imm2\": [\"yes\", \"no\"],\n","    \"exist-obj-rel-early\": [\"yes\", \"no\"],\n","    \"exist-obj-exclude-imm\": [\"yes\", \"no\"],\n","    \"exist-obj-exclude-early\": [\"yes\", \"no\"],\n","\n","    # Seek questions\n","    \"seek-attr-imm\": ATTRIBUTES_ALL,\n","    \"seek-attr-imm2\": ATTRIBUTES_ALL,\n","    \"seek-attr-early\": ATTRIBUTES_ALL,\n","    \"seek-attr-sim-early\": ATTRIBUTES_ALL,\n","    \"seek-attr-rel-imm\": ATTRIBUTES_ALL,\n","    \"seek-attr-rel-early\": ATTRIBUTES_ALL,\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Utils_m** just a function"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.802246Z","iopub.status.busy":"2024-02-14T17:58:32.801976Z","iopub.status.idle":"2024-02-14T17:58:32.819650Z","shell.execute_reply":"2024-02-14T17:58:32.818640Z","shell.execute_reply.started":"2024-02-14T17:58:32.802224Z"},"trusted":true},"outputs":[],"source":["import json\n","import numpy as np\n","\n","\n","def merge_captions_question_programs(path_cap, path_ques, caption_first=True):\n","    with open(path_cap, \"r\"):\n","        c_progs = path_cap.readlines()\n","    with open(path_ques, \"r\"):\n","        q_progs = path_ques.readlines()\n","\n","    all_merged_progs = []\n","    i = 0\n","    while i < len(q_progs):\n","        cap_idx = i % 11 if caption_first else i % 10\n","        start_idx_p = i + 1 if caption_first else i\n","        end_idx_p = start_idx_p + 12 if caption_first else  start_idx_p + 11\n","        temp = c_progs[cap_idx] + q_progs[start_idx_p, end_idx_p]\n","        all_merged_progs.append(temp)\n","        i = end_idx_p\n","\n","\n","def load_clevr_scenes(scenes_json):\n","    with open(scenes_json) as f:\n","        scenes_raw = json.load(f)\n","    if type(scenes_raw) == dict:\n","        scenes_raw = scenes_raw[\"scenes\"]\n","\n","    scenes = []\n","    for s in scenes_raw:\n","        table = []\n","        for i, o in enumerate(s['objects']):\n","            item = {}\n","            item['id'] = '%d-%d' % (s['image_index'], i)\n","            if '3d_coords' in o:\n","                item['position'] = [np.dot(o['3d_coords'], s['directions']['right']),\n","                                    np.dot(o['3d_coords'], s['directions']['front']),\n","                                    o['3d_coords'][2]]\n","            else:\n","                item['position'] = o['position']\n","            item['color'] = o['color']\n","            item['material'] = o['material']\n","            item['shape'] = o['shape']\n","            item['size'] = o['size']\n","            table.append(item)\n","        scenes.append(table)\n","    return scenes\n","\n","\n","def load_minecraft_scenes(scenes_json):\n","    with open(scenes_json) as f:\n","        scenes_raw = json.load(f)\n","    if type(scenes_raw) == dict:\n","        scenes_raw = scenes_raw[\"scenes\"]\n","\n","    scenes = []\n","    for s in scenes_raw:\n","        table = []\n","        for i, o in enumerate(s['objects']):\n","            item = {}\n","            item['id'] = '%d-%d' % (s['image_index'], i)\n","            if '3d_coords' in o:\n","                item['position'] = [np.dot(o['3d_coords'], s['directions']['right']),\n","                                    np.dot(o['3d_coords'], s['directions']['front']),\n","                                    o['3d_coords'][2]]\n","            else:\n","                item['position'] = o['position']\n","            item['nature'] = o['nature']\n","            item['class'] = o['class']\n","            item['direction'] = \"facing_\"\n","            if o['direction'] == \"front\":\n","                item['direction'] += \"forward\"\n","            elif o['direction'] == \"back\":\n","                item['direction'] += \"backward\"\n","            elif o['direction'] == \"right\":\n","                item['direction'] += \"right\"\n","            elif o['direction'] == \"left\":\n","                item['direction'] += \"left\"\n","            table.append(item)\n","        scenes.append(table)\n","    return scenes"]},{"cell_type":"markdown","metadata":{},"source":["## **Symbolic_executor**"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.821794Z","iopub.status.busy":"2024-02-14T17:58:32.821430Z","iopub.status.idle":"2024-02-14T17:58:32.944299Z","shell.execute_reply":"2024-02-14T17:58:32.943258Z","shell.execute_reply.started":"2024-02-14T17:58:32.821763Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from copy import deepcopy\n","\n","\n","#from executor.clevr_statics import COLORS, MATERIALS, SHAPES, SIZES\n","#from executor.clevr_statics import ANSWER_CANDIDATES as ANSWER_CANDIDATES_CLEVR\n","#from executor.clevr_statics import ATTRIBUTES_ALL as ATTRIBUTES_ALL_CLEVR\n","\n","#from utils_m import load_clevr_scenes\n","\n","\n","class SymbolicExecutorClevr(object):\n","    \"\"\"Symbolic executor for clevr-dialog\n","    \"\"\"\n","    def __init__(self, scenesPath):\n","        super(SymbolicExecutorClevr, self).__init__()\n","        self.functions = {}\n","        self.registerFunctions()\n","        self.uniqueObjFlag = False\n","        self.colors = COLORS\n","        self.materials = MATERIALS\n","        self.shapes = SHAPES\n","        self.sizes = SIZES\n","        self.answer_candidates = ANSWER_CANDIDATES#***\n","        self.attribute_all = ATTRIBUTES_ALL#***\n","        self.scenes = load_clevr_scenes(scenesPath)\n","\n","    def reset(self, sceneIdx):\n","        \"\"\"Resets the scene\n","\n","        Args:\n","            sceneIdx: The index of the new scene\n","        \"\"\"\n","        self.scene = self.scenes[sceneIdx]\n","        for _obj in self.scene:\n","            _obj[\"identifier\"] = None\n","        # store previous objects in a list to better answer\n","        # xxx-imm, xxx-imm2, xxx-group and xxx-early questions.\n","        self.objs = []\n","        self.groups = []\n","        self.visited = []\n","        self.currentObj = None\n","        self.currentGrp = []\n","        self.uniqueObjFlag = False\n","\n","    def registerFunctions(self):\n","        \"\"\"Registers the available functions of the executor.\n","        \"\"\"\n","        # Captions - extreme location\n","        self.functions[\"extreme-right\"] = self.extremeRight\n","        self.functions[\"extreme-left\"] = self.extremeLeft\n","        self.functions[\"extreme-behind\"] = self.extremeBehind\n","        self.functions[\"extreme-front\"] = self.extremeFront\n","        self.functions[\"extreme-center\"] = self.extremeCenter\n","\n","        # Captions - multiple objects\n","        self.functions[\"count-att\"] = self.countAttributeCaption\n","\n","        # Captions - object relations\n","        self.functions[\"obj-relation\"] = self.objRelation\n","\n","        # Captions - unique object\n","        self.functions[\"unique-obj\"] = self.uniqueObject\n","\n","        # Questions - Count\n","        self.functions[\"count-all\"] = self.countAll\n","        self.functions[\"count-other\"] = self.countOther\n","        self.functions[\"count-all-group\"] = self.countAllGroup\n","        self.functions[\"count-attribute\"] = self.countAttribute\n","        self.functions[\"count-attribute-group\"] = self.countAttributeGroup\n","        self.functions[\"count-obj-rel-imm\"] = self.countObjRelImm\n","        self.functions[\"count-obj-rel-imm2\"] = self.countObjRelImm2\n","        self.functions[\"count-obj-rel-early\"] = self.countObjRelEarly\n","        self.functions[\"count-obj-exclude-imm\"] = self.countObjExcludeImm\n","        self.functions[\"count-obj-exclude-early\"] = self.countObjExcludeEarly\n","\n","        # Questions - Exist\n","        self.functions[\"exist-other\"] = self.existOther\n","        self.functions[\"exist-attribute\"] = self.existAttribute\n","        self.functions[\"exist-attribute-group\"] = self.existAttributeGroup\n","        self.functions[\"exist-obj-rel-imm\"] = self.existObjRelImm\n","        self.functions[\"exist-obj-rel-imm2\"] = self.existObjRelImm\n","        self.functions[\"exist-obj-rel-early\"] = self.existObjRelEarly\n","        self.functions[\"exist-obj-exclude-imm\"] = self.existObjExcludeImm\n","        self.functions[\"exist-obj-exclude-early\"] = self.existObjExcludeEarly\n","\n","        # Questions - Seek\n","        self.functions[\"seek-attr-imm\"] = self.seekAttrImm\n","        self.functions[\"seek-attr-imm2\"] = self.seekAttrImm\n","        self.functions[\"seek-attr-early\"] = self.seekAttributeEarly\n","        self.functions[\"seek-attr-rel-imm\"] = self.seekAttributeRelImm\n","        self.functions[\"seek-attr-rel-early\"] = self.seekAttributeRelEarly\n","\n","\n","    ########################################################\n","    #                   Helper functions                   #\n","    ########################################################\n","    def getAttributeType(self, attribute):\n","        assert attribute in self.attribute_all, \"The attribute {} is unkown\".format(\n","            attribute)\n","        if attribute in self.colors:\n","            return \"color\"\n","        elif attribute in self.materials:\n","            return \"material\"\n","        elif attribute in self.shapes:\n","            return \"shape\"\n","        elif attribute in self.sizes:\n","            return \"size\"\n","\n","    def execute(self, functionLabel, functionArgs):\n","        assert functionLabel in self.functions, \"{} is not a valid function\".format(\n","            functionLabel)\n","        function = self.functions[functionLabel]\n","        answer = function(*functionArgs)\n","        return answer\n","\n","    def updateCurrentObj(self, obj):\n","        self.currentObj = obj\n","        objsCopy = deepcopy(self.objs)\n","        for i, _obj in enumerate(objsCopy):\n","            if _obj[\"id\"] == obj[\"id\"]:\n","                del self.objs[i]\n","        # Current obj is always kept at the end of the visited objs\n","        self.objs.append(obj)\n","\n","    def updateVisited(self, obj):\n","        if len(self.visited) == 0:\n","            self.visited.append(obj)\n","        else:\n","            newObjFlag = True\n","            for _obj in self.visited:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    newObjFlag = False\n","                    break\n","            if newObjFlag:\n","                self.visited.append(obj)\n","\n","    def getOther(self):\n","        others = []\n","        if len(self.visited) < len(self.scene):\n","            for _obj in self.scene:\n","                notExisting = True\n","                for __obj in self.visited:\n","                    if __obj[\"id\"] == _obj[\"id\"]:\n","                        notExisting = False\n","                        break\n","                if notExisting:\n","                    others.append(_obj)\n","        return others\n","\n","    def updateIdentifier(self, obj, attribute):\n","        if obj[\"identifier\"] is None:\n","            obj[\"identifier\"] = attribute\n","        else:\n","            identifiers = obj[\"identifier\"].split(\"-\")\n","            if attribute not in identifiers:\n","                identifiers.append(attribute)\n","                obj[\"identifier\"] = \"-\".join(identifiers)\n","\n","\n","    ########################################################\n","    #                   Caption programs                   #\n","    ########################################################\n","\n","    def extremeRight(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","\n","        leftToRight = deepcopy(self.scene)\n","        leftToRight.sort(key=lambda o: o[\"position\"][0])\n","        extremeRightObj = leftToRight[-1]\n","        for attributeType, attribute in zip(attributeTypes, attributes):\n","            assert extremeRightObj[attributeType] == attribute\n","            self.updateIdentifier(extremeRightObj, attribute)\n","\n","        self.updateCurrentObj(extremeRightObj)\n","        self.updateVisited(extremeRightObj)\n","        del leftToRight\n","\n","    def extremeLeft(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","\n","        leftToRight = deepcopy(self.scene)\n","       \n","        leftToRight.sort(key=lambda o: o[\"position\"][0])\n","        extremeLeftObj = leftToRight[0]\n","        \n","        for attributeType, attribute in zip(attributeTypes, attributes):\n","            assert extremeLeftObj[attributeType] == attribute\n","            self.updateIdentifier(extremeLeftObj, attribute)\n","\n","        self.updateCurrentObj(extremeLeftObj)\n","        self.updateVisited(extremeLeftObj)\n","        del leftToRight\n","\n","    def extremeFront(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","\n","        backToFront = deepcopy(self.scene)\n","        backToFront.sort(key=lambda o: o[\"position\"][1])\n","        extremeFrontObj = backToFront[-1]\n","        for attributeType, attribute in zip(attributeTypes, attributes):\n","        \n","\n","            assert extremeFrontObj[attributeType] == attribute\n","            self.updateIdentifier(extremeFrontObj, attribute)\n","\n","        self.updateCurrentObj(extremeFrontObj)\n","        self.updateVisited(extremeFrontObj)\n","        del backToFront\n","\n","    def extremeBehind(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","\n","        backToFront = deepcopy(self.scene)\n","        backToFront.sort(key=lambda o: o[\"position\"][1])\n","        extremeBehindObj = backToFront[0]\n","        for attributeType, attribute in zip(attributeTypes, attributes):\n","            assert extremeBehindObj[attributeType] == attribute\n","            self.updateIdentifier(extremeBehindObj, attribute)\n","\n","        self.updateCurrentObj(extremeBehindObj)\n","        self.updateVisited(extremeBehindObj)\n","        del backToFront\n","\n","    def extremeCenter(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","        numObjs = len(self.scene)\n","\n","        frontToBack = deepcopy(self.scene)\n","        frontToBack.sort(key=lambda o: o[\"position\"][1], reverse=True)\n","\n","        rightToLeft = deepcopy(self.scene)\n","        rightToLeft.sort(key=lambda o: o[\"position\"][0], reverse=True)\n","\n","        prelimenaryCandidates = []\n","\n","        for i, objFrontToBack in enumerate(frontToBack):\n","            numObjsInFront = i\n","            numObjsBehind = len(rightToLeft) - i - 1\n","            if numObjsInFront <= numObjs / 2 and numObjsBehind <= numObjs / 2:\n","                prelimenaryCandidates.append(objFrontToBack)\n","        foundCenter = False\n","        for _obj in prelimenaryCandidates:\n","            for i, objRightToLeft in enumerate(rightToLeft):\n","                if _obj[\"id\"] == objRightToLeft[\"id\"]:\n","                    numObjsToTheRight = i\n","                    numObjsToTheLeft = len(frontToBack) - i - 1\n","                    if numObjsToTheRight <= numObjs / 2 and numObjsToTheLeft <= numObjs / 2:\n","                        foundCenter = True\n","                        for attributeType, attribute in zip(attributeTypes, attributes):\n","                            if _obj[attributeType] != attribute:\n","                                foundCenter = False\n","                                break\n","                        break\n","            if foundCenter:\n","                break\n","        for attributeType, attribute in zip(attributeTypes, attributes):\n","            self.updateIdentifier(_obj, attribute)\n","        self.updateCurrentObj(_obj)\n","        self.updateVisited(_obj)\n","        del rightToLeft, frontToBack\n","\n","    def countAttributeCaption(self, attribute):\n","        attributeType = self.getAttributeType(attribute)\n","        objs = []\n","        for _obj in self.scene:\n","            if _obj[attributeType] == attribute:\n","                objs.append(deepcopy(_obj))\n","        for _obj in objs:\n","            self.updateIdentifier(_obj, attribute)\n","        # update the current group\n","        self.currentGrp = objs\n","\n","        # update the visited objects list\n","        for _obj in objs:\n","            self.updateVisited(_obj)\n","\n","    def getAnchorAttribute(self, attribute_1, attribute_2, scene):\n","        # The anchor object is unique. If we filter the object list\n","        # based on the attribute anchor, we must find only one object.\n","        filterAttribute_1 = self.filterAttribute(scene, attribute_1)\n","        if len(filterAttribute_1) == 1:\n","            return attribute_1\n","        else:\n","            return attribute_2\n","\n","    def objRelation(self, attribute, attributeAnchor, relation):\n","        assert relation in [\"left\", \"right\", \"front\", \"behind\"]\n","        # find the anchor object\n","        if attributeAnchor != self.getAnchorAttribute(attribute, attributeAnchor, self.scene):\n","            temp = deepcopy(attribute)\n","            attribute = deepcopy(attributeAnchor)\n","            attributeAnchor = temp\n","            if relation == \"left\":\n","                relation = \"right\"\n","            elif relation == \"right\":\n","                relation = \"left\"\n","            elif relation == \"behind\":\n","                relation = \"front\"\n","            elif relation == \"front\":\n","                relation = \"behind\"\n","\n","        # Order the objects in the scene w.r.t. the relation\n","        sceneCopy = deepcopy(self.scene)\n","\n","        if relation in [\"left\", \"right\"]:\n","            sceneCopy.sort(key=lambda o: o[\"position\"][0])\n","        else:\n","            sceneCopy.sort(key=lambda o: o[\"position\"][1])\n","\n","        # get the anchor object\n","        attributeTypeAnchor = self.getAttributeType(attributeAnchor)\n","        for i, _obj in enumerate(sceneCopy):\n","            if _obj[attributeTypeAnchor] == attributeAnchor:\n","                break\n","        # save the anchor object before the main object\n","        anchorObj = _obj\n","        self.updateIdentifier(anchorObj, attributeAnchor)\n","        self.updateCurrentObj(anchorObj)\n","        self.updateVisited(anchorObj)\n","\n","        if relation in [\"left\", \"behind\"]:\n","            sceneCopy = list(reversed(sceneCopy[:i]))\n","        else:\n","            sceneCopy = sceneCopy[i+1:]\n","\n","        attributeType = self.getAttributeType(attribute)\n","        # get the main object\n","        for _obj in sceneCopy:\n","            # and not equalDicts(_obj, anchorObj):\n","            if _obj[attributeType] == attribute:\n","                break\n","        self.updateIdentifier(_obj, attribute)\n","        self.updateCurrentObj(_obj)\n","        self.updateVisited(_obj)\n","        del sceneCopy\n","\n","    def uniqueObject(self, *attributes):\n","        attributes = list(attributes)\n","        attributeTypes = list(\n","            map(lambda att: self.getAttributeType(att), attributes))\n","\n","        for _obj in self.scene:\n","            found = True\n","            for attributeType, attribute in zip(attributeTypes, attributes):\n","                if _obj[attributeType] != attribute:\n","                    found = False\n","                    break\n","\n","            if found:\n","                break\n","        for att in attributes:\n","            self.updateIdentifier(_obj, att)\n","\n","        self.updateCurrentObj(_obj)\n","        self.updateVisited(_obj)\n","\n","    ######################################## Question Programs ########################################\n","    def filterOutObj(self, scene, obj):\n","        sceneCopy = deepcopy(scene)\n","        for i, _obj in enumerate(scene):\n","            if obj[\"id\"] == _obj[\"id\"]:\n","                break\n","        del sceneCopy[i]\n","        return sceneCopy\n","\n","    def filterAttribute(self, scene, attribute):\n","        attributeType = self.getAttributeType(attribute)\n","        filtered = []\n","        if len(scene) == 0:\n","            return filtered\n","\n","        for _obj in scene:\n","            if _obj[attributeType] == attribute:\n","                filtered.append(_obj)\n","        return filtered\n","\n","    def excludeAttribute(self, scene, obj, attributeType):\n","        filtered = []\n","        if len(scene) == 0:\n","            return filtered\n","        for _obj in scene:\n","            if _obj[\"id\"] != obj[\"id\"] and obj[attributeType] == _obj[attributeType]:\n","                filtered.append(_obj)\n","\n","        # Update the visited objects list\n","        if len(filtered) > 0:\n","            for _obj in filtered:\n","                self.updateVisited(_obj)\n","        return filtered\n","\n","    def filterLeft(self, scene, obj):\n","        filtered = []\n","        if len(scene) == 0:\n","            return filtered\n","\n","        for _obj in self.scene:\n","            # if the x-coordinate of _obj is smaller than the x-coordinate of slef.currentObj,\n","            # then _obj is located to the left of self.currentObj\n","            if _obj[\"position\"][0] < obj[\"position\"][0] and _obj[\"id\"] != obj[\"id\"]:\n","                filtered.append(_obj)\n","        return filtered\n","\n","    def filterRight(self, scene, obj):\n","        filtered = []\n","        for _obj in self.scene:\n","            # if the x-coordinate of _obj is bigger than the x-coordinate of slef.currentObj,\n","            # then _obj is located to the right of self.currentObj\n","            if _obj[\"position\"][0] > obj[\"position\"][0] and _obj[\"id\"] != obj[\"id\"]:\n","                filtered.append(_obj)\n","        return filtered\n","\n","    def filterFront(self, scene, obj):\n","        filtered = []\n","        if len(scene) == 0:\n","            return filtered\n","\n","        for _obj in self.scene:\n","            # if the y-coordinate of _obj is smaller than the y-coordinate of slef.currentObj,\n","            # then _obj is located in front of self.currentObj\n","            if _obj[\"position\"][1] > obj[\"position\"][1] and _obj[\"id\"] != obj[\"id\"]:\n","                filtered.append(_obj)\n","        return filtered\n","\n","    def filterBehind(self, scene, obj):\n","        # assert type(scene) == list, \"Excpected type list got {} instead\".format(type(scene))\n","        filtered = []\n","        if len(scene) == 0:\n","            return filtered\n","\n","        for _obj in scene:\n","            # if the y-coordinate of _obj is bigger than the y-coordinate of slef.currentObj,\n","            # then _obj is located behind self.currentObj\n","            if _obj[\"position\"][1] < obj[\"position\"][1] and _obj[\"id\"] != obj[\"id\"]:\n","                filtered.append(_obj)\n","        return filtered\n","\n","    def filterPosition(self, scene, obj, pos):\n","        # assert type(scene) == list, \"Excpected type list got {} instead\".format(type(scene))\n","        assert pos in [\"left\", \"right\", \"front\", \"behind\"]\n","        if pos == \"left\":\n","            filtered = self.filterLeft(scene, obj)\n","        elif pos == \"right\":\n","            filtered = self.filterRight(scene, obj)\n","        elif pos == \"front\":\n","            filtered = self.filterFront(scene, obj)\n","        elif pos == \"behind\":\n","            filtered = self.filterBehind(scene, obj)\n","\n","        return filtered\n","\n","    ###########################################################################\n","    #                           Counting questions                            #\n","    ###########################################################################\n","    def countAll(self):\n","        self.currentGrp = deepcopy(self.scene)\n","        self.groups.append(deepcopy(self.scene))\n","        return len(self.scene)\n","\n","    def countOther(self):\n","        others = self.getOther()\n","        if len(others) > 0:\n","            self.currentGrp = others\n","            self.groups.append(others)\n","        if len(others) == 1:\n","            obj = others[0]\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    break\n","            self.updateCurrentObj(obj)\n","\n","            self.updateVisited(obj)\n","        return len(others)\n","\n","    def countAllGroup(self):\n","        return len(self.currentGrp)\n","\n","    def countAttribute(self, attribute, updateCurrentObj=True):\n","        filtered = self.filterAttribute(self.scene, attribute)\n","        if len(filtered) == 0:\n","            return 0\n","        # Update the visited objects list\n","        for _obj in filtered:\n","            self.updateVisited(_obj)\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            self.updateIdentifier(obj, attribute)\n","            self.updateVisited(obj)\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","\n","        self.groups.append(filtered)\n","        self.currentGrp = filtered\n","        return len(filtered)\n","\n","    def countAttributeGroup(self, attribute, updateCurrentObj=True):\n","        filtered = self.filterAttribute(self.currentGrp, attribute)\n","        if len(filtered) == 0:\n","            return 0\n","        # Update the visited objects list\n","        for _obj in filtered:\n","            self.updateVisited(_obj)\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            self.updateIdentifier(obj, attribute)\n","            self.updateVisited(obj)\n","\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","\n","        self.groups.append(filtered)\n","        self.currentGrp = filtered\n","        return len(filtered)\n","\n","    def countObjRelImm(self, pos, updateCurrentObj=True):\n","        filtered = self.filterPosition(self.scene, self.currentObj, pos)\n","        if len(filtered) == 0:\n","            return 0\n","        # Update the visited objects list\n","        for _obj in filtered:\n","            self.updateVisited(_obj)\n","\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","                self.uniqueObjFlag = True\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","        return len(filtered)\n","\n","    def countObjRelImm2(self, pos):\n","        if self.uniqueObjFlag:\n","            # del self.objs[-1]\n","            self.updateCurrentObj(self.objs[-2])\n","            self.uniqueObjFlag = False\n","        return self.countObjRelImm(pos)\n","\n","    def countObjRelEarly(self, pos, earlyObjAttribute, updateCurrentObj=True):\n","        for objEarly in reversed(self.objs):\n","            if objEarly[\"identifier\"] is not None:\n","                identifiers = objEarly[\"identifier\"].split(\"-\")\n","                if earlyObjAttribute in identifiers:\n","                    break\n","            else:\n","                continue\n","        filtered = self.filterPosition(self.scene, objEarly, pos)\n","        if len(filtered) == 0:\n","            return 0\n","        # Update the visited objects list\n","        for _obj in filtered:\n","            self.updateVisited(_obj)\n","\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","        else:\n","            self.updateCurrentObj(objEarly)\n","\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","        return len(filtered)\n","\n","    def countObjExcludeImm(self, attributeType, updateCurrentObj=True):\n","        filtered = self.excludeAttribute(\n","            self.scene, self.currentObj, attributeType)\n","        if len(filtered) == 0:\n","            return 0\n","\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","        return len(filtered)\n","\n","    def countObjExcludeEarly(self, attributeType, earlyObjAttribute, updateCurrentObj=True):\n","        for objEarly in reversed(self.objs):\n","            if objEarly[\"identifier\"] is not None:\n","                identifiers = objEarly[\"identifier\"].split(\"-\")\n","                if earlyObjAttribute in identifiers:\n","                    break\n","            else:\n","                continue\n","\n","        filtered = self.excludeAttribute(self.scene, objEarly, attributeType)\n","        if len(filtered) == 0:\n","            return 0\n","\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj = _obj\n","                    new = False\n","                    break\n","            if updateCurrentObj:\n","                self.updateCurrentObj(obj)\n","            else:\n","                if new:\n","                    self.objs.append(obj)\n","        else:\n","            self.updateCurrentObj(objEarly)\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","        return len(filtered)\n","\n","    ###########################################################################\n","    #                           Existence questions                           #\n","    ###########################################################################\n","\n","    def existOther(self):\n","        others = self.getOther()\n","        numOther = len(others)\n","        if numOther > 0:\n","            self.currentGrp = others\n","            self.groups.append(others)\n","            for _obj in others:\n","                self.updateVisited(_obj)\n","        return \"yes\" if numOther > 0 else \"no\"\n","\n","    def existAttribute(self, attribute):\n","        filtered = self.filterAttribute(self.scene, attribute)\n","        numAttribute = len(filtered)\n","        if numAttribute == 0:\n","            return \"no\"\n","\n","        # Update the visited objects list\n","        for _obj in filtered:\n","            self.updateVisited(_obj)\n","        if len(filtered) == 1:\n","            obj = filtered[0]\n","            new = True\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    self.updateIdentifier(_obj, attribute)\n","                    new = False\n","                    break\n","            if new:\n","                self.updateIdentifier(obj, attribute)\n","                self.objs.append(obj)\n","                # self.updateCurrentObj(obj)\n","\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","        return \"yes\"\n","\n","    def existAttributeGroup(self, attribute):\n","        numAttributeGrp = self.countAttributeGroup(\n","            attribute, updateCurrentObj=False)\n","        return \"yes\" if numAttributeGrp > 0 else \"no\"\n","\n","    def existObjRelImm(self, pos):\n","        numObjs = self.countObjRelImm(pos, updateCurrentObj=False)\n","        return \"yes\" if numObjs > 0 else \"no\"\n","\n","    def existObjRelEarly(self, pos, earlyObjAttribute):\n","        numObjs = self.countObjRelEarly(\n","            pos, earlyObjAttribute, updateCurrentObj=False)\n","        return \"yes\" if numObjs > 0 else \"no\"\n","\n","    def existObjExcludeImm(self, attributeType):\n","        numObjs = self.countObjExcludeImm(\n","            attributeType, updateCurrentObj=False)\n","        return \"yes\" if numObjs > 0 else \"no\"\n","\n","    def existObjExcludeEarly(self, attributeType, earlyObjAttribute):\n","        for objEarly in reversed(self.objs):\n","            if objEarly[\"identifier\"] is not None:\n","                identifiers = objEarly[\"identifier\"].split(\"-\")\n","                if earlyObjAttribute in identifiers:\n","                    break\n","            else:\n","                continue\n","\n","        filtered = self.excludeAttribute(self.scene, objEarly, attributeType)\n","        numObjs = len(filtered)\n","        if numObjs == 0:\n","            return \"no\"\n","        self.currentGrp = filtered\n","        self.groups.append(filtered)\n","        return \"yes\"\n","\n","    ###########################################################################\n","    #                             Seek questions                              #\n","    ###########################################################################\n","\n","    def seekAttrImm(self, attributeType):\n","        assert attributeType in self.currentObj, \"Attributre <{}> is not valid\"\n","        self.updateIdentifier(self.currentObj, self.currentObj[attributeType])\n","        return self.currentObj[attributeType]\n","\n","    def seekAttributeEarly(self, attributeType, earlyObjAttribute):\n","        for objEarly in reversed(self.objs):\n","            if objEarly[\"identifier\"] is not None:\n","                identifiers = objEarly[\"identifier\"].split(\"-\")\n","                if earlyObjAttribute in identifiers:\n","                    break\n","            else:\n","                continue\n","        self.updateIdentifier(objEarly, objEarly[attributeType])\n","        self.updateCurrentObj(objEarly)\n","        self.updateVisited(objEarly)\n","        return objEarly[attributeType]\n","\n","    def seekAttributeRelImm(self, attributeType, pos):\n","        filtered = self.filterPosition(self.scene, self.currentObj, pos)\n","        if len(filtered) == 0:\n","            return \"none\"\n","        else:\n","            # Get the closest object to slef.obj\n","            if pos == \"left\":\n","                filtered.sort(key=lambda x: x[\"position\"][0])\n","                obj = filtered[-1]\n","            elif pos == \"right\":\n","                filtered.sort(key=lambda x: x[\"position\"][0])\n","                obj = filtered[0]\n","            elif pos == \"front\":\n","                filtered.sort(key=lambda x: x[\"position\"][1])\n","                obj = filtered[0]\n","            elif pos == \"behind\":\n","                filtered.sort(key=lambda x: x[\"position\"][1])\n","              \n","                obj = filtered[-1]\n","\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj[\"identifier\"] = _obj[\"identifier\"]\n","                    break\n","            self.updateIdentifier(obj, obj[attributeType])\n","            self.updateCurrentObj(obj)\n","            self.updateVisited(obj)\n","            return obj[attributeType]\n","\n","    def seekAttributeRelEarly(self, attributeType, pos, earlyObjAttribute):\n","        for objEarly in reversed(self.objs):\n","            if objEarly[\"identifier\"] is not None:\n","                identifiers = objEarly[\"identifier\"].split(\"-\")\n","                if earlyObjAttribute in identifiers:\n","                    break\n","            else:\n","                continue\n","\n","        filtered = self.filterPosition(self.scene, objEarly, pos)\n","        if len(filtered) == 0:\n","            return \"none\"\n","        else:\n","            # Get the closest object to slef.obj\n","            if pos == \"left\":\n","                filtered.sort(key=lambda x: x[\"position\"][0])\n","                obj = filtered[-1]\n","            elif pos == \"right\":\n","                filtered.sort(key=lambda x: x[\"position\"][0])\n","                obj = filtered[0]\n","            elif pos == \"front\":\n","                filtered.sort(key=lambda x: x[\"position\"][1])\n","                obj = filtered[0]\n","            elif pos == \"behind\":\n","                filtered.sort(key=lambda x: x[\"position\"][1])\n","                obj = filtered[-1]\n","            for _obj in self.objs:\n","                if _obj[\"id\"] == obj[\"id\"]:\n","                    obj[\"identifier\"] = _obj[\"identifier\"]\n","                    break\n","            self.updateIdentifier(obj, obj[attributeType])\n","            self.updateCurrentObj(obj)\n","            self.updateVisited(obj)\n","            return obj[attributeType]\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.946897Z","iopub.status.busy":"2024-02-14T17:58:32.945612Z","iopub.status.idle":"2024-02-14T17:58:32.952193Z","shell.execute_reply":"2024-02-14T17:58:32.951238Z","shell.execute_reply.started":"2024-02-14T17:58:32.946860Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import os\n","import torch\n","#import utils_m"]},{"cell_type":"markdown","metadata":{},"source":["### **Option_caption_parser**"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.953638Z","iopub.status.busy":"2024-02-14T17:58:32.953316Z","iopub.status.idle":"2024-02-14T17:58:32.965136Z","shell.execute_reply":"2024-02-14T17:58:32.964199Z","shell.execute_reply.started":"2024-02-14T17:58:32.953602Z"},"trusted":true},"outputs":[],"source":["#!touch \"/kaggle/working/\" res.txt"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:32.967048Z","iopub.status.busy":"2024-02-14T17:58:32.966480Z","iopub.status.idle":"2024-02-14T17:58:33.001000Z","shell.execute_reply":"2024-02-14T17:58:33.000069Z","shell.execute_reply.started":"2024-02-14T17:58:32.967001Z"},"trusted":true},"outputs":[],"source":["class OptionsC():#changed optiopn class as Option_c to differentiate it with the one belong to question\n","    def __init__(self):\n","        self.parser = argparse.ArgumentParser()\n","        self.initialized = False\n","\n","    def initialize(self):\n","        self.parser.add_argument(\n","            '--mode',\n","            default=\"train\",\n","            # required=True,\n","            type=str,\n","            choices=['train', 'test'],\n","            help='The mode of the experiment')\n","\n","        self.parser.add_argument(\n","            '--run_dir',\n","            default=\"caption_small\",\n","            # required=True,\n","            type=str,\n","            help='The experiment directory')\n","\n","        self.parser.add_argument(\n","            '--load_checkpoint_path',\n","            default='caption_small/ckpt_dir/ckpt_iter5000.pkl',\n","            type=str,\n","            help='The path the the pretrained CaptionNet')\n","\n","        self.parser.add_argument(\n","            '--res_path',\n","            default=\"caption_small/res.txt\",#***\n","            # required=True,\n","            type=str,\n","            help='Path where to log the predicted caption programs')\n","\n","        self.parser.add_argument(\n","            '--gpu_ids',\n","            default='0',\n","            type=str,\n","            help='Id of the gpu to be used')\n","\n","        self.parser.add_argument(\n","            '--seed',\n","            default=42,\n","            type=int,\n","            help='The seed used in training')\n","\n","        self.parser.add_argument(\n","            '--dataPathTr',\n","            default=\"/kaggle/input/cap_small/tr_cap_s.h5\", \n","            # required=True,\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed training data')\n","\n","        self.parser.add_argument(\n","            '--dataPathVal',\n","            default=\"/kaggle/input/cap_small/val_cap_s.h5\",\n","            # required=True,\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed validation data')\n","\n","        self.parser.add_argument(\n","            '--dataPathTest',\n","            # required=True,\n","            default=\"/kaggle/input/cap_small/test_cap_s.h5\",\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed test data')\n","\n","        self.parser.add_argument(\n","            '--vocabPath',\n","            default=\"/kaggle/input/caption/vocab_output_caption.json\",\n","\n","            # required=True,\n","            type=str,\n","            help='Path to the generated vocabulary')\n","\n","        self.parser.add_argument(\n","            '--batch_size',\n","            default=64,\n","            type=int,\n","            help='Batch size')\n","\n","        self.parser.add_argument(\n","            '--num_workers',\n","            default=0,\n","            type=int,\n","            help='Number of workers for loading')\n","\n","        self.parser.add_argument(\n","            '--num_iters',\n","            #default=5000,\n","            default=TOTAL_ITER,\n","            type=int,\n","            help='Total number of iterations')\n","\n","        self.parser.add_argument(\n","            '--display_every',\n","            default=5,\n","            type=int,\n","            help='Display training information every N iterations')\n","\n","        self.parser.add_argument(\n","            '--debug_every',\n","            default=100,\n","            type=int,\n","            help='Display debug message every N iterations')\n","\n","        self.parser.add_argument(\n","            '--validate_every',\n","            default=VALID_EVE,\n","            type=int,\n","            help='Validate every N iterations')\n","\n","        self.parser.add_argument(\n","            '--shuffle_data',\n","            default=1,\n","            type=int,\n","            help='Activate to shuffle the training data')\n","\n","        self.parser.add_argument(\n","            '--optim',\n","            default='adam',\n","            type=str,\n","            help='The name of the optimizer to be used')\n","\n","        self.parser.add_argument(\n","            '--lr',\n","            default=1e-3,\n","            type=float,\n","            help='Base learning rate')\n","\n","        self.parser.add_argument(\n","            '--betas',\n","            default='0.9, 0.98',\n","            type=str,\n","            help='Adam optimizer\\'s betas')\n","\n","        self.parser.add_argument(\n","            '--eps',\n","            default='1e-9',\n","            type=float,\n","            help='Adam optimizer\\'s epsilon')\n","\n","        self.parser.add_argument(\n","            '--lr_decay_marks',\n","            default='50000, 55000',\n","            type=str,\n","            help='Learing rate decay marks')\n","\n","        self.parser.add_argument(\n","            '--lr_decay_factor',\n","            default=0.5,\n","            type=float,\n","            help='Learning rate decay factor')\n","\n","        self.parser.add_argument(\n","            '--weight_decay',\n","            default=1e-6,\n","            type=float,\n","            help='Weight decay')\n","\n","        self.parser.add_argument(\n","            '--embedDim',\n","            default=300,\n","            type=int,\n","            help='Embedding dimension')\n","\n","        self.parser.add_argument(\n","            '--hiddenDim',\n","            default=512,\n","            type=int,\n","            help='LSTM hidden dimension')\n","\n","        self.parser.add_argument(\n","            '--numLayers',\n","            default=2,\n","            type=int,\n","            help='Number of hidden LSTM layers')\n","\n","        self.parser.add_argument(\n","            '--dropout',\n","            default=0.1,\n","            type=float,\n","            help='Dropout value')\n","\n","        self.parser.add_argument(\n","            '--multiHead',\n","            default=8,\n","            type=int,\n","            help='Number of attention heads')\n","\n","        self.parser.add_argument(\n","            '--hiddenSizeHead',\n","            default=64,\n","            type=int,\n","            help='Dimension of each attention head')\n","\n","        self.parser.add_argument(\n","            '--FeedForwardSize',\n","            default=2048,\n","            type=int,\n","            help='Dimension of the feed forward layer')\n","\n","        self.parser.add_argument(\n","            '--FlatMLPSize',\n","            default=512,\n","            type=int,\n","            help='MLP flatten size')\n","\n","        self.parser.add_argument(\n","            '--FlatGlimpses',\n","            default=1,\n","            type=int,\n","            help='Number of flatten glimpses')\n","\n","        self.parser.add_argument(\n","            '--FlatOutSize',\n","            default=512,\n","            type=int,\n","            help='Final attention reduction dimension')\n","\n","        self.parser.add_argument(\n","            '--layers',\n","            default=6,\n","            type=int,\n","            help='Number of self attention layers')\n","\n","        self.parser.add_argument(\n","            '--bidirectional',\n","            default=1,\n","            type=int,\n","            help='Activate to use bidirectional LSTMs')\n","\n","        self.initialized = True\n","\n","    def parse(self):\n","        # initialize parser\n","        if not self.initialized:\n","            self.initialize()\n","       # self.opts = self.parser.parse_args()\n","        self.opts, unknown = self.parser.parse_known_args()#this is added by me to fix the error of command line arguments.\n","\n","        # parse gpu id list\n","        str_gpu_ids = self.opts.gpu_ids.split(',')\n","        self.opts.gpu_ids = []\n","        for str_id in str_gpu_ids:\n","            if str_id.isdigit() and int(str_id) >= 0:\n","                self.opts.gpu_ids.append(int(str_id))\n","        if len(self.opts.gpu_ids) > 0 and torch.cuda.is_available():\n","            print('\\n[INFO] Using {} CUDA device(s) ...'.format(len(self.opts.gpu_ids)))\n","        else:\n","            print('\\n[INFO] Using cpu ...')\n","            self.opts.gpu_ids = []\n","\n","        # parse the optimizer's betas and lr decay marks\n","        self.opts.betas = [float(beta) for beta in self.opts.betas.split(',')]\n","        lr_decay_marks = [int(m) for m in self.opts.lr_decay_marks.split(',')]\n","        for i in range(1, len(lr_decay_marks)):\n","            assert lr_decay_marks[i] > lr_decay_marks[i-1]\n","        self.opts.lr_decay_marks = lr_decay_marks\n","\n","        # print and save options\n","        args = vars(self.opts)\n","        print('\\n ' + 30*'-' + 'Opts' + 30*'-')\n","        for k, v in args.items():\n","            print('%s: %s' % (str(k), str(v)))\n","\n","        if not os.path.isdir(self.opts.run_dir):\n","            os.makedirs(self.opts.run_dir)\n","        filename = 'opts_c.txt'\n","        file_path = os.path.join(self.opts.run_dir, filename)\n","        with open(file_path, 'wt') as fout:\n","            fout.write('| options\\n')\n","            for k, v in sorted(args.items()):\n","                fout.write('%s: %s\\n' % (str(k), str(v)))\n","        return self.opts"]},{"cell_type":"markdown","metadata":{},"source":["## **models.py**"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.006470Z","iopub.status.busy":"2024-02-14T17:58:33.005935Z","iopub.status.idle":"2024-02-14T17:58:33.014151Z","shell.execute_reply":"2024-02-14T17:58:33.013372Z","shell.execute_reply.started":"2024-02-14T17:58:33.006432Z"},"trusted":true},"outputs":[],"source":["import torch\n","import math\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.015408Z","iopub.status.busy":"2024-02-14T17:58:33.015135Z","iopub.status.idle":"2024-02-14T17:58:33.024084Z","shell.execute_reply":"2024-02-14T17:58:33.023278Z","shell.execute_reply.started":"2024-02-14T17:58:33.015384Z"},"trusted":true},"outputs":[],"source":["class FC(nn.Module):\n","    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n","        super(FC, self).__init__()\n","        self.dropout_r = dropout_r\n","        self.use_relu = use_relu\n","\n","        self.linear = nn.Linear(in_size, out_size)\n","\n","        if use_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","        if dropout_r > 0:\n","            self.dropout = nn.Dropout(dropout_r)\n","\n","    def forward(self, x):\n","        x = self.linear(x)\n","\n","        if self.use_relu:\n","            x = self.relu(x)\n","\n","        if self.dropout_r > 0:\n","            x = self.dropout(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.025743Z","iopub.status.busy":"2024-02-14T17:58:33.025324Z","iopub.status.idle":"2024-02-14T17:58:33.034107Z","shell.execute_reply":"2024-02-14T17:58:33.033254Z","shell.execute_reply.started":"2024-02-14T17:58:33.025676Z"},"trusted":true},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n","        super(MLP, self).__init__()\n","\n","        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n","        self.linear = nn.Linear(mid_size, out_size)\n","\n","    def forward(self, x):\n","        return self.linear(self.fc(x))"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.035427Z","iopub.status.busy":"2024-02-14T17:58:33.035160Z","iopub.status.idle":"2024-02-14T17:58:33.042886Z","shell.execute_reply":"2024-02-14T17:58:33.042006Z","shell.execute_reply.started":"2024-02-14T17:58:33.035403Z"},"trusted":true},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, size, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.eps = eps\n","\n","        self.a_2 = nn.Parameter(torch.ones(size))\n","        self.b_2 = nn.Parameter(torch.zeros(size))\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.044112Z","iopub.status.busy":"2024-02-14T17:58:33.043870Z","iopub.status.idle":"2024-02-14T17:58:33.056998Z","shell.execute_reply":"2024-02-14T17:58:33.056031Z","shell.execute_reply.started":"2024-02-14T17:58:33.044082Z"},"trusted":true},"outputs":[],"source":["class MHAtt(nn.Module):\n","    def __init__(self, opts):\n","        super(MHAtt, self).__init__()\n","        self.opts = opts\n","\n","        self.linear_v = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n","        self.linear_k = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n","        self.linear_q = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n","        self.linear_merge = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n","\n","        self.dropout = nn.Dropout(opts.dropout)\n","\n","    def forward(self, v, k, q, mask):\n","        n_batches = q.size(0)\n","\n","        v = self.linear_v(v).view(\n","            n_batches,\n","            -1,\n","            self.opts.multiHead,\n","            self.opts.hiddenSizeHead\n","        ).transpose(1, 2)\n","\n","        k = self.linear_k(k).view(\n","            n_batches,\n","            -1,\n","            self.opts.multiHead,\n","            self.opts.hiddenSizeHead\n","        ).transpose(1, 2)\n","\n","        q = self.linear_q(q).view(\n","            n_batches,\n","            -1,\n","            self.opts.multiHead,\n","            self.opts.hiddenSizeHead\n","        ).transpose(1, 2)\n","\n","        atted = self.att(v, k, q, mask)\n","        atted = atted.transpose(1, 2).contiguous().view(\n","            n_batches,\n","            -1,\n","            self.opts.hiddenDim\n","        )\n","\n","        atted = self.linear_merge(atted)\n","\n","        return atted\n","\n","    def att(self, value, key, query, mask):\n","        d_k = query.size(-1)\n","\n","        scores = torch.matmul(\n","            query, key.transpose(-2, -1)\n","        ) / math.sqrt(d_k)\n","\n","        if mask is not None:\n","            scores = scores.masked_fill(mask, -1e9)\n","\n","        att_map = F.softmax(scores, dim=-1)\n","        att_map = self.dropout(att_map)\n","\n","        return torch.matmul(att_map, value)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.058354Z","iopub.status.busy":"2024-02-14T17:58:33.058050Z","iopub.status.idle":"2024-02-14T17:58:33.068909Z","shell.execute_reply":"2024-02-14T17:58:33.067961Z","shell.execute_reply.started":"2024-02-14T17:58:33.058329Z"},"trusted":true},"outputs":[],"source":["class FFN(nn.Module):\n","    def __init__(self, opts):\n","        super(FFN, self).__init__()\n","\n","        self.mlp = MLP(\n","            in_size=opts.hiddenDim,\n","            mid_size=opts.FeedForwardSize,\n","            out_size=opts.hiddenDim,\n","            dropout_r=opts.dropout,\n","            use_relu=True\n","        )\n","\n","    def forward(self, x):\n","        return self.mlp(x)\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.071053Z","iopub.status.busy":"2024-02-14T17:58:33.070139Z","iopub.status.idle":"2024-02-14T17:58:33.078889Z","shell.execute_reply":"2024-02-14T17:58:33.078004Z","shell.execute_reply.started":"2024-02-14T17:58:33.071019Z"},"trusted":true},"outputs":[],"source":["class SA(nn.Module):\n","    def __init__(self, opts):\n","        super(SA, self).__init__()\n","        self.mhatt = MHAtt(opts)\n","        self.ffn = FFN(opts)\n","\n","        self.dropout1 = nn.Dropout(opts.dropout)\n","        self.norm1 = LayerNorm(opts.hiddenDim)\n","\n","        self.dropout2 = nn.Dropout(opts.dropout)\n","        self.norm2 = LayerNorm(opts.hiddenDim)\n","\n","    def forward(self, x, x_mask):\n","        x = self.norm1(x + self.dropout1(\n","            self.mhatt(x, x, x, x_mask)\n","        ))\n","\n","        x = self.norm2(x + self.dropout2(\n","            self.ffn(x)\n","        ))\n","\n","        return x"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.080616Z","iopub.status.busy":"2024-02-14T17:58:33.080041Z","iopub.status.idle":"2024-02-14T17:58:33.092892Z","shell.execute_reply":"2024-02-14T17:58:33.092061Z","shell.execute_reply.started":"2024-02-14T17:58:33.080590Z"},"trusted":true},"outputs":[],"source":["class AttFlat(nn.Module):\n","    def __init__(self, opts):\n","        super(AttFlat, self).__init__()\n","        self.opts = opts\n","\n","        self.mlp = MLP(\n","            in_size=opts.hiddenDim,\n","            mid_size=opts.FlatMLPSize,\n","            out_size=opts.FlatGlimpses,\n","            dropout_r=opts.dropout,\n","            use_relu=True\n","        )\n","        # FLAT_GLIMPSES = 1\n","        self.linear_merge = nn.Linear(\n","            opts.hiddenDim * opts.FlatGlimpses,\n","            opts.FlatOutSize\n","        )\n","\n","    def forward(self, x, x_mask):\n","        att = self.mlp(x)\n","        att = att.masked_fill(\n","            x_mask.squeeze(1).squeeze(1).unsqueeze(2),\n","            -1e9\n","        )\n","        att = F.softmax(att, dim=1)\n","\n","        att_list = []\n","        for i in range(self.opts.FlatGlimpses):\n","            att_list.append(\n","                torch.sum(att[:, :, i: i + 1] * x, dim=1)\n","            )\n","\n","        x_atted = torch.cat(att_list, dim=1)\n","        x_atted = self.linear_merge(x_atted)\n","\n","        return x_atted"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.096965Z","iopub.status.busy":"2024-02-14T17:58:33.096681Z","iopub.status.idle":"2024-02-14T17:58:33.112178Z","shell.execute_reply":"2024-02-14T17:58:33.111319Z","shell.execute_reply.started":"2024-02-14T17:58:33.096942Z"},"trusted":true},"outputs":[],"source":["class QuestEncoder_1(nn.Module):\n","    \"\"\"\n","        Concat encoder\n","    \"\"\"\n","    def __init__(self, opts, textVocabSize):\n","        super(QuestEncoder_1, self).__init__()\n","        bidirectional = opts.bidirectional > 0\n","\n","        self.embedding = nn.Embedding(textVocabSize, opts.embedDim)\n","        self.lstmQ = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            bidirectional=bidirectional,\n","            batch_first=True\n","        )\n","\n","        self.lstmH = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            bidirectional=bidirectional,\n","            batch_first=True)\n","\n","        if bidirectional:\n","            opts.hiddenDim *= 2\n","            opts.hiddenSizeHead *= 2\n","            opts.FlatOutSize *= 2\n","        self.attQues = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n","        self.attHist = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n","\n","        self.attFlatQuest = AttFlat(opts)\n","        self.fc = nn.Linear(2 * opts.hiddenDim, opts.hiddenDim)\n","\n","    def forward(self, quest, hist):\n","        questMask = self.make_mask(quest.unsqueeze(2))\n","        histMask = self.make_mask(hist.unsqueeze(2))\n","\n","        # quest = F.tanh(self.embedding(quest))\n","        quest = self.embedding(quest)\n","\n","        quest, (_, _) = self.lstmQ(quest)\n","        questO = quest.detach().clone()\n","\n","        hist = self.embedding(hist)\n","        hist, (_, _) = self.lstmH(hist)\n","\n","        for attQ, attH in zip(self.attQues, self.attHist):\n","            quest = attQ(quest, questMask)\n","            hist = attH(hist, histMask)\n","        # (batchSize, 512)\n","        quest = self.attFlatQuest(quest, questMask)\n","\n","        # hist: (batchSize, length, 512)\n","        attWeights = torch.sum(torch.mul(hist, quest.unsqueeze(1)), -1)\n","        attWeights = torch.softmax(attWeights, -1)\n","        hist = torch.sum(torch.mul(hist, attWeights.unsqueeze(2)), 1)\n","        encOut = self.fc(torch.cat([quest, hist], -1))\n","\n","        return encOut, questO\n","\n","    # Masking\n","    def make_mask(self, feature):\n","        return (torch.sum(\n","            torch.abs(feature),\n","            dim=-1\n","        ) == 0).unsqueeze(1).unsqueeze(2)"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.113913Z","iopub.status.busy":"2024-02-14T17:58:33.113622Z","iopub.status.idle":"2024-02-14T17:58:33.127387Z","shell.execute_reply":"2024-02-14T17:58:33.126518Z","shell.execute_reply.started":"2024-02-14T17:58:33.113888Z"},"trusted":true},"outputs":[],"source":["class QuestEncoder_2(nn.Module):\n","    \"\"\"\n","        Stack encoder\n","    \"\"\"\n","    def __init__(self, opts, textVocabSize):\n","        super(QuestEncoder_2, self).__init__()\n","        bidirectional = opts.bidirectional > 0\n","        self.embedding = nn.Embedding(textVocabSize, opts.embedDim)\n","        self.lstmQ = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            batch_first=True,\n","            bidirectional=bidirectional,\n","        )\n","\n","        self.lstmH = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            batch_first=True,\n","            bidirectional=bidirectional,\n","        )\n","        if bidirectional:\n","            opts.hiddenDim *= 2\n","\n","        self.fc = nn.Linear(2 * opts.hiddenDim, opts.hiddenDim)\n","\n","    def forward(self, quest, hist):\n","\n","        quest = F.tanh(self.embedding(quest))\n","        quest, (questH, _) = self.lstmQ(quest)\n","\n","        # concatenate the last hidden states from the forward and backward pass\n","        # of the bidirectional lstm\n","        lastHiddenForward = questH[1:2, :, :].squeeze(0)\n","        lastHiddenBackward = questH[3:4, :, :].squeeze(0)\n","\n","        # questH: (batchSize, 512)\n","        questH = torch.cat([lastHiddenForward, lastHiddenBackward], -1)\n","\n","        questO = quest.detach().clone()\n","\n","        hist = F.tanh(self.embedding(hist))\n","        numRounds = hist.size(1)\n","        histFeat = []\n","        for i in range(numRounds):\n","            round_i = hist[:, i, :, :]\n","            _, (round_i_h, _) = self.lstmH(round_i)\n","\n","            #Same as before\n","            lastHiddenForward = round_i_h[1:2, :, :].squeeze(0)\n","            lastHiddenBackward = round_i_h[3:4, :, :].squeeze(0)\n","            histFeat.append(torch.cat([lastHiddenForward, lastHiddenBackward], -1))\n","\n","        # hist: (batchSize, rounds, 512)\n","        histFeat = torch.stack(histFeat, 1)\n","        attWeights = torch.sum(torch.mul(histFeat, questH.unsqueeze(1)), -1)\n","        attWeights = torch.softmax(attWeights, -1)\n","        histFeat = torch.sum(torch.mul(histFeat, attWeights.unsqueeze(2)), 1)\n","        encOut = self.fc(torch.cat([questH, histFeat], -1))\n","        return encOut, questO"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.129100Z","iopub.status.busy":"2024-02-14T17:58:33.128828Z","iopub.status.idle":"2024-02-14T17:58:33.146992Z","shell.execute_reply":"2024-02-14T17:58:33.146025Z","shell.execute_reply.started":"2024-02-14T17:58:33.129067Z"},"trusted":true},"outputs":[],"source":["#*** ->for qiansu: copy and paste the whole class\n","class Decoder(nn.Module):\n","    def __init__(self, opts, progVocabSize, maxLen, startID=1, endID=2):\n","        super(Decoder, self).__init__()\n","        self.numLayers = opts.numLayers\n","        self.bidirectional = opts.bidirectional > 0\n","        self.maxLen = maxLen\n","        self.startID = startID\n","        self.endID = endID\n","\n","        self.embedding = nn.Embedding(progVocabSize, opts.embedDim)\n","        self.lstmProg = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=2*opts.hiddenDim if self.bidirectional else opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            batch_first=True,\n","            #bidirectional=self.bidirectional,#???????\n","        )\n","        hiddenDim = opts.hiddenDim\n","        if self.bidirectional:\n","            hiddenDim *= 2\n","\n","        self.fcAtt = nn.Linear(2*hiddenDim, hiddenDim)\n","        self.fcOut = nn.Linear(hiddenDim, progVocabSize)\n","\n","    def initPrgHidden(self, encOut):\n","        hidden = [encOut for _ in range(self.numLayers)]\n","        hidden = torch.stack(hidden, 0).contiguous()\n","        return hidden, hidden\n","\n","    def forwardStep(self, prog, progH, questO):\n","        #**********************************************our error relates to this prog cause in our case it is not acting as tensor anymore.\n","        batchSize = prog.size(0)\n","        inputDim = questO.size(1)\n","        prog = self.embedding(prog)\n","        outProg, progH = self.lstmProg(prog, progH)\n","\n","        att = torch.bmm(outProg, questO.transpose(1, 2))\n","        att = F.softmax(att.view(-1, inputDim), 1).view(batchSize, -1, inputDim)\n","        context = torch.bmm(att, questO)\n","        # (batchSize, progLength, hiddenDim)\n","        out = F.tanh(self.fcAtt(torch.cat([outProg, context], dim=-1)))\n","\n","        # (batchSize, progLength, progVocabSize)\n","        out = self.fcOut(out)\n","        predSoftmax = F.log_softmax(out, 2)\n","        return predSoftmax, progH\n","\n","    def forward(self, prog, encOut, questO):\n","        progH = self.initPrgHidden(encOut)\n","        predSoftmax, progH = self.forwardStep(prog, progH, questO)\n","\n","        return predSoftmax, progH\n","\n","    def sample(self, encOut, questO):\n","        batchSize = encOut.size(0)\n","        cudaFlag = encOut.is_cuda\n","        progH = self.initPrgHidden(encOut)\n","        # prog = progCopy[:, 0:3]\n","        prog = torch.LongTensor(batchSize, 1).fill_(self.startID)\n","        # prog = torch.cat((progStart, progEnd), -1)\n","        if cudaFlag:\n","            prog = prog.cuda()\n","        outputLogProbs = []\n","        outputTokens = []\n","     \n","\n","        def decode(i, output):\n","            tokens = output.topk(1, dim=-1)[1].view(batchSize, -1)\n","            #print(\"This is inside of the decode local function and this is the tockens=\", tokens)\n","            return tokens\n","\n","        for i in range(self.maxLen):\n","            predSoftmax, progH = self.forwardStep(prog, progH, questO)\n","            prog = decode(i, predSoftmax)\n","            prog_flat = list(chain(*prog))\n","            flat_list = [item.item() for item in prog_flat]\n","\n","        #****************************************my modification\n","            outputTokens.append(flat_list)#new\n","       #print(\"lets check what is inside outputTocken\", outputTokens)    \n","       # print(\"-----------------------------------------\")\n","        return outputTokens, outputLogProbs\n"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.148474Z","iopub.status.busy":"2024-02-14T17:58:33.148161Z","iopub.status.idle":"2024-02-14T17:58:33.160023Z","shell.execute_reply":"2024-02-14T17:58:33.159220Z","shell.execute_reply.started":"2024-02-14T17:58:33.148446Z"},"trusted":true},"outputs":[],"source":["class SeqToSeqC(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(SeqToSeqC, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, cap, prog):\n","        encOut, capO = self.encoder(cap)\n","        predSoftmax, progHC = self.decoder(prog, encOut, capO)\n","        return predSoftmax, progHC\n","   \n","    def sample(self, cap):\n","        with torch.no_grad():\n","            encOut, capO = self.encoder(cap)\n","        outputTokens, outputLogProbs = self.decoder.sample(encOut, capO)\n","        #***************************************************************** Added by Sepi to avoid returning an empty torchlist.\n","        #if not outputTokens:\n","          #  print(\"***\")\n","        # Handle the case where outputTokens is empty, for example, return a placeholder tensor\n","           # return torch.tensor([])\n","        #***************************************************************** \n","        #outputTokens = torch.stack(outputTokens, 0).transpose(0, 1)\n","        #outputTokens = torch.stack(outputTokens, dim=0).transpose(0, 1)\n","        outputTokens_t = [[row[i] for row in outputTokens] for i in range(len(outputTokens[0]))]#***transpose \n","        return outputTokens_t\n"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.161606Z","iopub.status.busy":"2024-02-14T17:58:33.161113Z","iopub.status.idle":"2024-02-14T17:58:33.172742Z","shell.execute_reply":"2024-02-14T17:58:33.171710Z","shell.execute_reply.started":"2024-02-14T17:58:33.161554Z"},"trusted":true},"outputs":[],"source":["class SeqToSeqQ(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(SeqToSeqQ, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, quest, hist, prog):\n","        encOut, questO = self.encoder(quest, hist)\n","        predSoftmax, progHC = self.decoder(prog, encOut, questO)\n","        return predSoftmax, progHC\n","\n","    def sample(self, quest, hist):\n","        with torch.no_grad():\n","            encOut, questO = self.encoder(quest, hist)\n","            outputTokens, outputLogProbs = self.decoder.sample(encOut, questO)\n","      \n","        outputTokens_t = [[row[i] for row in outputTokens] for i in range(len(outputTokens[0]))]#***transpose \n","        return outputTokens_t"]},{"cell_type":"markdown","metadata":{},"source":["## **optim.py**"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.174223Z","iopub.status.busy":"2024-02-14T17:58:33.173931Z","iopub.status.idle":"2024-02-14T17:58:33.181256Z","shell.execute_reply":"2024-02-14T17:58:33.180494Z","shell.execute_reply.started":"2024-02-14T17:58:33.174200Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.optim as Optim\n","from itertools import chain #***"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.182481Z","iopub.status.busy":"2024-02-14T17:58:33.182202Z","iopub.status.idle":"2024-02-14T17:58:33.194164Z","shell.execute_reply":"2024-02-14T17:58:33.193343Z","shell.execute_reply.started":"2024-02-14T17:58:33.182458Z"},"trusted":true},"outputs":[],"source":["class WarmupOptimizer(object):\n","    def __init__(self, lr_base, optimizer, data_size, batch_size):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.lr_base = lr_base\n","        self._rate = 0\n","        self.data_size = data_size\n","        self.batch_size = batch_size\n","\n","    def step(self):\n","        self._step += 1\n","\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","\n","        self.optimizer.step()\n","\n","    def zero_grad(self):\n","        self.optimizer.zero_grad()\n","\n","    def rate(self, step=None):\n","        if step is None:\n","            step = self._step\n","\n","        if step <= int(self.data_size / self.batch_size * 1):\n","            r = self.lr_base * 1/2.\n","        else:\n","            r = self.lr_base\n","\n","        return r\n","\n","\n","def get_optim(opts, model, data_size, lr_base=None):\n","    if lr_base is None:\n","        lr_base = opts.lr\n","\n","    if opts.optim == 'adam':\n","        optim = Optim.Adam(\n","                filter(lambda p: p.requires_grad, model.parameters()),\n","                lr=0,\n","                betas=opts.betas,\n","                eps=opts.eps,\n","\n","            )\n","    elif opts.optim == 'rmsprop':\n","        optim = Optim.RMSprop(\n","                filter(lambda p: p.requires_grad, model.parameters()),\n","                lr=0,\n","                eps=opts.eps,\n","                weight_decay=opts.weight_decay\n","            )\n","    else:\n","        raise ValueError('{} optimizer is not supported'.fromat(opts.optim))\n","    return WarmupOptimizer(\n","        lr_base,\n","        optim,\n","        data_size,\n","        opts.batch_size\n","    )\n","\n","def adjust_lr(optim, decay_r):\n","    optim.lr_base *= decay_r\n"]},{"cell_type":"markdown","metadata":{},"source":["## **option_question_parser.py**"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.195797Z","iopub.status.busy":"2024-02-14T17:58:33.195370Z","iopub.status.idle":"2024-02-14T17:58:33.206502Z","shell.execute_reply":"2024-02-14T17:58:33.205529Z","shell.execute_reply.started":"2024-02-14T17:58:33.195765Z"},"trusted":true},"outputs":[],"source":["import argparse\n","import os\n","#import utils_m\n","import torch"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.208916Z","iopub.status.busy":"2024-02-14T17:58:33.208483Z","iopub.status.idle":"2024-02-14T17:58:33.247567Z","shell.execute_reply":"2024-02-14T17:58:33.246612Z","shell.execute_reply.started":"2024-02-14T17:58:33.208882Z"},"trusted":true},"outputs":[],"source":["class OptionsQ():#changed optiopn class as Option_q to differentiate it with the one belong to caption\n","    def __init__(self):\n","        self.parser = argparse.ArgumentParser()\n","        self.initialized = False\n","        \n","\n","    def initialize(self):\n","        self.parser.add_argument(\n","            '--mode',\n","            default='test_with_gt',#***\n","            type=str,\n","            #choices=['train', 'test_with_gt', 'test_with_pred'],\n","            help='The mode of the experiment')\n","\n","        self.parser.add_argument(\n","            '--run_dir',\n","            #required=True,\n","            default= 'train_concat',\n","            type=str,\n","            help='The experiment directory')\n","        #***\n","        self.parser.add_argument(\n","            '--useCuda',\n","            default=1,\n","            type=int,\n","            help='To be able to use cuda')\n","\n","        self.parser.add_argument(\n","            '--text_log_dir',\n","            #required=True,\n","            default=\"train_concat/res.txt\",\n","            type=str,\n","            help='File to save the logged text')\n","\n","        self.parser.add_argument(\n","            '--questionNetPath',\n","            #default='',\n","            default = '/kaggle/input/nsvd-dataset/ckpt_iter3000.pkl',\n","            type=str,\n","            help='Path to the pretrained QuestionNet that will be used for testing.')\n","\n","        self.parser.add_argument(\n","            '--captionNetPath',\n","            default= '/kaggle/input/nsvd-dataset/ckpt_iter5000.pkl',#***\n","            type=str,\n","            help='Path to the pretrained CaptionNet that will be used for testing.')\n","\n","        self.parser.add_argument(\n","            '--dialogLen',\n","            default=10,\n","            type=int,\n","            help='Length of the dialogs to be used for testing. We used 10, 15, and 20 in our experiments.')\n","\n","        self.parser.add_argument(\n","            '--last_n_rounds',\n","            default=10,\n","            type=int,\n","            help='Number of the last rounds to consider in the history. We used 1, 2, 3, 4, and 10 in our experiments. ')\n","\n","        self.parser.add_argument(\n","            '--encoderType',\n","            #required=True,\n","            default=1,#***-> this is for question-concat\n","            type=int,\n","            choices=[1, 2],\n","            help='Type of the encoder: 1 --> Concat, 2 --> Stack')\n","\n","        self.parser.add_argument(\n","            '--load_checkpoint_path',\n","            default='None',\n","            type=str,\n","            help='Path to a QestionNet checkpoint path to resume training')\n","\n","        self.parser.add_argument(\n","            '--gpu_ids',\n","            default='0',\n","            type=str,\n","            help='Id of the gpu to be used')\n","\n","        self.parser.add_argument(\n","            '--seed',\n","            default=42,\n","            type=int,\n","            help='The seed used in training')\n","\n","        self.parser.add_argument(\n","            '--dataPathTr',\n","            #required=True,\n","            default=\"/kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/train_concat_half.h5\",\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed training data')\n","\n","        self.parser.add_argument(\n","            '--dataPathVal',\n","            #required=True,\n","            default=\"/kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/val_concat_half.h5\",\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed validation data')\n","\n","        self.parser.add_argument(\n","            '--dataPathTest',\n","            #required=True,\n","            default = \"/kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/test_concat_75000.h5\",\n","            type=str,\n","            help='Path to the h5 file of the Clevr-Dialog preprocessed test data')\n","\n","        self.parser.add_argument(\n","            '--scenesPath',\n","            #required=True,\n","            default='/kaggle/input/d/sepibhm/nsvd-dataset/data/CLEVR_scenes_test.json',\n","            type=str,\n","            help='Path to the derendered clevr-dialog scenes')\n","\n","\n","        self.parser.add_argument(\n","            '--vocabTestPath',\n","            #required=True,\n","            default = \"/kaggle/input/d/sepibhm/nsvd-dataset/test_concat/vocab_output.json\",\n","            type=str,\n","            help='Path to the test vocabulary')\n","\n","\n","        \n","\n","        self.parser.add_argument(\n","            '--vocabPath',\n","            #required=True,\n","            default = \"/kaggle/input/d/sepibhm/nsvd-dataset/train_concat/vocab_output.json\",\n","            type=str,\n","            help='Path to the generated vocabulary')\n","\n","        self.parser.add_argument(\n","            '--batch_size',\n","            #default=64,\n","            default=32,\n","            type=int,\n","            help='Batch size')\n","\n","        self.parser.add_argument(\n","            '--countFirstFailueRound',\n","            default=0,\n","            type=int,\n","            help='If activated, we count the first failure round')\n","\n","        self.parser.add_argument(\n","            '--maxSamples',\n","            default=-1,\n","            type=int,\n","            help='Maximum number of training samples')\n","\n","        self.parser.add_argument(\n","            '--num_workers',\n","            default=0,\n","            type=int,\n","            help='Number of workers for loading')\n","\n","        self.parser.add_argument(\n","            '--num_iters',\n","            default=TOTAL_ITER,\n","            type=int,\n","            help='Total number of iterations')\n","\n","        self.parser.add_argument(\n","            '--display_every',\n","            default=5,\n","            type=int,\n","            help='Display training information every N iterations')\n","\n","        self.parser.add_argument(\n","            '--validate_every',\n","            default=VALID_EVE,\n","            type=int,\n","            help='Validate every N iterations')\n","\n","        self.parser.add_argument(\n","            '--shuffle_data',\n","            default=1,\n","            type=int,\n","            help='Activate to shuffle the training data')\n","\n","        self.parser.add_argument(\n","            '--optim',\n","            default='adam',\n","            type=str,\n","            help='The name of the optimizer to be used')\n","\n","        self.parser.add_argument(\n","            '--lr',\n","            default=1e-3,\n","            type=float,\n","            help='Base learning rate')\n","\n","        self.parser.add_argument(\n","            '--betas',\n","            default='0.9, 0.98',\n","            type=str,\n","            help='Adam optimizer\\'s betas')\n","\n","        self.parser.add_argument(\n","            '--eps',\n","            default='1e-9',\n","            type=float,\n","            help='Adam optimizer\\'s epsilon')\n","\n","        self.parser.add_argument(\n","            '--lr_decay_marks',\n","            default='50000, 55000',\n","            type=str,\n","            help='Learing rate decay marks')\n","\n","        self.parser.add_argument(\n","            '--lr_decay_factor',\n","            default=0.5,\n","            type=float,\n","            help='Learning rate decay factor')\n","\n","        self.parser.add_argument(\n","            '--weight_decay',\n","            default=1e-6,\n","            type=float,\n","            help='Weight decay')\n","\n","        self.parser.add_argument(\n","            '--embedDim',\n","            default=300,\n","            type=int,\n","            help='Embedding dimension')\n","\n","        self.parser.add_argument(\n","            '--hiddenDim',\n","            default=512,\n","            type=int,\n","            help='LSTM hidden dimension')\n","\n","        self.parser.add_argument(\n","            '--numLayers',\n","            default=2,\n","            type=int,\n","            help='Number of hidden LSTM layers')\n","\n","        self.parser.add_argument(\n","            '--dropout',\n","            default=0.1,\n","            type=float,\n","            help='Dropout value')\n","\n","        self.parser.add_argument(\n","            '--multiHead',\n","            default=8,\n","            type=int,\n","            help='Number of attention heads')\n","\n","        self.parser.add_argument(\n","            '--hiddenSizeHead',\n","            default=64,\n","            type=int,\n","            help='Dimension of each attention head')\n","\n","        self.parser.add_argument(\n","            '--FeedForwardSize',\n","            default=2048,\n","            type=int,\n","            help='Dimension of the feed forward layer')\n","\n","        self.parser.add_argument(\n","            '--FlatMLPSize',\n","            default=512,\n","            type=int,\n","            help='MLP flatten size')\n","\n","        self.parser.add_argument(\n","            '--FlatGlimpses',\n","            default=1,\n","            type=int,\n","            help='Number of flatten glimpses')\n","\n","        self.parser.add_argument(\n","            '--FlatOutSize',\n","            default=512,\n","            type=int,\n","            help='Final attention reduction dimension')\n","\n","        self.parser.add_argument(\n","            '--layers',\n","            default=6,\n","            type=int,\n","            help='Number of self attention layers')\n","\n","        self.parser.add_argument(\n","            '--bidirectional',\n","            default=1,\n","            type=int,\n","            help='Activate to use bidirectional LSTMs')\n","\n","        self.initialized = True\n","\n","    def parse(self):\n","        # initialize parser\n","        if not self.initialized:\n","            self.initialize()\n","        #self.opts = self.parser.parse_args()#***\n","        self.opts, unknown = self.parser.parse_known_args()#this is added by me to fix the error of command line arguments.\n","        # parse gpu id list\n","        str_gpu_ids = self.opts.gpu_ids.split(',')\n","        self.opts.gpu_ids = []\n","        for str_id in str_gpu_ids:\n","            if str_id.isdigit() and int(str_id) >= 0:\n","                self.opts.gpu_ids.append(int(str_id))\n","        if len(self.opts.gpu_ids) > 0 and torch.cuda.is_available():\n","            print('\\n[INFO] Using {} CUDA device(s) ...'.format(\n","                len(self.opts.gpu_ids)))\n","        else:\n","            print('\\n[INFO] Using cpu ...')\n","            self.opts.gpu_ids = []\n","\n","        # parse the optimizer's betas and lr decay marks\n","        self.opts.betas = [float(beta) for beta in self.opts.betas.split(',')]\n","        lr_decay_marks = [int(m) for m in self.opts.lr_decay_marks.split(',')]\n","        for i in range(1, len(lr_decay_marks)):\n","            assert lr_decay_marks[i] > lr_decay_marks[i-1]\n","        self.opts.lr_decay_marks = lr_decay_marks\n","\n","        # print and save options\n","        args = vars(self.opts)\n","        print('\\n ' + 30*'-' + 'Opts' + 30*'-')\n","        for k, v in args.items():\n","            print('%s: %s' % (str(k), str(v)))\n","\n","        if not os.path.isdir(self.opts.run_dir):\n","            os.makedirs(self.opts.run_dir)\n","        filename = 'opts.txt'\n","        file_path = os.path.join(self.opts.run_dir, filename)\n","        with open(file_path, 'wt') as fout:\n","            fout.write('| options\\n')\n","            for k, v in sorted(args.items()):\n","                fout.write('%s: %s\\n' % (str(k), str(v)))\n","        return self.opts\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Train_question_parser**"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.249133Z","iopub.status.busy":"2024-02-14T17:58:33.248831Z","iopub.status.idle":"2024-02-14T17:58:33.260080Z","shell.execute_reply":"2024-02-14T17:58:33.259207Z","shell.execute_reply.started":"2024-02-14T17:58:33.249108Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import json, torch, pickle, copy, time\n","import numpy as np\n","import argparse\n","import torch.nn as nn\n","import torch.utils.data as Data\n","from tensorboardX import SummaryWriter\n","from copy import deepcopy\n","#from clevrDialog_dataset import ClevrDialogQuestionDataset\n","import pickle\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.261500Z","iopub.status.busy":"2024-02-14T17:58:33.261239Z","iopub.status.idle":"2024-02-14T17:58:33.272610Z","shell.execute_reply":"2024-02-14T17:58:33.271697Z","shell.execute_reply.started":"2024-02-14T17:58:33.261477Z"},"trusted":true},"outputs":[],"source":["class CaptionEncoder(nn.Module):\n","    def __init__(self, opts, textVocabSize):\n","        super(CaptionEncoder, self).__init__()\n","        self.embedding = nn.Embedding(textVocabSize, opts.embedDim)\n","        bidirectional = opts.bidirectional > 0\n","        self.lstmC = nn.LSTM(\n","            input_size=opts.embedDim,\n","            hidden_size=opts.hiddenDim,\n","            num_layers=opts.numLayers,\n","            batch_first=True,\n","            bidirectional=bidirectional\n","        )\n","        if bidirectional:\n","            opts.hiddenDim *= 2\n","            opts.hiddenSizeHead *= 2\n","            opts.FlatOutSize *= 2\n","\n","        self.attCap = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n","        self.attFlatCap = AttFlat(opts)\n","        self.fc = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n","\n","    def forward(self, cap, hist=None):\n","        capMask = self.make_mask(cap.unsqueeze(2))\n","        cap = self.embedding(cap)\n","        cap, (_, _) = self.lstmC(cap)\n","        capO = cap.detach().clone()\n","\n","        for attC in self.attCap:\n","            cap = attC(cap, capMask)\n","        # (batchSize, 512)\n","        cap = self.attFlatCap(cap, capMask)\n","        encOut = self.fc(cap)\n","        return encOut, capO\n","    \n","    # Masking\n","    def make_mask(self, feature):\n","        return (torch.sum(\n","            torch.abs(feature),\n","            dim=-1\n","        ) == 0).unsqueeze(1).unsqueeze(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.274050Z","iopub.status.busy":"2024-02-14T17:58:33.273769Z","iopub.status.idle":"2024-02-14T17:58:33.353652Z","shell.execute_reply":"2024-02-14T17:58:33.352611Z","shell.execute_reply.started":"2024-02-14T17:58:33.274021Z"},"trusted":true},"outputs":[],"source":["class Execution:\n","    def __init__(self, optsQ, optsC):\n","        self.opts = deepcopy(optsQ)\n","        if self.opts.useCuda > 0 and torch.cuda.is_available():\n","            self.device = torch.device(\"cuda:0\")\n","            print(\"[INFO] Using GPU {} ...\".format(torch.cuda.get_device_name(0)))\n","        else:\n","            print(\"[INFO] Using CPU ...\")\n","            self.device = torch.device(\"cpu\")\n","\n","        self.loss_fn = torch.nn.NLLLoss().to(self.device)\n","\n","        print(\"[INFO] Loading dataset ...\")\n","\n","        self.datasetTr = ClevrDialogQuestionDataset(\n","            self.opts.dataPathTr, self.opts.vocabPath, \"train\", \"All tr data\")\n","\n","        self.datasetVal = ClevrDialogQuestionDataset(\n","            self.opts.dataPathVal, self.opts.vocabPath, \"val\", \"All val data\", train=False)\n","\n","        self.datasetTest = ClevrDialogQuestionDataset(\n","            self.opts.dataPathTest, self.opts.vocabTestPath, \"test\", \"All val data\", train=False)\n","        \n","\n","\n","     \n","        self.QuestionNet = constructQuestionNet(\n","            self.opts,\n","            self.datasetTr.lenVocabText,\n","            self.datasetTr.lenVocabProg,\n","            self.datasetTr.maxLenProg,\n","            #self.datasetTest.lenVocabText,#*** to solve mismatch problems\n","            #self.datasetTest.lenVocabProg,#***\n","            #self.datasetTest.maxLenProg#***\n","            )\n","\n","        if os.path.isfile(self.opts.captionNetPath):\n","            self.CaptionNet = constructCaptionNet(\n","                optsC,\n","                self.datasetTr.lenVocabText,\n","                self.datasetTr.lenVocabProg,\n","                self.datasetTr.maxLenProg,\n","                #self.datasetTest.lenVocabText,#*** \n","                #self.datasetTest.lenVocabProg,#***\n","                #self.datasetTest.maxLenProg#***\n","                )\n","            print('Loading CaptionNet from {}'.format(self.opts.captionNetPath))\n","            state_dict = torch.load(self.opts.captionNetPath)['state_dict']\n","            self.CaptionNet.load_state_dict(state_dict)\n","            self.CaptionNet.to(self.device)\n","            total_params_cap = sum(p.numel() for p in self.CaptionNet.parameters() if p.requires_grad)\n","            print(\"The caption encoder has {} trainable parameters\".format(total_params_cap))\n","\n","        self.QuestionNet.to(self.device)\n","        if os.path.isfile(self.opts.questionNetPath):\n","            print('Loading QuestionNet from {}'.format(optsQ.questionNetPath))\n","            state_dict = torch.load(self.opts.questionNetPath)['state_dict']\n","            self.QuestionNet.load_state_dict(state_dict)\n","        total_params_quest = sum(p.numel() for p in self.QuestionNet.parameters() if p.requires_grad)\n","        print(\"The question encoder has {} trainable parameters\".format(total_params_quest))\n","\n","        if \"minecraft\" in self.opts.scenesPath:\n","            self.symbolicExecutor = SymbolicExecutorMinecraft(self.opts.scenesPath)\n","        else:\n","            self.symbolicExecutor = SymbolicExecutorClevr(self.opts.scenesPath)\n","\n","        tb_path = os.path.join(self.opts.run_dir, \"tb_logdir\")\n","        if not os.path.isdir(tb_path):\n","            os.makedirs(tb_path)\n","\n","        self.ckpt_path = os.path.join(self.opts.run_dir, \"ckpt_dir\")\n","        if not os.path.isdir(self.ckpt_path):\n","            os.makedirs(self.ckpt_path)\n","        if not os.path.isdir(self.opts.text_log_dir):\n","            os.makedirs(self.opts.text_log_dir)\n","\n","        self.writer = SummaryWriter(tb_path)\n","        self.iter_val = 0\n","#***\n","        #if os.path.isfile(self.opts.dependenciesPath):\n","            #with open(self.opts.dependenciesPath, \"rb\") as f:\n","                #self.dependencies = pickle.load(f)\n","\n","\n","\n","    # Evaluation\n","    def eval_with_gt(self):\n","        # Define the multi-gpu training if needed\n","        all_pred_answers = []\n","        all_gt_answers = []\n","        all_question_types = []\n","        all_penalties = []\n","        all_pred_programs = []\n","        all_gt_programs = []\n","        first_failure_round = 0\n","        total_correct = 0\n","        total_acc_pen = 0\n","        total = 0\n","        total_quest_prog_correct = 0\n","\n","        if len(self.opts.gpu_ids) > 1:\n","            self.QuestionNet = nn.DataParallel(self.QuestionNet, device_ids=self.opts.gpu_ids)\n","        self.QuestionNet = self.QuestionNet.eval()\n","        self.CaptionNet = self.CaptionNet.eval()\n","        if self.opts.batch_size != self.opts.dialogLen:\n","            print(\"[INFO] Changed batch size from {} to {}\".format(self.opts.batch_size, self.opts.dialogLen))\n","            self.opts.batch_size = self.opts.dialogLen\n","        dataloader = Data.DataLoader(\n","            self.datasetTest,\n","            batch_size=self.opts.batch_size,\n","            shuffle=False,\n","            num_workers=self.opts.num_workers,\n","            pin_memory=False\n","        )\n","        _iterCur = 0\n","        _totalCur = len(dataloader)\n","\n","        for step, (question, questionPrg, questionImgIdx, questionRounds, history, historiesProg, answer) in enumerate(dataloader):\n","            # print(\"\\rEvaluation: [step %4d/%4d]\" % (\n","            #     step + 1,\n","            #     int(data_size / self.opts.batch_size),\n","            # ), end='          ')\n","            # if step >= 5000:\n","            #     break\n","            batchSize = question.size(0)\n","            question = question.to(self.device)\n","            # dependecy = self.dependencies[step*batchSize:(step+1)*batchSize]\n","\n","            if history.dim() == 3:\n","                caption = history.detach()\n","                caption = caption[:, 0, :]\n","                caption = caption[:, :16].to(self.device)\n","            elif history.dim() == 2:\n","                caption = history.detach()\n","                caption = caption[:, :16].to(self.device)\n","            if self.opts.last_n_rounds < 10:\n","                last_n_rounds_batch = []\n","                for i, r in enumerate(questionRounds.tolist()):\n","                    startIdx = max(r - self.opts.last_n_rounds, 0)\n","                    endIdx = max(r, self.opts.last_n_rounds)\n","                    if history.dim() == 3:\n","                        assert endIdx - startIdx == self.opts.last_n_rounds\n","                        histBatch = history[i, :, :]\n","                        last_n_rounds_batch.append(histBatch[startIdx:endIdx, :])\n","                    elif history.dim() == 2:\n","                        startIdx *= 20\n","                        endIdx *= 20\n","                        histBatch = history[i, :]\n","                        temp = histBatch[startIdx:endIdx]\n","                        if r > self.opts.last_n_rounds:\n","                            last_n_rounds_batch.append(torch.cat([torch.tensor([1]), temp, torch.tensor([2])], 0))\n","                        else:\n","                            last_n_rounds_batch.append(torch.cat([temp, torch.tensor([2, 0])], 0))\n","                history = torch.stack(last_n_rounds_batch, dim=0)\n","\n","            history = history.to(self.device)\n","            questionPrg = questionPrg.to(self.device)\n","            historiesProg = historiesProg.tolist()\n","            questionRounds = questionRounds.tolist()\n","            answer = answer.tolist()\n","            answers = list(map(lambda a: self.datasetTest.vocab[\"idx_text_to_token\"][a], answer))\n","            questionImgIdx = questionImgIdx.tolist()\n","            # if \"minecraft\" in self.opts.scenesPath:\n","            #     questionImgIdx = [idx - 1 for idx in questionImgIdx]\n","            questProgsToksPred = self.QuestionNet.sample(question, history)\n","            capProgsToksPred = self.CaptionNet.sample(caption)\n","\n","            questProgsPred = decodeProg(questProgsToksPred, self.datasetTest.vocab[\"idx_prog_to_token\"])\n","            capProgsPred = decodeProg(capProgsToksPred, self.datasetTest.vocab[\"idx_prog_to_token\"])\n","\n","            targetProgs = decodeProg(questionPrg, self.datasetTest.vocab[\"idx_prog_to_token\"], target=True)\n","            questionTypes = [targetProg[0] for targetProg in targetProgs]\n","            # progHistories = getProgHistories(historiesProg[0], dataset.vocab[\"idx_prog_to_token\"])\n","            progHistories = [getProgHistories(progHistToks, self.datasetTest.vocab[\"idx_prog_to_token\"]) for progHistToks in historiesProg]\n","            pred_answers = []\n","            all_pred_programs.append([capProgsPred[0]] + questProgsPred)\n","            all_gt_programs.append([progHistories[0]] + (targetProgs))\n","\n","            for i in range(batchSize):\n","              \n","                ans = self.getPrediction(\n","                    questProgsPred[i],\n","                    capProgsPred[i],\n","                    progHistories[i],\n","                    questionImgIdx[i]\n","                )\n","        \n","                    \n","\n","                pred_answers.append(ans)\n","\n","            correct = [1 if pred == ans else 0 for (pred, ans) in zip(pred_answers, answers)]\n","            correct_prog = [1 if pred == ans else 0 for (pred, ans) in zip(questProgsPred, targetProgs)]\n","            idx_false = np.argwhere(np.array(correct) == 0).squeeze(-1)\n","            if idx_false.shape[-1] > 0:\n","                first_failure_round += idx_false[0] + 1\n","            else:\n","                first_failure_round += self.opts.dialogLen + 1\n","\n","            correct = sum(correct)\n","            correct_prog = sum(correct_prog)\n","            total_correct += correct\n","            total_quest_prog_correct += correct_prog\n","            total += len(answers)\n","            all_pred_answers.append(pred_answers)\n","            all_gt_answers.append(answers)\n","            all_question_types.append(questionTypes)\n","            penalty = np.zeros_like(answers) \n","            all_penalties.append(penalty)\n","            _iterCur += 1\n","            if _iterCur % self.opts.display_every == 0:\n","                print(\"[Evaluation] step {0} / {1} | acc. = {2:.2f}\".format(\n","                    _iterCur, _totalCur, 100.0 * (total_correct / total)))\n","\n","        ffr = 1.0 * (first_failure_round/_totalCur)/(self.opts.dialogLen + 1)\n","\n","        textOut = \"\\n --------------- Average First Failure Round --------------- \\n\"\n","        textOut += \"{} / {}\".format(ffr, self.opts.dialogLen)\n","\n","        # print(total_correct, total)\n","        accuracy = total_correct / total\n","        vd_acc = total_acc_pen / total\n","        quest_prog_acc = total_quest_prog_correct / total\n","        textOut += \"\\n --------------- Overall acc. --------------- \\n\"\n","        textOut += \"{}\".format(100.0 * accuracy)\n","        textOut += \"\\n --------------- Overall VD acc. --------------- \\n\"\n","        textOut += \"{}\".format(100.0 * vd_acc)\n","        textOut += \"\\n --------------- Question Prog. Acc --------------- \\n\"\n","        textOut += \"{}\".format(100.0 * quest_prog_acc)\n","        textOut += get_per_round_acc(\n","            all_pred_answers, all_gt_answers, all_penalties)\n","\n","        textOut += get_per_question_type_acc(\n","            all_pred_answers, all_gt_answers, all_question_types, all_penalties)\n","\n","        # textOut += get_per_dependency_type_acc(\n","        #     all_pred_answers, all_gt_answers, all_penalties)\n","\n","        textOut += \"\\n --------------- Done --------------- \\n\"\n","        print(textOut)\n","\n","\n","\n","\n","    def getPrediction(self, questProgPred, capProgPred, historyProg, imgIndex):\n","        self.symbolicExecutor.reset(imgIndex)\n","        # if round one, execute the predicted caption program first then answer the question\n","        if len(historyProg) == 1:\n","            captionFuncLabel = capProgPred[0]\n","            captionFuncArgs = capProgPred[1:]\n","\n","            questionFuncLabel = questProgPred[0]\n","            questionFuncArgs = questProgPred[1:]\n","\n","            try:\n","                _ = self.symbolicExecutor.execute(captionFuncLabel, captionFuncArgs)\n","            except Exception as e:\n","                #print(\"Error is in caption This is the first round\",e)\n","               # print(captionFuncLabel, captionFuncArgs)\n","                return \"Error\"\n","\n","                \n","\n","            try:\n","                predAnswer = self.symbolicExecutor.execute(questionFuncLabel, questionFuncArgs)\n","            except Exception as e:\n","               # print (\"Error is in question. This is the first round \",e)\n","               # print(\"The question questionFuncLabel \",questionFuncLabel)\n","               # print(\"The questionFunc Args \",questionFuncArgs)\n","               # print(self.symbolicExecutor.execute(questionFuncLabel,questionFuncArgs))\n","\n","                return \"Error\"\n","\n","        # If it is not the first round, we have to execute the program history and\n","        # then answer the question.\n","        else:\n","            questionFuncLabel = questProgPred[0]\n","            questionFuncArgs = questProgPred[1:]\n","            for prg in historyProg:\n","                # prg = prg.split(\" \")\n","                FuncLabel = prg[0]\n","                FuncArgs = prg[1:]\n","                try:\n","                    _ = self.symbolicExecutor.execute(FuncLabel, FuncArgs)\n","                except:\n","                    #print(\"Error executing in history program:\", )\n","                    return \"Error\"\n","\n","            try:\n","                predAnswer = self.symbolicExecutor.execute(questionFuncLabel, questionFuncArgs)\n","            except Exception as e:\n","                #print(\"Error executing in current program:\", e)  #\n","\n","                return \"Error\"\n","        return str(predAnswer)\n","\n","    def run(self, run_mode, epoch=None):\n","        self.set_seed(self.opts.seed)\n","        if run_mode == 'test_with_gt':\n","            print('Testing with gt answers in history')\n","            print('Loading ckpt {}'.format(self.opts.questionNetPath))\n","            state_dict = torch.load(self.opts.questionNetPath)['state_dict']\n","            self.QuestionNet.load_state_dict(state_dict)\n","            self.eval_with_gt()\n","\n","        else:\n","            exit(-1)\n","\n","    def set_seed(self, seed):\n","        \"\"\"Sets the seed for reproducibility.\n","        Args:\n","            seed (int): The seed used\n","        \"\"\"\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        print('[INFO] Seed set to {}...'.format(seed))\n","\n","\n","def constructQuestionNet(opts, lenVocabText, lenVocabProg, maxLenProg):\n","    decoder = Decoder(opts, lenVocabProg, maxLenProg)\n","    if opts.encoderType == 1:\n","        encoder = QuestEncoder_1(opts, lenVocabText)\n","    elif opts.encoderType == 2:\n","        encoder = QuestEncoder_2(opts, lenVocabText)\n","\n","    net = SeqToSeqQ(encoder, decoder)\n","    return net\n","\n","\n","def constructCaptionNet(opts, lenVocabText, lenVocabProg, maxLenProg):\n","    decoder = Decoder(opts, lenVocabProg, maxLenProg)\n","    encoder = CaptionEncoder(opts, lenVocabText)\n","    net = SeqToSeqC(encoder, decoder)\n","    return net\n","\n","\n","def getProgHistories(progHistToks, prgIdxToToken):\n","    progHist = []\n","    temp = []\n","    for tok in progHistToks:\n","        if tok not in [0, 1, 2]:\n","            temp.append(prgIdxToToken[tok])\n","            # del progHistToks[i]\n","        if tok == 2:\n","            # del progHistToks[i]\n","            # progHist.append(\" \".join(temp))\n","            progHist.append(temp)\n","            temp = []\n","    return progHist\n","\n","\n","def getHistoriesFromStack(histToks, textIdxToToken):\n","    histories = \"\\n\"\n","    temp = []\n","    for i, roundToks in enumerate(histToks):\n","        for tok in roundToks:\n","            if tok not in [0, 1, 2]:\n","                temp.append(textIdxToToken[tok])\n","                # del progHistToks[i]\n","            if tok == 2:\n","                # del progHistToks[i]\n","                if i == 0:\n","                    histories += \" \".join(temp) + \".\\n\"\n","                else:\n","                    histories += \" \".join(temp[:-1]) + \"? | {}.\\n\".format(temp[-1])\n","                # histories.append(temp)\n","                temp = []\n","                break\n","    return histories\n","\n","\n","def getHistoriesFromConcat(histToks, textIdxToToken):\n","    histories = []\n","    temp = []\n","    for tok in histToks:\n","        if tok not in [0, 1, 2]:\n","            temp.append(textIdxToToken[tok])\n","            # del progHistToks[i]\n","        if tok == 2:\n","            # del progHistToks[i]\n","            histories.append(\" \".join(temp[:-1]) + \"? | {}\".format(temp[-1]))\n","            # histories.append(temp)\n","            temp = []\n","    return histories\n","\n","\n","def decodeProg(tokens, prgIdxToToken, target=False):\n","    #tokensBatch = tokens.tolist()\n","    if (target == True):#***\n","        tokensBatch = tokens.tolist()\n","    else:#***\n","        tokensBatch = tokens\n","    progsBatch = []\n","    for tokens in tokensBatch:\n","        prog = []\n","        for tok in tokens:\n","            if tok == 2:  # <END> has index 2\n","                break\n","            prog.append(prgIdxToToken.get(tok))\n","        if target:\n","            prog = prog[1:]\n","        # progsBatch.append(\" \".join(prog))\n","        progsBatch.append(prog)\n","    return progsBatch\n","\n","\n","def printPred(predSoftmax, gts, prgIdxToToken):\n","    assert predSoftmax.size(0) == gts.size(0)\n","    tokens = predSoftmax.topk(1)[1].squeeze(-1)\n","    tokens = tokens.tolist()\n","    gts = gts.tolist()\n","    message = \"\\n ------------------------ \\n\"\n","    for token, gt in zip(tokens, gts):\n","        message += \"Prediction: \"\n","        for tok in token:\n","            message += prgIdxToToken.get(tok) + \" \"\n","        message += \"\\n Target   : \"\n","        for tok in gt:\n","            message += prgIdxToToken.get(tok) + \" \"\n","        message += \"\\n ------------------------ \\n\"\n","    return message\n","\n","\n","def get_per_round_acc(preds, gts, penalties):\n","    res = {}\n","    for img_preds, img_gt, img_pen in zip(preds, gts, penalties):\n","        img_preds = list(img_preds)\n","        img_gt = list(img_gt)\n","        img_pen = list(img_pen)\n","        # print(img_pen)\n","        for i, (pred, gt, pen) in enumerate(zip(img_preds, img_gt, img_pen)):\n","            _round = str(i + 1)\n","            if _round not in res:\n","                res[_round] = {\n","                    \"correct\": 0,\n","                    \"all\": 0\n","                }\n","            res[_round][\"all\"] += 1\n","            if pred == gt:\n","                pen = 0.1\n","                res[_round][\"correct\"] += 0.5** float(pen)\n","\n","    textOut = \"\\n --------------- Per round Acc --------------- \\n\"\n","    for k in res:\n","        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res[k][\"correct\"]/res[k][\"all\"]))\n","    return textOut\n","\n","\n","def get_per_question_type_acc(preds, gts, qtypes, penalties):\n","    res1 = {}\n","    res2 = {}\n","\n","    for img_preds, img_gt, img_qtypes, img_pen in zip(preds, gts, qtypes, penalties):\n","        # img_preds = list(img_preds)\n","        # img_gt = list(img_gt)\n","        img_pen = list(img_pen)\n","        for pred, gt, temp, pen in zip(img_preds, img_gt, img_qtypes, img_pen):\n","            if temp not in res1:\n","                res1[temp] = {\n","                    \"correct\": 0,\n","                    \"all\": 0\n","                }\n","            temp_cat = temp.split(\"-\")[0]\n","            if temp_cat not in res2:\n","                res2[temp_cat] = {\n","                    \"correct\": 0,\n","                    \"all\": 0\n","                }\n","            res1[temp][\"all\"] += 1\n","            res2[temp_cat][\"all\"] += 1\n","\n","            if pred == gt:\n","                pen = 0.1 \n","                res1[temp][\"correct\"] += 0.5** float(pen)\n","                res2[temp_cat][\"correct\"] += 0.5** float(pen)\n","\n","    textOut = \"\\n --------------- Per question Type Acc --------------- \\n\"\n","    for k in res1:\n","        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res1[k][\"correct\"]/res1[k][\"all\"]))\n","\n","    textOut += \"\\n --------------- Per question Category Acc --------------- \\n\"\n","    for k in res2:\n","        textOut += \"{}: {} %\\n\".format(k, 100.0 * (res2[k][\"correct\"]/res2[k][\"all\"]))\n","    return textOut\n","\n","\n","def decode(tokens, prgIdxToToken, target=False):\n","    if type(tokens) != list:\n","        tokens = tokens.tolist()\n","\n","    progsBatch = []\n","    for token in tokens:\n","        prog = []\n","        for tok in token:\n","            if tok == 2:  # <END> has index 2\n","                break\n","            prog.append(prgIdxToToken.get(tok))\n","        if target:\n","            prog = prog[1:]\n","        # progsBatch.append(\" \".join(prog))\n","        progsBatch.append(prog)\n","    return progsBatch\n","\n","#if __name__ == \"__main__\":#***"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.355247Z","iopub.status.busy":"2024-02-14T17:58:33.354898Z","iopub.status.idle":"2024-02-14T17:58:33.369325Z","shell.execute_reply":"2024-02-14T17:58:33.368290Z","shell.execute_reply.started":"2024-02-14T17:58:33.355218Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[INFO] Using 1 CUDA device(s) ...\n","\n"," ------------------------------Opts------------------------------\n","mode: train\n","run_dir: caption_small\n","load_checkpoint_path: caption_small/ckpt_dir/ckpt_iter5000.pkl\n","res_path: caption_small/res.txt\n","gpu_ids: [0]\n","seed: 42\n","dataPathTr: /kaggle/input/cap_small/tr_cap_s.h5\n","dataPathVal: /kaggle/input/cap_small/val_cap_s.h5\n","dataPathTest: /kaggle/input/cap_small/test_cap_s.h5\n","vocabPath: /kaggle/input/caption/vocab_output_caption.json\n","batch_size: 64\n","num_workers: 0\n","num_iters: 5000\n","display_every: 5\n","debug_every: 100\n","validate_every: 1000\n","shuffle_data: 1\n","optim: adam\n","lr: 0.001\n","betas: [0.9, 0.98]\n","eps: 1e-09\n","lr_decay_marks: [50000, 55000]\n","lr_decay_factor: 0.5\n","weight_decay: 1e-06\n","embedDim: 300\n","hiddenDim: 512\n","numLayers: 2\n","dropout: 0.1\n","multiHead: 8\n","hiddenSizeHead: 64\n","FeedForwardSize: 2048\n","FlatMLPSize: 512\n","FlatGlimpses: 1\n","FlatOutSize: 512\n","layers: 6\n","bidirectional: 1\n"]}],"source":["optsC = OptionsC().parse()#***"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.370900Z","iopub.status.busy":"2024-02-14T17:58:33.370550Z","iopub.status.idle":"2024-02-14T17:58:33.380149Z","shell.execute_reply":"2024-02-14T17:58:33.379161Z","shell.execute_reply.started":"2024-02-14T17:58:33.370841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[INFO] Using 1 CUDA device(s) ...\n","\n"," ------------------------------Opts------------------------------\n","mode: test_with_gt\n","run_dir: train_concat\n","useCuda: 1\n","text_log_dir: train_concat/res.txt\n","questionNetPath: /kaggle/input/nsvd-dataset/ckpt_iter3000.pkl\n","captionNetPath: /kaggle/input/nsvd-dataset/ckpt_iter5000.pkl\n","dialogLen: 10\n","last_n_rounds: 10\n","encoderType: 1\n","load_checkpoint_path: None\n","gpu_ids: [0]\n","seed: 42\n","dataPathTr: /kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/train_concat_half.h5\n","dataPathVal: /kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/val_concat_half.h5\n","dataPathTest: /kaggle/input/d/sepibhm/nsvd-dataset/Small_Tr_Val_Test_Final/test_concat_75000.h5\n","scenesPath: /kaggle/input/d/sepibhm/nsvd-dataset/data/CLEVR_scenes_test.json\n","vocabTestPath: /kaggle/input/d/sepibhm/nsvd-dataset/test_concat/vocab_output.json\n","vocabPath: /kaggle/input/d/sepibhm/nsvd-dataset/train_concat/vocab_output.json\n","batch_size: 32\n","countFirstFailueRound: 0\n","maxSamples: -1\n","num_workers: 0\n","num_iters: 5000\n","display_every: 5\n","validate_every: 1000\n","shuffle_data: 1\n","optim: adam\n","lr: 0.001\n","betas: [0.9, 0.98]\n","eps: 1e-09\n","lr_decay_marks: [50000, 55000]\n","lr_decay_factor: 0.5\n","weight_decay: 1e-06\n","embedDim: 300\n","hiddenDim: 512\n","numLayers: 2\n","dropout: 0.1\n","multiHead: 8\n","hiddenSizeHead: 64\n","FeedForwardSize: 2048\n","FlatMLPSize: 512\n","FlatGlimpses: 1\n","FlatOutSize: 512\n","layers: 6\n","bidirectional: 1\n"]}],"source":["optsQ = OptionsQ().parse()#***"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-02-14T17:58:33.381968Z","iopub.status.busy":"2024-02-14T17:58:33.381589Z","iopub.status.idle":"2024-02-14T18:15:22.797494Z","shell.execute_reply":"2024-02-14T18:15:22.796453Z","shell.execute_reply.started":"2024-02-14T17:58:33.381935Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Using GPU Tesla T4 ...\n","[INFO] Loading dataset ...\n","Loading CaptionNet from /kaggle/input/nsvd-dataset/ckpt_iter5000.pkl\n","The caption encoder has 78702534 trainable parameters\n","Loading QuestionNet from /kaggle/input/nsvd-dataset/ckpt_iter3000.pkl\n","The question encoder has 139784134 trainable parameters\n","[INFO] Seed set to 42...\n","Testing with gt answers in history\n","Loading ckpt /kaggle/input/nsvd-dataset/ckpt_iter3000.pkl\n","[INFO] Changed batch size from 32 to 10\n","[Evaluation] step 5 / 7500 | acc. = 16.00\n","[Evaluation] step 10 / 7500 | acc. = 13.00\n","[Evaluation] step 15 / 7500 | acc. = 12.00\n","[Evaluation] step 20 / 7500 | acc. = 13.50\n","[Evaluation] step 25 / 7500 | acc. = 12.00\n","[Evaluation] step 30 / 7500 | acc. = 13.00\n","[Evaluation] step 35 / 7500 | acc. = 13.14\n","[Evaluation] step 40 / 7500 | acc. = 12.25\n","[Evaluation] step 45 / 7500 | acc. = 12.89\n","[Evaluation] step 50 / 7500 | acc. = 13.00\n","[Evaluation] step 55 / 7500 | acc. = 12.73\n","[Evaluation] step 60 / 7500 | acc. = 12.50\n","[Evaluation] step 65 / 7500 | acc. = 13.23\n","[Evaluation] step 70 / 7500 | acc. = 13.00\n","[Evaluation] step 75 / 7500 | acc. = 12.80\n","[Evaluation] step 80 / 7500 | acc. = 12.62\n","[Evaluation] step 85 / 7500 | acc. = 12.12\n","[Evaluation] step 90 / 7500 | acc. = 12.33\n","[Evaluation] step 95 / 7500 | acc. = 11.89\n","[Evaluation] step 100 / 7500 | acc. = 11.80\n","[Evaluation] step 105 / 7500 | acc. = 11.81\n","[Evaluation] step 110 / 7500 | acc. = 11.82\n","[Evaluation] step 115 / 7500 | acc. = 11.57\n","[Evaluation] step 120 / 7500 | acc. = 11.42\n","[Evaluation] step 125 / 7500 | acc. = 11.44\n","[Evaluation] step 130 / 7500 | acc. = 11.23\n","[Evaluation] step 135 / 7500 | acc. = 11.33\n","[Evaluation] step 140 / 7500 | acc. = 11.50\n","[Evaluation] step 145 / 7500 | acc. = 11.38\n","[Evaluation] step 150 / 7500 | acc. = 11.47\n","[Evaluation] step 155 / 7500 | acc. = 11.35\n","[Evaluation] step 160 / 7500 | acc. = 11.31\n","[Evaluation] step 165 / 7500 | acc. = 11.39\n","[Evaluation] step 170 / 7500 | acc. = 11.24\n","[Evaluation] step 175 / 7500 | acc. = 11.31\n","[Evaluation] step 180 / 7500 | acc. = 11.17\n","[Evaluation] step 185 / 7500 | acc. = 11.14\n","[Evaluation] step 190 / 7500 | acc. = 11.11\n","[Evaluation] step 195 / 7500 | acc. = 11.03\n","[Evaluation] step 200 / 7500 | acc. = 11.15\n","[Evaluation] step 205 / 7500 | acc. = 11.22\n","[Evaluation] step 210 / 7500 | acc. = 11.14\n","[Evaluation] step 215 / 7500 | acc. = 11.16\n","[Evaluation] step 220 / 7500 | acc. = 11.18\n","[Evaluation] step 225 / 7500 | acc. = 11.20\n","[Evaluation] step 230 / 7500 | acc. = 11.26\n","[Evaluation] step 235 / 7500 | acc. = 11.19\n","[Evaluation] step 240 / 7500 | acc. = 11.08\n","[Evaluation] step 245 / 7500 | acc. = 11.02\n","[Evaluation] step 250 / 7500 | acc. = 11.12\n","[Evaluation] step 255 / 7500 | acc. = 11.25\n","[Evaluation] step 260 / 7500 | acc. = 11.35\n","[Evaluation] step 265 / 7500 | acc. = 11.17\n","[Evaluation] step 270 / 7500 | acc. = 11.33\n","[Evaluation] step 275 / 7500 | acc. = 11.27\n","[Evaluation] step 280 / 7500 | acc. = 11.32\n","[Evaluation] step 285 / 7500 | acc. = 11.23\n","[Evaluation] step 290 / 7500 | acc. = 11.24\n","[Evaluation] step 295 / 7500 | acc. = 11.25\n","[Evaluation] step 300 / 7500 | acc. = 11.40\n","[Evaluation] step 305 / 7500 | acc. = 11.51\n","[Evaluation] step 310 / 7500 | acc. = 11.48\n","[Evaluation] step 315 / 7500 | acc. = 11.52\n","[Evaluation] step 320 / 7500 | acc. = 11.47\n","[Evaluation] step 325 / 7500 | acc. = 11.54\n","[Evaluation] step 330 / 7500 | acc. = 11.58\n","[Evaluation] step 335 / 7500 | acc. = 11.55\n","[Evaluation] step 340 / 7500 | acc. = 11.65\n","[Evaluation] step 345 / 7500 | acc. = 11.68\n","[Evaluation] step 350 / 7500 | acc. = 11.77\n","[Evaluation] step 355 / 7500 | acc. = 11.80\n","[Evaluation] step 360 / 7500 | acc. = 11.81\n","[Evaluation] step 365 / 7500 | acc. = 11.81\n","[Evaluation] step 370 / 7500 | acc. = 11.73\n","[Evaluation] step 375 / 7500 | acc. = 11.76\n","[Evaluation] step 380 / 7500 | acc. = 11.63\n","[Evaluation] step 385 / 7500 | acc. = 11.66\n","[Evaluation] step 390 / 7500 | acc. = 11.69\n","[Evaluation] step 395 / 7500 | acc. = 11.59\n","[Evaluation] step 400 / 7500 | acc. = 11.70\n","[Evaluation] step 405 / 7500 | acc. = 11.70\n","[Evaluation] step 410 / 7500 | acc. = 11.63\n","[Evaluation] step 415 / 7500 | acc. = 11.61\n","[Evaluation] step 420 / 7500 | acc. = 11.55\n","[Evaluation] step 425 / 7500 | acc. = 11.51\n","[Evaluation] step 430 / 7500 | acc. = 11.47\n","[Evaluation] step 435 / 7500 | acc. = 11.54\n","[Evaluation] step 440 / 7500 | acc. = 11.50\n","[Evaluation] step 445 / 7500 | acc. = 11.53\n","[Evaluation] step 450 / 7500 | acc. = 11.49\n","[Evaluation] step 455 / 7500 | acc. = 11.49\n","[Evaluation] step 460 / 7500 | acc. = 11.46\n","[Evaluation] step 465 / 7500 | acc. = 11.38\n","[Evaluation] step 470 / 7500 | acc. = 11.40\n","[Evaluation] step 475 / 7500 | acc. = 11.45\n","[Evaluation] step 480 / 7500 | acc. = 11.46\n","[Evaluation] step 485 / 7500 | acc. = 11.48\n","[Evaluation] step 490 / 7500 | acc. = 11.51\n","[Evaluation] step 495 / 7500 | acc. = 11.52\n","[Evaluation] step 500 / 7500 | acc. = 11.48\n","[Evaluation] step 505 / 7500 | acc. = 11.54\n","[Evaluation] step 510 / 7500 | acc. = 11.61\n","[Evaluation] step 515 / 7500 | acc. = 11.57\n","[Evaluation] step 520 / 7500 | acc. = 11.62\n","[Evaluation] step 525 / 7500 | acc. = 11.70\n","[Evaluation] step 530 / 7500 | acc. = 11.70\n","[Evaluation] step 535 / 7500 | acc. = 11.70\n","[Evaluation] step 540 / 7500 | acc. = 11.72\n","[Evaluation] step 545 / 7500 | acc. = 11.76\n","[Evaluation] step 550 / 7500 | acc. = 11.76\n","[Evaluation] step 555 / 7500 | acc. = 11.78\n","[Evaluation] step 560 / 7500 | acc. = 11.82\n","[Evaluation] step 565 / 7500 | acc. = 11.82\n","[Evaluation] step 570 / 7500 | acc. = 11.86\n","[Evaluation] step 575 / 7500 | acc. = 11.95\n","[Evaluation] step 580 / 7500 | acc. = 11.88\n","[Evaluation] step 585 / 7500 | acc. = 11.91\n","[Evaluation] step 590 / 7500 | acc. = 11.86\n","[Evaluation] step 595 / 7500 | acc. = 11.85\n","[Evaluation] step 600 / 7500 | acc. = 11.93\n","[Evaluation] step 605 / 7500 | acc. = 11.92\n","[Evaluation] step 610 / 7500 | acc. = 11.89\n","[Evaluation] step 615 / 7500 | acc. = 11.90\n","[Evaluation] step 620 / 7500 | acc. = 11.87\n","[Evaluation] step 625 / 7500 | acc. = 11.87\n","[Evaluation] step 630 / 7500 | acc. = 11.83\n","[Evaluation] step 635 / 7500 | acc. = 11.84\n","[Evaluation] step 640 / 7500 | acc. = 11.84\n","[Evaluation] step 645 / 7500 | acc. = 11.88\n","[Evaluation] step 650 / 7500 | acc. = 11.80\n","[Evaluation] step 655 / 7500 | acc. = 11.74\n","[Evaluation] step 660 / 7500 | acc. = 11.74\n","[Evaluation] step 665 / 7500 | acc. = 11.73\n","[Evaluation] step 670 / 7500 | acc. = 11.73\n","[Evaluation] step 675 / 7500 | acc. = 11.76\n","[Evaluation] step 680 / 7500 | acc. = 11.75\n","[Evaluation] step 685 / 7500 | acc. = 11.74\n","[Evaluation] step 690 / 7500 | acc. = 11.72\n","[Evaluation] step 695 / 7500 | acc. = 11.70\n","[Evaluation] step 700 / 7500 | acc. = 11.64\n","[Evaluation] step 705 / 7500 | acc. = 11.65\n","[Evaluation] step 710 / 7500 | acc. = 11.61\n","[Evaluation] step 715 / 7500 | acc. = 11.62\n","[Evaluation] step 720 / 7500 | acc. = 11.60\n","[Evaluation] step 725 / 7500 | acc. = 11.60\n","[Evaluation] step 730 / 7500 | acc. = 11.66\n","[Evaluation] step 735 / 7500 | acc. = 11.69\n","[Evaluation] step 740 / 7500 | acc. = 11.69\n","[Evaluation] step 745 / 7500 | acc. = 11.69\n","[Evaluation] step 750 / 7500 | acc. = 11.67\n","[Evaluation] step 755 / 7500 | acc. = 11.67\n","[Evaluation] step 760 / 7500 | acc. = 11.66\n","[Evaluation] step 765 / 7500 | acc. = 11.66\n","[Evaluation] step 770 / 7500 | acc. = 11.70\n","[Evaluation] step 775 / 7500 | acc. = 11.66\n","[Evaluation] step 780 / 7500 | acc. = 11.67\n","[Evaluation] step 785 / 7500 | acc. = 11.68\n","[Evaluation] step 790 / 7500 | acc. = 11.65\n","[Evaluation] step 795 / 7500 | acc. = 11.64\n","[Evaluation] step 800 / 7500 | acc. = 11.66\n","[Evaluation] step 805 / 7500 | acc. = 11.68\n","[Evaluation] step 810 / 7500 | acc. = 11.68\n","[Evaluation] step 815 / 7500 | acc. = 11.67\n","[Evaluation] step 820 / 7500 | acc. = 11.66\n","[Evaluation] step 825 / 7500 | acc. = 11.66\n","[Evaluation] step 830 / 7500 | acc. = 11.67\n","[Evaluation] step 835 / 7500 | acc. = 11.71\n","[Evaluation] step 840 / 7500 | acc. = 11.69\n","[Evaluation] step 845 / 7500 | acc. = 11.69\n","[Evaluation] step 850 / 7500 | acc. = 11.69\n","[Evaluation] step 855 / 7500 | acc. = 11.68\n","[Evaluation] step 860 / 7500 | acc. = 11.71\n","[Evaluation] step 865 / 7500 | acc. = 11.70\n","[Evaluation] step 870 / 7500 | acc. = 11.69\n","[Evaluation] step 875 / 7500 | acc. = 11.67\n","[Evaluation] step 880 / 7500 | acc. = 11.65\n","[Evaluation] step 885 / 7500 | acc. = 11.64\n","[Evaluation] step 890 / 7500 | acc. = 11.63\n","[Evaluation] step 895 / 7500 | acc. = 11.61\n","[Evaluation] step 900 / 7500 | acc. = 11.58\n","[Evaluation] step 905 / 7500 | acc. = 11.64\n","[Evaluation] step 910 / 7500 | acc. = 11.69\n","[Evaluation] step 915 / 7500 | acc. = 11.66\n","[Evaluation] step 920 / 7500 | acc. = 11.63\n","[Evaluation] step 925 / 7500 | acc. = 11.62\n","[Evaluation] step 930 / 7500 | acc. = 11.61\n","[Evaluation] step 935 / 7500 | acc. = 11.59\n","[Evaluation] step 940 / 7500 | acc. = 11.59\n","[Evaluation] step 945 / 7500 | acc. = 11.53\n","[Evaluation] step 950 / 7500 | acc. = 11.52\n","[Evaluation] step 955 / 7500 | acc. = 11.51\n","[Evaluation] step 960 / 7500 | acc. = 11.49\n","[Evaluation] step 965 / 7500 | acc. = 11.48\n","[Evaluation] step 970 / 7500 | acc. = 11.48\n","[Evaluation] step 975 / 7500 | acc. = 11.50\n","[Evaluation] step 980 / 7500 | acc. = 11.50\n","[Evaluation] step 985 / 7500 | acc. = 11.49\n","[Evaluation] step 990 / 7500 | acc. = 11.51\n","[Evaluation] step 995 / 7500 | acc. = 11.51\n","[Evaluation] step 1000 / 7500 | acc. = 11.52\n","[Evaluation] step 1005 / 7500 | acc. = 11.51\n","[Evaluation] step 1010 / 7500 | acc. = 11.51\n","[Evaluation] step 1015 / 7500 | acc. = 11.48\n","[Evaluation] step 1020 / 7500 | acc. = 11.44\n","[Evaluation] step 1025 / 7500 | acc. = 11.42\n","[Evaluation] step 1030 / 7500 | acc. = 11.41\n","[Evaluation] step 1035 / 7500 | acc. = 11.43\n","[Evaluation] step 1040 / 7500 | acc. = 11.39\n","[Evaluation] step 1045 / 7500 | acc. = 11.43\n","[Evaluation] step 1050 / 7500 | acc. = 11.42\n","[Evaluation] step 1055 / 7500 | acc. = 11.39\n","[Evaluation] step 1060 / 7500 | acc. = 11.43\n","[Evaluation] step 1065 / 7500 | acc. = 11.41\n","[Evaluation] step 1070 / 7500 | acc. = 11.44\n","[Evaluation] step 1075 / 7500 | acc. = 11.41\n","[Evaluation] step 1080 / 7500 | acc. = 11.41\n","[Evaluation] step 1085 / 7500 | acc. = 11.43\n","[Evaluation] step 1090 / 7500 | acc. = 11.41\n","[Evaluation] step 1095 / 7500 | acc. = 11.42\n","[Evaluation] step 1100 / 7500 | acc. = 11.44\n","[Evaluation] step 1105 / 7500 | acc. = 11.43\n","[Evaluation] step 1110 / 7500 | acc. = 11.42\n","[Evaluation] step 1115 / 7500 | acc. = 11.46\n","[Evaluation] step 1120 / 7500 | acc. = 11.46\n","[Evaluation] step 1125 / 7500 | acc. = 11.45\n","[Evaluation] step 1130 / 7500 | acc. = 11.46\n","[Evaluation] step 1135 / 7500 | acc. = 11.49\n","[Evaluation] step 1140 / 7500 | acc. = 11.53\n","[Evaluation] step 1145 / 7500 | acc. = 11.55\n","[Evaluation] step 1150 / 7500 | acc. = 11.57\n","[Evaluation] step 1155 / 7500 | acc. = 11.59\n","[Evaluation] step 1160 / 7500 | acc. = 11.58\n","[Evaluation] step 1165 / 7500 | acc. = 11.57\n","[Evaluation] step 1170 / 7500 | acc. = 11.56\n","[Evaluation] step 1175 / 7500 | acc. = 11.56\n","[Evaluation] step 1180 / 7500 | acc. = 11.54\n","[Evaluation] step 1185 / 7500 | acc. = 11.53\n","[Evaluation] step 1190 / 7500 | acc. = 11.53\n","[Evaluation] step 1195 / 7500 | acc. = 11.51\n","[Evaluation] step 1200 / 7500 | acc. = 11.50\n","[Evaluation] step 1205 / 7500 | acc. = 11.53\n","[Evaluation] step 1210 / 7500 | acc. = 11.52\n","[Evaluation] step 1215 / 7500 | acc. = 11.49\n","[Evaluation] step 1220 / 7500 | acc. = 11.51\n","[Evaluation] step 1225 / 7500 | acc. = 11.49\n","[Evaluation] step 1230 / 7500 | acc. = 11.48\n","[Evaluation] step 1235 / 7500 | acc. = 11.47\n","[Evaluation] step 1240 / 7500 | acc. = 11.47\n","[Evaluation] step 1245 / 7500 | acc. = 11.46\n","[Evaluation] step 1250 / 7500 | acc. = 11.47\n","[Evaluation] step 1255 / 7500 | acc. = 11.47\n","[Evaluation] step 1260 / 7500 | acc. = 11.48\n","[Evaluation] step 1265 / 7500 | acc. = 11.48\n","[Evaluation] step 1270 / 7500 | acc. = 11.48\n","[Evaluation] step 1275 / 7500 | acc. = 11.47\n","[Evaluation] step 1280 / 7500 | acc. = 11.46\n","[Evaluation] step 1285 / 7500 | acc. = 11.47\n","[Evaluation] step 1290 / 7500 | acc. = 11.48\n","[Evaluation] step 1295 / 7500 | acc. = 11.47\n","[Evaluation] step 1300 / 7500 | acc. = 11.47\n","[Evaluation] step 1305 / 7500 | acc. = 11.45\n","[Evaluation] step 1310 / 7500 | acc. = 11.47\n","[Evaluation] step 1315 / 7500 | acc. = 11.47\n","[Evaluation] step 1320 / 7500 | acc. = 11.46\n","[Evaluation] step 1325 / 7500 | acc. = 11.47\n","[Evaluation] step 1330 / 7500 | acc. = 11.47\n","[Evaluation] step 1335 / 7500 | acc. = 11.48\n","[Evaluation] step 1340 / 7500 | acc. = 11.47\n","[Evaluation] step 1345 / 7500 | acc. = 11.44\n","[Evaluation] step 1350 / 7500 | acc. = 11.43\n","[Evaluation] step 1355 / 7500 | acc. = 11.44\n","[Evaluation] step 1360 / 7500 | acc. = 11.46\n","[Evaluation] step 1365 / 7500 | acc. = 11.46\n","[Evaluation] step 1370 / 7500 | acc. = 11.47\n","[Evaluation] step 1375 / 7500 | acc. = 11.52\n","[Evaluation] step 1380 / 7500 | acc. = 11.51\n","[Evaluation] step 1385 / 7500 | acc. = 11.52\n","[Evaluation] step 1390 / 7500 | acc. = 11.52\n","[Evaluation] step 1395 / 7500 | acc. = 11.53\n","[Evaluation] step 1400 / 7500 | acc. = 11.52\n","[Evaluation] step 1405 / 7500 | acc. = 11.50\n","[Evaluation] step 1410 / 7500 | acc. = 11.50\n","[Evaluation] step 1415 / 7500 | acc. = 11.51\n","[Evaluation] step 1420 / 7500 | acc. = 11.51\n","[Evaluation] step 1425 / 7500 | acc. = 11.52\n","[Evaluation] step 1430 / 7500 | acc. = 11.53\n","[Evaluation] step 1435 / 7500 | acc. = 11.55\n","[Evaluation] step 1440 / 7500 | acc. = 11.55\n","[Evaluation] step 1445 / 7500 | acc. = 11.54\n","[Evaluation] step 1450 / 7500 | acc. = 11.56\n","[Evaluation] step 1455 / 7500 | acc. = 11.55\n","[Evaluation] step 1460 / 7500 | acc. = 11.57\n","[Evaluation] step 1465 / 7500 | acc. = 11.54\n","[Evaluation] step 1470 / 7500 | acc. = 11.56\n","[Evaluation] step 1475 / 7500 | acc. = 11.57\n","[Evaluation] step 1480 / 7500 | acc. = 11.57\n","[Evaluation] step 1485 / 7500 | acc. = 11.54\n","[Evaluation] step 1490 / 7500 | acc. = 11.54\n","[Evaluation] step 1495 / 7500 | acc. = 11.53\n","[Evaluation] step 1500 / 7500 | acc. = 11.53\n","[Evaluation] step 1505 / 7500 | acc. = 11.51\n","[Evaluation] step 1510 / 7500 | acc. = 11.50\n","[Evaluation] step 1515 / 7500 | acc. = 11.48\n","[Evaluation] step 1520 / 7500 | acc. = 11.48\n","[Evaluation] step 1525 / 7500 | acc. = 11.46\n","[Evaluation] step 1530 / 7500 | acc. = 11.44\n","[Evaluation] step 1535 / 7500 | acc. = 11.45\n","[Evaluation] step 1540 / 7500 | acc. = 11.43\n","[Evaluation] step 1545 / 7500 | acc. = 11.43\n","[Evaluation] step 1550 / 7500 | acc. = 11.45\n","[Evaluation] step 1555 / 7500 | acc. = 11.47\n","[Evaluation] step 1560 / 7500 | acc. = 11.46\n","[Evaluation] step 1565 / 7500 | acc. = 11.46\n","[Evaluation] step 1570 / 7500 | acc. = 11.45\n","[Evaluation] step 1575 / 7500 | acc. = 11.42\n","[Evaluation] step 1580 / 7500 | acc. = 11.40\n","[Evaluation] step 1585 / 7500 | acc. = 11.41\n","[Evaluation] step 1590 / 7500 | acc. = 11.40\n","[Evaluation] step 1595 / 7500 | acc. = 11.44\n","[Evaluation] step 1600 / 7500 | acc. = 11.44\n","[Evaluation] step 1605 / 7500 | acc. = 11.46\n","[Evaluation] step 1610 / 7500 | acc. = 11.47\n","[Evaluation] step 1615 / 7500 | acc. = 11.49\n","[Evaluation] step 1620 / 7500 | acc. = 11.49\n","[Evaluation] step 1625 / 7500 | acc. = 11.49\n","[Evaluation] step 1630 / 7500 | acc. = 11.49\n","[Evaluation] step 1635 / 7500 | acc. = 11.50\n","[Evaluation] step 1640 / 7500 | acc. = 11.49\n","[Evaluation] step 1645 / 7500 | acc. = 11.48\n","[Evaluation] step 1650 / 7500 | acc. = 11.50\n","[Evaluation] step 1655 / 7500 | acc. = 11.49\n","[Evaluation] step 1660 / 7500 | acc. = 11.49\n","[Evaluation] step 1665 / 7500 | acc. = 11.51\n","[Evaluation] step 1670 / 7500 | acc. = 11.52\n","[Evaluation] step 1675 / 7500 | acc. = 11.52\n","[Evaluation] step 1680 / 7500 | acc. = 11.51\n","[Evaluation] step 1685 / 7500 | acc. = 11.50\n","[Evaluation] step 1690 / 7500 | acc. = 11.50\n","[Evaluation] step 1695 / 7500 | acc. = 11.51\n","[Evaluation] step 1700 / 7500 | acc. = 11.51\n","[Evaluation] step 1705 / 7500 | acc. = 11.50\n","[Evaluation] step 1710 / 7500 | acc. = 11.54\n","[Evaluation] step 1715 / 7500 | acc. = 11.55\n","[Evaluation] step 1720 / 7500 | acc. = 11.57\n","[Evaluation] step 1725 / 7500 | acc. = 11.57\n","[Evaluation] step 1730 / 7500 | acc. = 11.56\n","[Evaluation] step 1735 / 7500 | acc. = 11.54\n","[Evaluation] step 1740 / 7500 | acc. = 11.58\n","[Evaluation] step 1745 / 7500 | acc. = 11.56\n","[Evaluation] step 1750 / 7500 | acc. = 11.57\n","[Evaluation] step 1755 / 7500 | acc. = 11.59\n","[Evaluation] step 1760 / 7500 | acc. = 11.61\n","[Evaluation] step 1765 / 7500 | acc. = 11.61\n","[Evaluation] step 1770 / 7500 | acc. = 11.60\n","[Evaluation] step 1775 / 7500 | acc. = 11.61\n","[Evaluation] step 1780 / 7500 | acc. = 11.59\n","[Evaluation] step 1785 / 7500 | acc. = 11.57\n","[Evaluation] step 1790 / 7500 | acc. = 11.56\n","[Evaluation] step 1795 / 7500 | acc. = 11.57\n","[Evaluation] step 1800 / 7500 | acc. = 11.57\n","[Evaluation] step 1805 / 7500 | acc. = 11.57\n","[Evaluation] step 1810 / 7500 | acc. = 11.59\n","[Evaluation] step 1815 / 7500 | acc. = 11.58\n","[Evaluation] step 1820 / 7500 | acc. = 11.58\n","[Evaluation] step 1825 / 7500 | acc. = 11.58\n","[Evaluation] step 1830 / 7500 | acc. = 11.57\n","[Evaluation] step 1835 / 7500 | acc. = 11.60\n","[Evaluation] step 1840 / 7500 | acc. = 11.59\n","[Evaluation] step 1845 / 7500 | acc. = 11.59\n","[Evaluation] step 1850 / 7500 | acc. = 11.59\n","[Evaluation] step 1855 / 7500 | acc. = 11.60\n","[Evaluation] step 1860 / 7500 | acc. = 11.61\n","[Evaluation] step 1865 / 7500 | acc. = 11.61\n","[Evaluation] step 1870 / 7500 | acc. = 11.60\n","[Evaluation] step 1875 / 7500 | acc. = 11.57\n","[Evaluation] step 1880 / 7500 | acc. = 11.60\n","[Evaluation] step 1885 / 7500 | acc. = 11.59\n","[Evaluation] step 1890 / 7500 | acc. = 11.59\n","[Evaluation] step 1895 / 7500 | acc. = 11.59\n","[Evaluation] step 1900 / 7500 | acc. = 11.60\n","[Evaluation] step 1905 / 7500 | acc. = 11.61\n","[Evaluation] step 1910 / 7500 | acc. = 11.61\n","[Evaluation] step 1915 / 7500 | acc. = 11.62\n","[Evaluation] step 1920 / 7500 | acc. = 11.61\n","[Evaluation] step 1925 / 7500 | acc. = 11.64\n","[Evaluation] step 1930 / 7500 | acc. = 11.65\n","[Evaluation] step 1935 / 7500 | acc. = 11.64\n","[Evaluation] step 1940 / 7500 | acc. = 11.66\n","[Evaluation] step 1945 / 7500 | acc. = 11.66\n","[Evaluation] step 1950 / 7500 | acc. = 11.67\n","[Evaluation] step 1955 / 7500 | acc. = 11.68\n","[Evaluation] step 1960 / 7500 | acc. = 11.68\n","[Evaluation] step 1965 / 7500 | acc. = 11.68\n","[Evaluation] step 1970 / 7500 | acc. = 11.69\n","[Evaluation] step 1975 / 7500 | acc. = 11.67\n","[Evaluation] step 1980 / 7500 | acc. = 11.67\n","[Evaluation] step 1985 / 7500 | acc. = 11.66\n","[Evaluation] step 1990 / 7500 | acc. = 11.67\n","[Evaluation] step 1995 / 7500 | acc. = 11.67\n","[Evaluation] step 2000 / 7500 | acc. = 11.69\n","[Evaluation] step 2005 / 7500 | acc. = 11.70\n","[Evaluation] step 2010 / 7500 | acc. = 11.70\n","[Evaluation] step 2015 / 7500 | acc. = 11.69\n","[Evaluation] step 2020 / 7500 | acc. = 11.68\n","[Evaluation] step 2025 / 7500 | acc. = 11.68\n","[Evaluation] step 2030 / 7500 | acc. = 11.68\n","[Evaluation] step 2035 / 7500 | acc. = 11.69\n","[Evaluation] step 2040 / 7500 | acc. = 11.70\n","[Evaluation] step 2045 / 7500 | acc. = 11.72\n","[Evaluation] step 2050 / 7500 | acc. = 11.73\n","[Evaluation] step 2055 / 7500 | acc. = 11.73\n","[Evaluation] step 2060 / 7500 | acc. = 11.73\n","[Evaluation] step 2065 / 7500 | acc. = 11.74\n","[Evaluation] step 2070 / 7500 | acc. = 11.76\n","[Evaluation] step 2075 / 7500 | acc. = 11.76\n","[Evaluation] step 2080 / 7500 | acc. = 11.76\n","[Evaluation] step 2085 / 7500 | acc. = 11.75\n","[Evaluation] step 2090 / 7500 | acc. = 11.77\n","[Evaluation] step 2095 / 7500 | acc. = 11.77\n","[Evaluation] step 2100 / 7500 | acc. = 11.79\n","[Evaluation] step 2105 / 7500 | acc. = 11.78\n","[Evaluation] step 2110 / 7500 | acc. = 11.78\n","[Evaluation] step 2115 / 7500 | acc. = 11.80\n","[Evaluation] step 2120 / 7500 | acc. = 11.79\n","[Evaluation] step 2125 / 7500 | acc. = 11.78\n","[Evaluation] step 2130 / 7500 | acc. = 11.78\n","[Evaluation] step 2135 / 7500 | acc. = 11.78\n","[Evaluation] step 2140 / 7500 | acc. = 11.79\n","[Evaluation] step 2145 / 7500 | acc. = 11.78\n","[Evaluation] step 2150 / 7500 | acc. = 11.80\n","[Evaluation] step 2155 / 7500 | acc. = 11.79\n","[Evaluation] step 2160 / 7500 | acc. = 11.78\n","[Evaluation] step 2165 / 7500 | acc. = 11.77\n","[Evaluation] step 2170 / 7500 | acc. = 11.77\n","[Evaluation] step 2175 / 7500 | acc. = 11.77\n","[Evaluation] step 2180 / 7500 | acc. = 11.77\n","[Evaluation] step 2185 / 7500 | acc. = 11.79\n","[Evaluation] step 2190 / 7500 | acc. = 11.79\n","[Evaluation] step 2195 / 7500 | acc. = 11.79\n","[Evaluation] step 2200 / 7500 | acc. = 11.79\n","[Evaluation] step 2205 / 7500 | acc. = 11.77\n","[Evaluation] step 2210 / 7500 | acc. = 11.78\n","[Evaluation] step 2215 / 7500 | acc. = 11.78\n","[Evaluation] step 2220 / 7500 | acc. = 11.77\n","[Evaluation] step 2225 / 7500 | acc. = 11.75\n","[Evaluation] step 2230 / 7500 | acc. = 11.76\n","[Evaluation] step 2235 / 7500 | acc. = 11.76\n","[Evaluation] step 2240 / 7500 | acc. = 11.77\n","[Evaluation] step 2245 / 7500 | acc. = 11.77\n","[Evaluation] step 2250 / 7500 | acc. = 11.77\n","[Evaluation] step 2255 / 7500 | acc. = 11.79\n","[Evaluation] step 2260 / 7500 | acc. = 11.80\n","[Evaluation] step 2265 / 7500 | acc. = 11.79\n","[Evaluation] step 2270 / 7500 | acc. = 11.80\n","[Evaluation] step 2275 / 7500 | acc. = 11.80\n","[Evaluation] step 2280 / 7500 | acc. = 11.79\n","[Evaluation] step 2285 / 7500 | acc. = 11.79\n","[Evaluation] step 2290 / 7500 | acc. = 11.82\n","[Evaluation] step 2295 / 7500 | acc. = 11.82\n","[Evaluation] step 2300 / 7500 | acc. = 11.81\n","[Evaluation] step 2305 / 7500 | acc. = 11.82\n","[Evaluation] step 2310 / 7500 | acc. = 11.82\n","[Evaluation] step 2315 / 7500 | acc. = 11.84\n","[Evaluation] step 2320 / 7500 | acc. = 11.82\n","[Evaluation] step 2325 / 7500 | acc. = 11.83\n","[Evaluation] step 2330 / 7500 | acc. = 11.82\n","[Evaluation] step 2335 / 7500 | acc. = 11.81\n","[Evaluation] step 2340 / 7500 | acc. = 11.79\n","[Evaluation] step 2345 / 7500 | acc. = 11.81\n","[Evaluation] step 2350 / 7500 | acc. = 11.80\n","[Evaluation] step 2355 / 7500 | acc. = 11.79\n","[Evaluation] step 2360 / 7500 | acc. = 11.80\n","[Evaluation] step 2365 / 7500 | acc. = 11.81\n","[Evaluation] step 2370 / 7500 | acc. = 11.81\n","[Evaluation] step 2375 / 7500 | acc. = 11.82\n","[Evaluation] step 2380 / 7500 | acc. = 11.83\n","[Evaluation] step 2385 / 7500 | acc. = 11.84\n","[Evaluation] step 2390 / 7500 | acc. = 11.85\n","[Evaluation] step 2395 / 7500 | acc. = 11.85\n","[Evaluation] step 2400 / 7500 | acc. = 11.86\n","[Evaluation] step 2405 / 7500 | acc. = 11.85\n","[Evaluation] step 2410 / 7500 | acc. = 11.86\n","[Evaluation] step 2415 / 7500 | acc. = 11.86\n","[Evaluation] step 2420 / 7500 | acc. = 11.85\n","[Evaluation] step 2425 / 7500 | acc. = 11.84\n","[Evaluation] step 2430 / 7500 | acc. = 11.85\n","[Evaluation] step 2435 / 7500 | acc. = 11.84\n","[Evaluation] step 2440 / 7500 | acc. = 11.85\n","[Evaluation] step 2445 / 7500 | acc. = 11.86\n","[Evaluation] step 2450 / 7500 | acc. = 11.85\n","[Evaluation] step 2455 / 7500 | acc. = 11.85\n","[Evaluation] step 2460 / 7500 | acc. = 11.86\n","[Evaluation] step 2465 / 7500 | acc. = 11.85\n","[Evaluation] step 2470 / 7500 | acc. = 11.85\n","[Evaluation] step 2475 / 7500 | acc. = 11.84\n","[Evaluation] step 2480 / 7500 | acc. = 11.83\n","[Evaluation] step 2485 / 7500 | acc. = 11.82\n","[Evaluation] step 2490 / 7500 | acc. = 11.81\n","[Evaluation] step 2495 / 7500 | acc. = 11.81\n","[Evaluation] step 2500 / 7500 | acc. = 11.81\n","[Evaluation] step 2505 / 7500 | acc. = 11.80\n","[Evaluation] step 2510 / 7500 | acc. = 11.80\n","[Evaluation] step 2515 / 7500 | acc. = 11.81\n","[Evaluation] step 2520 / 7500 | acc. = 11.80\n","[Evaluation] step 2525 / 7500 | acc. = 11.81\n","[Evaluation] step 2530 / 7500 | acc. = 11.82\n","[Evaluation] step 2535 / 7500 | acc. = 11.81\n","[Evaluation] step 2540 / 7500 | acc. = 11.81\n","[Evaluation] step 2545 / 7500 | acc. = 11.81\n","[Evaluation] step 2550 / 7500 | acc. = 11.81\n","[Evaluation] step 2555 / 7500 | acc. = 11.80\n","[Evaluation] step 2560 / 7500 | acc. = 11.82\n","[Evaluation] step 2565 / 7500 | acc. = 11.82\n","[Evaluation] step 2570 / 7500 | acc. = 11.82\n","[Evaluation] step 2575 / 7500 | acc. = 11.82\n","[Evaluation] step 2580 / 7500 | acc. = 11.81\n","[Evaluation] step 2585 / 7500 | acc. = 11.79\n","[Evaluation] step 2590 / 7500 | acc. = 11.79\n","[Evaluation] step 2595 / 7500 | acc. = 11.78\n","[Evaluation] step 2600 / 7500 | acc. = 11.79\n","[Evaluation] step 2605 / 7500 | acc. = 11.79\n","[Evaluation] step 2610 / 7500 | acc. = 11.79\n","[Evaluation] step 2615 / 7500 | acc. = 11.79\n","[Evaluation] step 2620 / 7500 | acc. = 11.79\n","[Evaluation] step 2625 / 7500 | acc. = 11.79\n","[Evaluation] step 2630 / 7500 | acc. = 11.79\n","[Evaluation] step 2635 / 7500 | acc. = 11.78\n","[Evaluation] step 2640 / 7500 | acc. = 11.78\n","[Evaluation] step 2645 / 7500 | acc. = 11.77\n","[Evaluation] step 2650 / 7500 | acc. = 11.78\n","[Evaluation] step 2655 / 7500 | acc. = 11.79\n","[Evaluation] step 2660 / 7500 | acc. = 11.80\n","[Evaluation] step 2665 / 7500 | acc. = 11.79\n","[Evaluation] step 2670 / 7500 | acc. = 11.79\n","[Evaluation] step 2675 / 7500 | acc. = 11.80\n","[Evaluation] step 2680 / 7500 | acc. = 11.79\n","[Evaluation] step 2685 / 7500 | acc. = 11.78\n","[Evaluation] step 2690 / 7500 | acc. = 11.80\n","[Evaluation] step 2695 / 7500 | acc. = 11.81\n","[Evaluation] step 2700 / 7500 | acc. = 11.81\n","[Evaluation] step 2705 / 7500 | acc. = 11.81\n","[Evaluation] step 2710 / 7500 | acc. = 11.81\n","[Evaluation] step 2715 / 7500 | acc. = 11.80\n","[Evaluation] step 2720 / 7500 | acc. = 11.81\n","[Evaluation] step 2725 / 7500 | acc. = 11.81\n","[Evaluation] step 2730 / 7500 | acc. = 11.79\n","[Evaluation] step 2735 / 7500 | acc. = 11.78\n","[Evaluation] step 2740 / 7500 | acc. = 11.79\n","[Evaluation] step 2745 / 7500 | acc. = 11.80\n","[Evaluation] step 2750 / 7500 | acc. = 11.80\n","[Evaluation] step 2755 / 7500 | acc. = 11.80\n","[Evaluation] step 2760 / 7500 | acc. = 11.80\n","[Evaluation] step 2765 / 7500 | acc. = 11.79\n","[Evaluation] step 2770 / 7500 | acc. = 11.79\n","[Evaluation] step 2775 / 7500 | acc. = 11.79\n","[Evaluation] step 2780 / 7500 | acc. = 11.79\n","[Evaluation] step 2785 / 7500 | acc. = 11.80\n","[Evaluation] step 2790 / 7500 | acc. = 11.80\n","[Evaluation] step 2795 / 7500 | acc. = 11.80\n","[Evaluation] step 2800 / 7500 | acc. = 11.80\n","[Evaluation] step 2805 / 7500 | acc. = 11.79\n","[Evaluation] step 2810 / 7500 | acc. = 11.80\n","[Evaluation] step 2815 / 7500 | acc. = 11.79\n","[Evaluation] step 2820 / 7500 | acc. = 11.79\n","[Evaluation] step 2825 / 7500 | acc. = 11.81\n","[Evaluation] step 2830 / 7500 | acc. = 11.81\n","[Evaluation] step 2835 / 7500 | acc. = 11.81\n","[Evaluation] step 2840 / 7500 | acc. = 11.81\n","[Evaluation] step 2845 / 7500 | acc. = 11.82\n","[Evaluation] step 2850 / 7500 | acc. = 11.82\n","[Evaluation] step 2855 / 7500 | acc. = 11.80\n","[Evaluation] step 2860 / 7500 | acc. = 11.80\n","[Evaluation] step 2865 / 7500 | acc. = 11.80\n","[Evaluation] step 2870 / 7500 | acc. = 11.79\n","[Evaluation] step 2875 / 7500 | acc. = 11.81\n","[Evaluation] step 2880 / 7500 | acc. = 11.80\n","[Evaluation] step 2885 / 7500 | acc. = 11.81\n","[Evaluation] step 2890 / 7500 | acc. = 11.80\n","[Evaluation] step 2895 / 7500 | acc. = 11.80\n","[Evaluation] step 2900 / 7500 | acc. = 11.81\n","[Evaluation] step 2905 / 7500 | acc. = 11.80\n","[Evaluation] step 2910 / 7500 | acc. = 11.80\n","[Evaluation] step 2915 / 7500 | acc. = 11.79\n","[Evaluation] step 2920 / 7500 | acc. = 11.80\n","[Evaluation] step 2925 / 7500 | acc. = 11.80\n","[Evaluation] step 2930 / 7500 | acc. = 11.82\n","[Evaluation] step 2935 / 7500 | acc. = 11.81\n","[Evaluation] step 2940 / 7500 | acc. = 11.81\n","[Evaluation] step 2945 / 7500 | acc. = 11.81\n","[Evaluation] step 2950 / 7500 | acc. = 11.81\n","[Evaluation] step 2955 / 7500 | acc. = 11.81\n","[Evaluation] step 2960 / 7500 | acc. = 11.80\n","[Evaluation] step 2965 / 7500 | acc. = 11.81\n","[Evaluation] step 2970 / 7500 | acc. = 11.80\n","[Evaluation] step 2975 / 7500 | acc. = 11.80\n","[Evaluation] step 2980 / 7500 | acc. = 11.80\n","[Evaluation] step 2985 / 7500 | acc. = 11.80\n","[Evaluation] step 2990 / 7500 | acc. = 11.79\n","[Evaluation] step 2995 / 7500 | acc. = 11.79\n","[Evaluation] step 3000 / 7500 | acc. = 11.79\n","[Evaluation] step 3005 / 7500 | acc. = 11.78\n","[Evaluation] step 3010 / 7500 | acc. = 11.78\n","[Evaluation] step 3015 / 7500 | acc. = 11.78\n","[Evaluation] step 3020 / 7500 | acc. = 11.78\n","[Evaluation] step 3025 / 7500 | acc. = 11.80\n","[Evaluation] step 3030 / 7500 | acc. = 11.80\n","[Evaluation] step 3035 / 7500 | acc. = 11.80\n","[Evaluation] step 3040 / 7500 | acc. = 11.80\n","[Evaluation] step 3045 / 7500 | acc. = 11.79\n","[Evaluation] step 3050 / 7500 | acc. = 11.80\n","[Evaluation] step 3055 / 7500 | acc. = 11.80\n","[Evaluation] step 3060 / 7500 | acc. = 11.79\n","[Evaluation] step 3065 / 7500 | acc. = 11.78\n","[Evaluation] step 3070 / 7500 | acc. = 11.77\n","[Evaluation] step 3075 / 7500 | acc. = 11.76\n","[Evaluation] step 3080 / 7500 | acc. = 11.77\n","[Evaluation] step 3085 / 7500 | acc. = 11.77\n","[Evaluation] step 3090 / 7500 | acc. = 11.76\n","[Evaluation] step 3095 / 7500 | acc. = 11.76\n","[Evaluation] step 3100 / 7500 | acc. = 11.76\n","[Evaluation] step 3105 / 7500 | acc. = 11.75\n","[Evaluation] step 3110 / 7500 | acc. = 11.75\n","[Evaluation] step 3115 / 7500 | acc. = 11.77\n","[Evaluation] step 3120 / 7500 | acc. = 11.77\n","[Evaluation] step 3125 / 7500 | acc. = 11.76\n","[Evaluation] step 3130 / 7500 | acc. = 11.75\n","[Evaluation] step 3135 / 7500 | acc. = 11.76\n","[Evaluation] step 3140 / 7500 | acc. = 11.77\n","[Evaluation] step 3145 / 7500 | acc. = 11.76\n","[Evaluation] step 3150 / 7500 | acc. = 11.76\n","[Evaluation] step 3155 / 7500 | acc. = 11.75\n","[Evaluation] step 3160 / 7500 | acc. = 11.75\n","[Evaluation] step 3165 / 7500 | acc. = 11.75\n","[Evaluation] step 3170 / 7500 | acc. = 11.74\n","[Evaluation] step 3175 / 7500 | acc. = 11.74\n","[Evaluation] step 3180 / 7500 | acc. = 11.75\n","[Evaluation] step 3185 / 7500 | acc. = 11.75\n","[Evaluation] step 3190 / 7500 | acc. = 11.75\n","[Evaluation] step 3195 / 7500 | acc. = 11.75\n","[Evaluation] step 3200 / 7500 | acc. = 11.75\n","[Evaluation] step 3205 / 7500 | acc. = 11.76\n","[Evaluation] step 3210 / 7500 | acc. = 11.76\n","[Evaluation] step 3215 / 7500 | acc. = 11.77\n","[Evaluation] step 3220 / 7500 | acc. = 11.77\n","[Evaluation] step 3225 / 7500 | acc. = 11.77\n","[Evaluation] step 3230 / 7500 | acc. = 11.77\n","[Evaluation] step 3235 / 7500 | acc. = 11.77\n","[Evaluation] step 3240 / 7500 | acc. = 11.77\n","[Evaluation] step 3245 / 7500 | acc. = 11.76\n","[Evaluation] step 3250 / 7500 | acc. = 11.76\n","[Evaluation] step 3255 / 7500 | acc. = 11.75\n","[Evaluation] step 3260 / 7500 | acc. = 11.75\n","[Evaluation] step 3265 / 7500 | acc. = 11.75\n","[Evaluation] step 3270 / 7500 | acc. = 11.74\n","[Evaluation] step 3275 / 7500 | acc. = 11.73\n","[Evaluation] step 3280 / 7500 | acc. = 11.72\n","[Evaluation] step 3285 / 7500 | acc. = 11.72\n","[Evaluation] step 3290 / 7500 | acc. = 11.72\n","[Evaluation] step 3295 / 7500 | acc. = 11.72\n","[Evaluation] step 3300 / 7500 | acc. = 11.72\n","[Evaluation] step 3305 / 7500 | acc. = 11.72\n","[Evaluation] step 3310 / 7500 | acc. = 11.72\n","[Evaluation] step 3315 / 7500 | acc. = 11.71\n","[Evaluation] step 3320 / 7500 | acc. = 11.70\n","[Evaluation] step 3325 / 7500 | acc. = 11.70\n","[Evaluation] step 3330 / 7500 | acc. = 11.70\n","[Evaluation] step 3335 / 7500 | acc. = 11.72\n","[Evaluation] step 3340 / 7500 | acc. = 11.73\n","[Evaluation] step 3345 / 7500 | acc. = 11.73\n","[Evaluation] step 3350 / 7500 | acc. = 11.73\n","[Evaluation] step 3355 / 7500 | acc. = 11.72\n","[Evaluation] step 3360 / 7500 | acc. = 11.72\n","[Evaluation] step 3365 / 7500 | acc. = 11.72\n","[Evaluation] step 3370 / 7500 | acc. = 11.72\n","[Evaluation] step 3375 / 7500 | acc. = 11.72\n","[Evaluation] step 3380 / 7500 | acc. = 11.72\n","[Evaluation] step 3385 / 7500 | acc. = 11.73\n","[Evaluation] step 3390 / 7500 | acc. = 11.75\n","[Evaluation] step 3395 / 7500 | acc. = 11.75\n","[Evaluation] step 3400 / 7500 | acc. = 11.73\n","[Evaluation] step 3405 / 7500 | acc. = 11.72\n","[Evaluation] step 3410 / 7500 | acc. = 11.72\n","[Evaluation] step 3415 / 7500 | acc. = 11.71\n","[Evaluation] step 3420 / 7500 | acc. = 11.71\n","[Evaluation] step 3425 / 7500 | acc. = 11.71\n","[Evaluation] step 3430 / 7500 | acc. = 11.69\n","[Evaluation] step 3435 / 7500 | acc. = 11.69\n","[Evaluation] step 3440 / 7500 | acc. = 11.70\n","[Evaluation] step 3445 / 7500 | acc. = 11.70\n","[Evaluation] step 3450 / 7500 | acc. = 11.70\n","[Evaluation] step 3455 / 7500 | acc. = 11.70\n","[Evaluation] step 3460 / 7500 | acc. = 11.70\n","[Evaluation] step 3465 / 7500 | acc. = 11.71\n","[Evaluation] step 3470 / 7500 | acc. = 11.72\n","[Evaluation] step 3475 / 7500 | acc. = 11.73\n","[Evaluation] step 3480 / 7500 | acc. = 11.73\n","[Evaluation] step 3485 / 7500 | acc. = 11.73\n","[Evaluation] step 3490 / 7500 | acc. = 11.72\n","[Evaluation] step 3495 / 7500 | acc. = 11.73\n","[Evaluation] step 3500 / 7500 | acc. = 11.74\n","[Evaluation] step 3505 / 7500 | acc. = 11.74\n","[Evaluation] step 3510 / 7500 | acc. = 11.74\n","[Evaluation] step 3515 / 7500 | acc. = 11.74\n","[Evaluation] step 3520 / 7500 | acc. = 11.75\n","[Evaluation] step 3525 / 7500 | acc. = 11.75\n","[Evaluation] step 3530 / 7500 | acc. = 11.75\n","[Evaluation] step 3535 / 7500 | acc. = 11.75\n","[Evaluation] step 3540 / 7500 | acc. = 11.75\n","[Evaluation] step 3545 / 7500 | acc. = 11.75\n","[Evaluation] step 3550 / 7500 | acc. = 11.77\n","[Evaluation] step 3555 / 7500 | acc. = 11.76\n","[Evaluation] step 3560 / 7500 | acc. = 11.76\n","[Evaluation] step 3565 / 7500 | acc. = 11.76\n","[Evaluation] step 3570 / 7500 | acc. = 11.76\n","[Evaluation] step 3575 / 7500 | acc. = 11.77\n","[Evaluation] step 3580 / 7500 | acc. = 11.77\n","[Evaluation] step 3585 / 7500 | acc. = 11.77\n","[Evaluation] step 3590 / 7500 | acc. = 11.76\n","[Evaluation] step 3595 / 7500 | acc. = 11.76\n","[Evaluation] step 3600 / 7500 | acc. = 11.76\n","[Evaluation] step 3605 / 7500 | acc. = 11.75\n","[Evaluation] step 3610 / 7500 | acc. = 11.75\n","[Evaluation] step 3615 / 7500 | acc. = 11.75\n","[Evaluation] step 3620 / 7500 | acc. = 11.74\n","[Evaluation] step 3625 / 7500 | acc. = 11.74\n","[Evaluation] step 3630 / 7500 | acc. = 11.74\n","[Evaluation] step 3635 / 7500 | acc. = 11.75\n","[Evaluation] step 3640 / 7500 | acc. = 11.74\n","[Evaluation] step 3645 / 7500 | acc. = 11.74\n","[Evaluation] step 3650 / 7500 | acc. = 11.75\n","[Evaluation] step 3655 / 7500 | acc. = 11.75\n","[Evaluation] step 3660 / 7500 | acc. = 11.75\n","[Evaluation] step 3665 / 7500 | acc. = 11.75\n","[Evaluation] step 3670 / 7500 | acc. = 11.75\n","[Evaluation] step 3675 / 7500 | acc. = 11.76\n","[Evaluation] step 3680 / 7500 | acc. = 11.76\n","[Evaluation] step 3685 / 7500 | acc. = 11.76\n","[Evaluation] step 3690 / 7500 | acc. = 11.76\n","[Evaluation] step 3695 / 7500 | acc. = 11.75\n","[Evaluation] step 3700 / 7500 | acc. = 11.75\n","[Evaluation] step 3705 / 7500 | acc. = 11.75\n","[Evaluation] step 3710 / 7500 | acc. = 11.75\n","[Evaluation] step 3715 / 7500 | acc. = 11.74\n","[Evaluation] step 3720 / 7500 | acc. = 11.75\n","[Evaluation] step 3725 / 7500 | acc. = 11.74\n","[Evaluation] step 3730 / 7500 | acc. = 11.74\n","[Evaluation] step 3735 / 7500 | acc. = 11.75\n","[Evaluation] step 3740 / 7500 | acc. = 11.75\n","[Evaluation] step 3745 / 7500 | acc. = 11.74\n","[Evaluation] step 3750 / 7500 | acc. = 11.73\n","[Evaluation] step 3755 / 7500 | acc. = 11.73\n","[Evaluation] step 3760 / 7500 | acc. = 11.74\n","[Evaluation] step 3765 / 7500 | acc. = 11.73\n","[Evaluation] step 3770 / 7500 | acc. = 11.72\n","[Evaluation] step 3775 / 7500 | acc. = 11.72\n","[Evaluation] step 3780 / 7500 | acc. = 11.72\n","[Evaluation] step 3785 / 7500 | acc. = 11.73\n","[Evaluation] step 3790 / 7500 | acc. = 11.73\n","[Evaluation] step 3795 / 7500 | acc. = 11.74\n","[Evaluation] step 3800 / 7500 | acc. = 11.74\n","[Evaluation] step 3805 / 7500 | acc. = 11.74\n","[Evaluation] step 3810 / 7500 | acc. = 11.73\n","[Evaluation] step 3815 / 7500 | acc. = 11.73\n","[Evaluation] step 3820 / 7500 | acc. = 11.73\n","[Evaluation] step 3825 / 7500 | acc. = 11.73\n","[Evaluation] step 3830 / 7500 | acc. = 11.73\n","[Evaluation] step 3835 / 7500 | acc. = 11.73\n","[Evaluation] step 3840 / 7500 | acc. = 11.72\n","[Evaluation] step 3845 / 7500 | acc. = 11.72\n","[Evaluation] step 3850 / 7500 | acc. = 11.72\n","[Evaluation] step 3855 / 7500 | acc. = 11.74\n","[Evaluation] step 3860 / 7500 | acc. = 11.74\n","[Evaluation] step 3865 / 7500 | acc. = 11.74\n","[Evaluation] step 3870 / 7500 | acc. = 11.73\n","[Evaluation] step 3875 / 7500 | acc. = 11.73\n","[Evaluation] step 3880 / 7500 | acc. = 11.73\n","[Evaluation] step 3885 / 7500 | acc. = 11.73\n","[Evaluation] step 3890 / 7500 | acc. = 11.74\n","[Evaluation] step 3895 / 7500 | acc. = 11.74\n","[Evaluation] step 3900 / 7500 | acc. = 11.73\n","[Evaluation] step 3905 / 7500 | acc. = 11.74\n","[Evaluation] step 3910 / 7500 | acc. = 11.73\n","[Evaluation] step 3915 / 7500 | acc. = 11.73\n","[Evaluation] step 3920 / 7500 | acc. = 11.73\n","[Evaluation] step 3925 / 7500 | acc. = 11.73\n","[Evaluation] step 3930 / 7500 | acc. = 11.73\n","[Evaluation] step 3935 / 7500 | acc. = 11.73\n","[Evaluation] step 3940 / 7500 | acc. = 11.73\n","[Evaluation] step 3945 / 7500 | acc. = 11.72\n","[Evaluation] step 3950 / 7500 | acc. = 11.74\n","[Evaluation] step 3955 / 7500 | acc. = 11.74\n","[Evaluation] step 3960 / 7500 | acc. = 11.74\n","[Evaluation] step 3965 / 7500 | acc. = 11.75\n","[Evaluation] step 3970 / 7500 | acc. = 11.75\n","[Evaluation] step 3975 / 7500 | acc. = 11.75\n","[Evaluation] step 3980 / 7500 | acc. = 11.74\n","[Evaluation] step 3985 / 7500 | acc. = 11.74\n","[Evaluation] step 3990 / 7500 | acc. = 11.74\n","[Evaluation] step 3995 / 7500 | acc. = 11.75\n","[Evaluation] step 4000 / 7500 | acc. = 11.75\n","[Evaluation] step 4005 / 7500 | acc. = 11.76\n","[Evaluation] step 4010 / 7500 | acc. = 11.76\n","[Evaluation] step 4015 / 7500 | acc. = 11.76\n","[Evaluation] step 4020 / 7500 | acc. = 11.76\n","[Evaluation] step 4025 / 7500 | acc. = 11.76\n","[Evaluation] step 4030 / 7500 | acc. = 11.76\n","[Evaluation] step 4035 / 7500 | acc. = 11.76\n","[Evaluation] step 4040 / 7500 | acc. = 11.75\n","[Evaluation] step 4045 / 7500 | acc. = 11.75\n","[Evaluation] step 4050 / 7500 | acc. = 11.75\n","[Evaluation] step 4055 / 7500 | acc. = 11.76\n","[Evaluation] step 4060 / 7500 | acc. = 11.77\n","[Evaluation] step 4065 / 7500 | acc. = 11.78\n","[Evaluation] step 4070 / 7500 | acc. = 11.78\n","[Evaluation] step 4075 / 7500 | acc. = 11.78\n","[Evaluation] step 4080 / 7500 | acc. = 11.78\n","[Evaluation] step 4085 / 7500 | acc. = 11.78\n","[Evaluation] step 4090 / 7500 | acc. = 11.78\n","[Evaluation] step 4095 / 7500 | acc. = 11.78\n","[Evaluation] step 4100 / 7500 | acc. = 11.77\n","[Evaluation] step 4105 / 7500 | acc. = 11.77\n","[Evaluation] step 4110 / 7500 | acc. = 11.77\n","[Evaluation] step 4115 / 7500 | acc. = 11.77\n","[Evaluation] step 4120 / 7500 | acc. = 11.78\n","[Evaluation] step 4125 / 7500 | acc. = 11.78\n","[Evaluation] step 4130 / 7500 | acc. = 11.78\n","[Evaluation] step 4135 / 7500 | acc. = 11.79\n","[Evaluation] step 4140 / 7500 | acc. = 11.79\n","[Evaluation] step 4145 / 7500 | acc. = 11.80\n","[Evaluation] step 4150 / 7500 | acc. = 11.80\n","[Evaluation] step 4155 / 7500 | acc. = 11.80\n","[Evaluation] step 4160 / 7500 | acc. = 11.80\n","[Evaluation] step 4165 / 7500 | acc. = 11.80\n","[Evaluation] step 4170 / 7500 | acc. = 11.80\n","[Evaluation] step 4175 / 7500 | acc. = 11.80\n","[Evaluation] step 4180 / 7500 | acc. = 11.80\n","[Evaluation] step 4185 / 7500 | acc. = 11.81\n","[Evaluation] step 4190 / 7500 | acc. = 11.81\n","[Evaluation] step 4195 / 7500 | acc. = 11.81\n","[Evaluation] step 4200 / 7500 | acc. = 11.81\n","[Evaluation] step 4205 / 7500 | acc. = 11.80\n","[Evaluation] step 4210 / 7500 | acc. = 11.81\n","[Evaluation] step 4215 / 7500 | acc. = 11.81\n","[Evaluation] step 4220 / 7500 | acc. = 11.82\n","[Evaluation] step 4225 / 7500 | acc. = 11.81\n","[Evaluation] step 4230 / 7500 | acc. = 11.83\n","[Evaluation] step 4235 / 7500 | acc. = 11.83\n","[Evaluation] step 4240 / 7500 | acc. = 11.83\n","[Evaluation] step 4245 / 7500 | acc. = 11.83\n","[Evaluation] step 4250 / 7500 | acc. = 11.83\n","[Evaluation] step 4255 / 7500 | acc. = 11.83\n","[Evaluation] step 4260 / 7500 | acc. = 11.83\n","[Evaluation] step 4265 / 7500 | acc. = 11.83\n","[Evaluation] step 4270 / 7500 | acc. = 11.84\n","[Evaluation] step 4275 / 7500 | acc. = 11.83\n","[Evaluation] step 4280 / 7500 | acc. = 11.83\n","[Evaluation] step 4285 / 7500 | acc. = 11.84\n","[Evaluation] step 4290 / 7500 | acc. = 11.84\n","[Evaluation] step 4295 / 7500 | acc. = 11.84\n","[Evaluation] step 4300 / 7500 | acc. = 11.84\n","[Evaluation] step 4305 / 7500 | acc. = 11.84\n","[Evaluation] step 4310 / 7500 | acc. = 11.84\n","[Evaluation] step 4315 / 7500 | acc. = 11.84\n","[Evaluation] step 4320 / 7500 | acc. = 11.84\n","[Evaluation] step 4325 / 7500 | acc. = 11.83\n","[Evaluation] step 4330 / 7500 | acc. = 11.84\n","[Evaluation] step 4335 / 7500 | acc. = 11.84\n","[Evaluation] step 4340 / 7500 | acc. = 11.83\n","[Evaluation] step 4345 / 7500 | acc. = 11.83\n","[Evaluation] step 4350 / 7500 | acc. = 11.84\n","[Evaluation] step 4355 / 7500 | acc. = 11.84\n","[Evaluation] step 4360 / 7500 | acc. = 11.84\n","[Evaluation] step 4365 / 7500 | acc. = 11.85\n","[Evaluation] step 4370 / 7500 | acc. = 11.85\n","[Evaluation] step 4375 / 7500 | acc. = 11.85\n","[Evaluation] step 4380 / 7500 | acc. = 11.85\n","[Evaluation] step 4385 / 7500 | acc. = 11.86\n","[Evaluation] step 4390 / 7500 | acc. = 11.86\n","[Evaluation] step 4395 / 7500 | acc. = 11.86\n","[Evaluation] step 4400 / 7500 | acc. = 11.86\n","[Evaluation] step 4405 / 7500 | acc. = 11.86\n","[Evaluation] step 4410 / 7500 | acc. = 11.85\n","[Evaluation] step 4415 / 7500 | acc. = 11.85\n","[Evaluation] step 4420 / 7500 | acc. = 11.86\n","[Evaluation] step 4425 / 7500 | acc. = 11.85\n","[Evaluation] step 4430 / 7500 | acc. = 11.85\n","[Evaluation] step 4435 / 7500 | acc. = 11.86\n","[Evaluation] step 4440 / 7500 | acc. = 11.86\n","[Evaluation] step 4445 / 7500 | acc. = 11.86\n","[Evaluation] step 4450 / 7500 | acc. = 11.86\n","[Evaluation] step 4455 / 7500 | acc. = 11.87\n","[Evaluation] step 4460 / 7500 | acc. = 11.86\n","[Evaluation] step 4465 / 7500 | acc. = 11.87\n","[Evaluation] step 4470 / 7500 | acc. = 11.87\n","[Evaluation] step 4475 / 7500 | acc. = 11.88\n","[Evaluation] step 4480 / 7500 | acc. = 11.87\n","[Evaluation] step 4485 / 7500 | acc. = 11.86\n","[Evaluation] step 4490 / 7500 | acc. = 11.87\n","[Evaluation] step 4495 / 7500 | acc. = 11.88\n","[Evaluation] step 4500 / 7500 | acc. = 11.87\n","[Evaluation] step 4505 / 7500 | acc. = 11.88\n","[Evaluation] step 4510 / 7500 | acc. = 11.88\n","[Evaluation] step 4515 / 7500 | acc. = 11.89\n","[Evaluation] step 4520 / 7500 | acc. = 11.89\n","[Evaluation] step 4525 / 7500 | acc. = 11.90\n","[Evaluation] step 4530 / 7500 | acc. = 11.89\n","[Evaluation] step 4535 / 7500 | acc. = 11.89\n","[Evaluation] step 4540 / 7500 | acc. = 11.89\n","[Evaluation] step 4545 / 7500 | acc. = 11.89\n","[Evaluation] step 4550 / 7500 | acc. = 11.88\n","[Evaluation] step 4555 / 7500 | acc. = 11.88\n","[Evaluation] step 4560 / 7500 | acc. = 11.89\n","[Evaluation] step 4565 / 7500 | acc. = 11.89\n","[Evaluation] step 4570 / 7500 | acc. = 11.89\n","[Evaluation] step 4575 / 7500 | acc. = 11.90\n","[Evaluation] step 4580 / 7500 | acc. = 11.89\n","[Evaluation] step 4585 / 7500 | acc. = 11.90\n","[Evaluation] step 4590 / 7500 | acc. = 11.88\n","[Evaluation] step 4595 / 7500 | acc. = 11.88\n","[Evaluation] step 4600 / 7500 | acc. = 11.88\n","[Evaluation] step 4605 / 7500 | acc. = 11.88\n","[Evaluation] step 4610 / 7500 | acc. = 11.88\n","[Evaluation] step 4615 / 7500 | acc. = 11.87\n","[Evaluation] step 4620 / 7500 | acc. = 11.88\n","[Evaluation] step 4625 / 7500 | acc. = 11.87\n","[Evaluation] step 4630 / 7500 | acc. = 11.87\n","[Evaluation] step 4635 / 7500 | acc. = 11.87\n","[Evaluation] step 4640 / 7500 | acc. = 11.87\n","[Evaluation] step 4645 / 7500 | acc. = 11.86\n","[Evaluation] step 4650 / 7500 | acc. = 11.86\n","[Evaluation] step 4655 / 7500 | acc. = 11.85\n","[Evaluation] step 4660 / 7500 | acc. = 11.86\n","[Evaluation] step 4665 / 7500 | acc. = 11.87\n","[Evaluation] step 4670 / 7500 | acc. = 11.88\n","[Evaluation] step 4675 / 7500 | acc. = 11.87\n","[Evaluation] step 4680 / 7500 | acc. = 11.87\n","[Evaluation] step 4685 / 7500 | acc. = 11.86\n","[Evaluation] step 4690 / 7500 | acc. = 11.87\n","[Evaluation] step 4695 / 7500 | acc. = 11.88\n","[Evaluation] step 4700 / 7500 | acc. = 11.87\n","[Evaluation] step 4705 / 7500 | acc. = 11.87\n","[Evaluation] step 4710 / 7500 | acc. = 11.87\n","[Evaluation] step 4715 / 7500 | acc. = 11.87\n","[Evaluation] step 4720 / 7500 | acc. = 11.88\n","[Evaluation] step 4725 / 7500 | acc. = 11.87\n","[Evaluation] step 4730 / 7500 | acc. = 11.88\n","[Evaluation] step 4735 / 7500 | acc. = 11.88\n","[Evaluation] step 4740 / 7500 | acc. = 11.88\n","[Evaluation] step 4745 / 7500 | acc. = 11.88\n","[Evaluation] step 4750 / 7500 | acc. = 11.89\n","[Evaluation] step 4755 / 7500 | acc. = 11.90\n","[Evaluation] step 4760 / 7500 | acc. = 11.91\n","[Evaluation] step 4765 / 7500 | acc. = 11.91\n","[Evaluation] step 4770 / 7500 | acc. = 11.90\n","[Evaluation] step 4775 / 7500 | acc. = 11.90\n","[Evaluation] step 4780 / 7500 | acc. = 11.90\n","[Evaluation] step 4785 / 7500 | acc. = 11.90\n","[Evaluation] step 4790 / 7500 | acc. = 11.91\n","[Evaluation] step 4795 / 7500 | acc. = 11.90\n","[Evaluation] step 4800 / 7500 | acc. = 11.91\n","[Evaluation] step 4805 / 7500 | acc. = 11.91\n","[Evaluation] step 4810 / 7500 | acc. = 11.91\n","[Evaluation] step 4815 / 7500 | acc. = 11.91\n","[Evaluation] step 4820 / 7500 | acc. = 11.91\n","[Evaluation] step 4825 / 7500 | acc. = 11.90\n","[Evaluation] step 4830 / 7500 | acc. = 11.90\n","[Evaluation] step 4835 / 7500 | acc. = 11.90\n","[Evaluation] step 4840 / 7500 | acc. = 11.90\n","[Evaluation] step 4845 / 7500 | acc. = 11.90\n","[Evaluation] step 4850 / 7500 | acc. = 11.91\n","[Evaluation] step 4855 / 7500 | acc. = 11.90\n","[Evaluation] step 4860 / 7500 | acc. = 11.91\n","[Evaluation] step 4865 / 7500 | acc. = 11.91\n","[Evaluation] step 4870 / 7500 | acc. = 11.91\n","[Evaluation] step 4875 / 7500 | acc. = 11.91\n","[Evaluation] step 4880 / 7500 | acc. = 11.92\n","[Evaluation] step 4885 / 7500 | acc. = 11.92\n","[Evaluation] step 4890 / 7500 | acc. = 11.92\n","[Evaluation] step 4895 / 7500 | acc. = 11.93\n","[Evaluation] step 4900 / 7500 | acc. = 11.93\n","[Evaluation] step 4905 / 7500 | acc. = 11.94\n","[Evaluation] step 4910 / 7500 | acc. = 11.93\n","[Evaluation] step 4915 / 7500 | acc. = 11.93\n","[Evaluation] step 4920 / 7500 | acc. = 11.93\n","[Evaluation] step 4925 / 7500 | acc. = 11.94\n","[Evaluation] step 4930 / 7500 | acc. = 11.94\n","[Evaluation] step 4935 / 7500 | acc. = 11.94\n","[Evaluation] step 4940 / 7500 | acc. = 11.94\n","[Evaluation] step 4945 / 7500 | acc. = 11.94\n","[Evaluation] step 4950 / 7500 | acc. = 11.94\n","[Evaluation] step 4955 / 7500 | acc. = 11.94\n","[Evaluation] step 4960 / 7500 | acc. = 11.94\n","[Evaluation] step 4965 / 7500 | acc. = 11.94\n","[Evaluation] step 4970 / 7500 | acc. = 11.94\n","[Evaluation] step 4975 / 7500 | acc. = 11.94\n","[Evaluation] step 4980 / 7500 | acc. = 11.93\n","[Evaluation] step 4985 / 7500 | acc. = 11.93\n","[Evaluation] step 4990 / 7500 | acc. = 11.92\n","[Evaluation] step 4995 / 7500 | acc. = 11.92\n","[Evaluation] step 5000 / 7500 | acc. = 11.92\n","[Evaluation] step 5005 / 7500 | acc. = 11.92\n","[Evaluation] step 5010 / 7500 | acc. = 11.92\n","[Evaluation] step 5015 / 7500 | acc. = 11.91\n","[Evaluation] step 5020 / 7500 | acc. = 11.91\n","[Evaluation] step 5025 / 7500 | acc. = 11.92\n","[Evaluation] step 5030 / 7500 | acc. = 11.92\n","[Evaluation] step 5035 / 7500 | acc. = 11.92\n","[Evaluation] step 5040 / 7500 | acc. = 11.91\n","[Evaluation] step 5045 / 7500 | acc. = 11.91\n","[Evaluation] step 5050 / 7500 | acc. = 11.92\n","[Evaluation] step 5055 / 7500 | acc. = 11.92\n","[Evaluation] step 5060 / 7500 | acc. = 11.92\n","[Evaluation] step 5065 / 7500 | acc. = 11.93\n","[Evaluation] step 5070 / 7500 | acc. = 11.94\n","[Evaluation] step 5075 / 7500 | acc. = 11.94\n","[Evaluation] step 5080 / 7500 | acc. = 11.94\n","[Evaluation] step 5085 / 7500 | acc. = 11.94\n","[Evaluation] step 5090 / 7500 | acc. = 11.94\n","[Evaluation] step 5095 / 7500 | acc. = 11.94\n","[Evaluation] step 5100 / 7500 | acc. = 11.95\n","[Evaluation] step 5105 / 7500 | acc. = 11.95\n","[Evaluation] step 5110 / 7500 | acc. = 11.94\n","[Evaluation] step 5115 / 7500 | acc. = 11.94\n","[Evaluation] step 5120 / 7500 | acc. = 11.95\n","[Evaluation] step 5125 / 7500 | acc. = 11.95\n","[Evaluation] step 5130 / 7500 | acc. = 11.95\n","[Evaluation] step 5135 / 7500 | acc. = 11.95\n","[Evaluation] step 5140 / 7500 | acc. = 11.94\n","[Evaluation] step 5145 / 7500 | acc. = 11.94\n","[Evaluation] step 5150 / 7500 | acc. = 11.94\n","[Evaluation] step 5155 / 7500 | acc. = 11.94\n","[Evaluation] step 5160 / 7500 | acc. = 11.93\n","[Evaluation] step 5165 / 7500 | acc. = 11.93\n","[Evaluation] step 5170 / 7500 | acc. = 11.93\n","[Evaluation] step 5175 / 7500 | acc. = 11.92\n","[Evaluation] step 5180 / 7500 | acc. = 11.92\n","[Evaluation] step 5185 / 7500 | acc. = 11.92\n","[Evaluation] step 5190 / 7500 | acc. = 11.92\n","[Evaluation] step 5195 / 7500 | acc. = 11.92\n","[Evaluation] step 5200 / 7500 | acc. = 11.92\n","[Evaluation] step 5205 / 7500 | acc. = 11.92\n","[Evaluation] step 5210 / 7500 | acc. = 11.93\n","[Evaluation] step 5215 / 7500 | acc. = 11.93\n","[Evaluation] step 5220 / 7500 | acc. = 11.94\n","[Evaluation] step 5225 / 7500 | acc. = 11.94\n","[Evaluation] step 5230 / 7500 | acc. = 11.94\n","[Evaluation] step 5235 / 7500 | acc. = 11.94\n","[Evaluation] step 5240 / 7500 | acc. = 11.94\n","[Evaluation] step 5245 / 7500 | acc. = 11.94\n","[Evaluation] step 5250 / 7500 | acc. = 11.93\n","[Evaluation] step 5255 / 7500 | acc. = 11.93\n","[Evaluation] step 5260 / 7500 | acc. = 11.93\n","[Evaluation] step 5265 / 7500 | acc. = 11.93\n","[Evaluation] step 5270 / 7500 | acc. = 11.93\n","[Evaluation] step 5275 / 7500 | acc. = 11.92\n","[Evaluation] step 5280 / 7500 | acc. = 11.93\n","[Evaluation] step 5285 / 7500 | acc. = 11.93\n","[Evaluation] step 5290 / 7500 | acc. = 11.93\n","[Evaluation] step 5295 / 7500 | acc. = 11.93\n","[Evaluation] step 5300 / 7500 | acc. = 11.93\n","[Evaluation] step 5305 / 7500 | acc. = 11.93\n","[Evaluation] step 5310 / 7500 | acc. = 11.92\n","[Evaluation] step 5315 / 7500 | acc. = 11.92\n","[Evaluation] step 5320 / 7500 | acc. = 11.91\n","[Evaluation] step 5325 / 7500 | acc. = 11.92\n","[Evaluation] step 5330 / 7500 | acc. = 11.92\n","[Evaluation] step 5335 / 7500 | acc. = 11.91\n","[Evaluation] step 5340 / 7500 | acc. = 11.91\n","[Evaluation] step 5345 / 7500 | acc. = 11.91\n","[Evaluation] step 5350 / 7500 | acc. = 11.91\n","[Evaluation] step 5355 / 7500 | acc. = 11.91\n","[Evaluation] step 5360 / 7500 | acc. = 11.91\n","[Evaluation] step 5365 / 7500 | acc. = 11.91\n","[Evaluation] step 5370 / 7500 | acc. = 11.91\n","[Evaluation] step 5375 / 7500 | acc. = 11.91\n","[Evaluation] step 5380 / 7500 | acc. = 11.90\n","[Evaluation] step 5385 / 7500 | acc. = 11.91\n","[Evaluation] step 5390 / 7500 | acc. = 11.91\n","[Evaluation] step 5395 / 7500 | acc. = 11.91\n","[Evaluation] step 5400 / 7500 | acc. = 11.90\n","[Evaluation] step 5405 / 7500 | acc. = 11.89\n","[Evaluation] step 5410 / 7500 | acc. = 11.89\n","[Evaluation] step 5415 / 7500 | acc. = 11.89\n","[Evaluation] step 5420 / 7500 | acc. = 11.89\n","[Evaluation] step 5425 / 7500 | acc. = 11.88\n","[Evaluation] step 5430 / 7500 | acc. = 11.89\n","[Evaluation] step 5435 / 7500 | acc. = 11.88\n","[Evaluation] step 5440 / 7500 | acc. = 11.88\n","[Evaluation] step 5445 / 7500 | acc. = 11.89\n","[Evaluation] step 5450 / 7500 | acc. = 11.89\n","[Evaluation] step 5455 / 7500 | acc. = 11.90\n","[Evaluation] step 5460 / 7500 | acc. = 11.90\n","[Evaluation] step 5465 / 7500 | acc. = 11.90\n","[Evaluation] step 5470 / 7500 | acc. = 11.91\n","[Evaluation] step 5475 / 7500 | acc. = 11.91\n","[Evaluation] step 5480 / 7500 | acc. = 11.91\n","[Evaluation] step 5485 / 7500 | acc. = 11.91\n","[Evaluation] step 5490 / 7500 | acc. = 11.90\n","[Evaluation] step 5495 / 7500 | acc. = 11.90\n","[Evaluation] step 5500 / 7500 | acc. = 11.90\n","[Evaluation] step 5505 / 7500 | acc. = 11.90\n","[Evaluation] step 5510 / 7500 | acc. = 11.90\n","[Evaluation] step 5515 / 7500 | acc. = 11.90\n","[Evaluation] step 5520 / 7500 | acc. = 11.90\n","[Evaluation] step 5525 / 7500 | acc. = 11.91\n","[Evaluation] step 5530 / 7500 | acc. = 11.91\n","[Evaluation] step 5535 / 7500 | acc. = 11.91\n","[Evaluation] step 5540 / 7500 | acc. = 11.91\n","[Evaluation] step 5545 / 7500 | acc. = 11.92\n","[Evaluation] step 5550 / 7500 | acc. = 11.92\n","[Evaluation] step 5555 / 7500 | acc. = 11.92\n","[Evaluation] step 5560 / 7500 | acc. = 11.92\n","[Evaluation] step 5565 / 7500 | acc. = 11.92\n","[Evaluation] step 5570 / 7500 | acc. = 11.91\n","[Evaluation] step 5575 / 7500 | acc. = 11.91\n","[Evaluation] step 5580 / 7500 | acc. = 11.91\n","[Evaluation] step 5585 / 7500 | acc. = 11.91\n","[Evaluation] step 5590 / 7500 | acc. = 11.91\n","[Evaluation] step 5595 / 7500 | acc. = 11.91\n","[Evaluation] step 5600 / 7500 | acc. = 11.91\n","[Evaluation] step 5605 / 7500 | acc. = 11.91\n","[Evaluation] step 5610 / 7500 | acc. = 11.90\n","[Evaluation] step 5615 / 7500 | acc. = 11.90\n","[Evaluation] step 5620 / 7500 | acc. = 11.90\n","[Evaluation] step 5625 / 7500 | acc. = 11.90\n","[Evaluation] step 5630 / 7500 | acc. = 11.90\n","[Evaluation] step 5635 / 7500 | acc. = 11.89\n","[Evaluation] step 5640 / 7500 | acc. = 11.90\n","[Evaluation] step 5645 / 7500 | acc. = 11.90\n","[Evaluation] step 5650 / 7500 | acc. = 11.90\n","[Evaluation] step 5655 / 7500 | acc. = 11.90\n","[Evaluation] step 5660 / 7500 | acc. = 11.90\n","[Evaluation] step 5665 / 7500 | acc. = 11.90\n","[Evaluation] step 5670 / 7500 | acc. = 11.90\n","[Evaluation] step 5675 / 7500 | acc. = 11.90\n","[Evaluation] step 5680 / 7500 | acc. = 11.90\n","[Evaluation] step 5685 / 7500 | acc. = 11.90\n","[Evaluation] step 5690 / 7500 | acc. = 11.90\n","[Evaluation] step 5695 / 7500 | acc. = 11.90\n","[Evaluation] step 5700 / 7500 | acc. = 11.90\n","[Evaluation] step 5705 / 7500 | acc. = 11.90\n","[Evaluation] step 5710 / 7500 | acc. = 11.91\n","[Evaluation] step 5715 / 7500 | acc. = 11.90\n","[Evaluation] step 5720 / 7500 | acc. = 11.90\n","[Evaluation] step 5725 / 7500 | acc. = 11.90\n","[Evaluation] step 5730 / 7500 | acc. = 11.90\n","[Evaluation] step 5735 / 7500 | acc. = 11.90\n","[Evaluation] step 5740 / 7500 | acc. = 11.90\n","[Evaluation] step 5745 / 7500 | acc. = 11.91\n","[Evaluation] step 5750 / 7500 | acc. = 11.91\n","[Evaluation] step 5755 / 7500 | acc. = 11.91\n","[Evaluation] step 5760 / 7500 | acc. = 11.90\n","[Evaluation] step 5765 / 7500 | acc. = 11.90\n","[Evaluation] step 5770 / 7500 | acc. = 11.90\n","[Evaluation] step 5775 / 7500 | acc. = 11.90\n","[Evaluation] step 5780 / 7500 | acc. = 11.90\n","[Evaluation] step 5785 / 7500 | acc. = 11.90\n","[Evaluation] step 5790 / 7500 | acc. = 11.91\n","[Evaluation] step 5795 / 7500 | acc. = 11.91\n","[Evaluation] step 5800 / 7500 | acc. = 11.91\n","[Evaluation] step 5805 / 7500 | acc. = 11.91\n","[Evaluation] step 5810 / 7500 | acc. = 11.91\n","[Evaluation] step 5815 / 7500 | acc. = 11.91\n","[Evaluation] step 5820 / 7500 | acc. = 11.91\n","[Evaluation] step 5825 / 7500 | acc. = 11.91\n","[Evaluation] step 5830 / 7500 | acc. = 11.90\n","[Evaluation] step 5835 / 7500 | acc. = 11.90\n","[Evaluation] step 5840 / 7500 | acc. = 11.90\n","[Evaluation] step 5845 / 7500 | acc. = 11.90\n","[Evaluation] step 5850 / 7500 | acc. = 11.90\n","[Evaluation] step 5855 / 7500 | acc. = 11.90\n","[Evaluation] step 5860 / 7500 | acc. = 11.91\n","[Evaluation] step 5865 / 7500 | acc. = 11.91\n","[Evaluation] step 5870 / 7500 | acc. = 11.91\n","[Evaluation] step 5875 / 7500 | acc. = 11.91\n","[Evaluation] step 5880 / 7500 | acc. = 11.90\n","[Evaluation] step 5885 / 7500 | acc. = 11.90\n","[Evaluation] step 5890 / 7500 | acc. = 11.90\n","[Evaluation] step 5895 / 7500 | acc. = 11.90\n","[Evaluation] step 5900 / 7500 | acc. = 11.90\n","[Evaluation] step 5905 / 7500 | acc. = 11.90\n","[Evaluation] step 5910 / 7500 | acc. = 11.89\n","[Evaluation] step 5915 / 7500 | acc. = 11.89\n","[Evaluation] step 5920 / 7500 | acc. = 11.90\n","[Evaluation] step 5925 / 7500 | acc. = 11.90\n","[Evaluation] step 5930 / 7500 | acc. = 11.90\n","[Evaluation] step 5935 / 7500 | acc. = 11.90\n","[Evaluation] step 5940 / 7500 | acc. = 11.91\n","[Evaluation] step 5945 / 7500 | acc. = 11.91\n","[Evaluation] step 5950 / 7500 | acc. = 11.91\n","[Evaluation] step 5955 / 7500 | acc. = 11.91\n","[Evaluation] step 5960 / 7500 | acc. = 11.90\n","[Evaluation] step 5965 / 7500 | acc. = 11.90\n","[Evaluation] step 5970 / 7500 | acc. = 11.91\n","[Evaluation] step 5975 / 7500 | acc. = 11.91\n","[Evaluation] step 5980 / 7500 | acc. = 11.91\n","[Evaluation] step 5985 / 7500 | acc. = 11.91\n","[Evaluation] step 5990 / 7500 | acc. = 11.90\n","[Evaluation] step 5995 / 7500 | acc. = 11.89\n","[Evaluation] step 6000 / 7500 | acc. = 11.89\n","[Evaluation] step 6005 / 7500 | acc. = 11.90\n","[Evaluation] step 6010 / 7500 | acc. = 11.90\n","[Evaluation] step 6015 / 7500 | acc. = 11.90\n","[Evaluation] step 6020 / 7500 | acc. = 11.89\n","[Evaluation] step 6025 / 7500 | acc. = 11.89\n","[Evaluation] step 6030 / 7500 | acc. = 11.89\n","[Evaluation] step 6035 / 7500 | acc. = 11.89\n","[Evaluation] step 6040 / 7500 | acc. = 11.89\n","[Evaluation] step 6045 / 7500 | acc. = 11.88\n","[Evaluation] step 6050 / 7500 | acc. = 11.88\n","[Evaluation] step 6055 / 7500 | acc. = 11.88\n","[Evaluation] step 6060 / 7500 | acc. = 11.88\n","[Evaluation] step 6065 / 7500 | acc. = 11.88\n","[Evaluation] step 6070 / 7500 | acc. = 11.88\n","[Evaluation] step 6075 / 7500 | acc. = 11.88\n","[Evaluation] step 6080 / 7500 | acc. = 11.88\n","[Evaluation] step 6085 / 7500 | acc. = 11.88\n","[Evaluation] step 6090 / 7500 | acc. = 11.87\n","[Evaluation] step 6095 / 7500 | acc. = 11.87\n","[Evaluation] step 6100 / 7500 | acc. = 11.87\n","[Evaluation] step 6105 / 7500 | acc. = 11.87\n","[Evaluation] step 6110 / 7500 | acc. = 11.87\n","[Evaluation] step 6115 / 7500 | acc. = 11.87\n","[Evaluation] step 6120 / 7500 | acc. = 11.87\n","[Evaluation] step 6125 / 7500 | acc. = 11.87\n","[Evaluation] step 6130 / 7500 | acc. = 11.87\n","[Evaluation] step 6135 / 7500 | acc. = 11.88\n","[Evaluation] step 6140 / 7500 | acc. = 11.89\n","[Evaluation] step 6145 / 7500 | acc. = 11.89\n","[Evaluation] step 6150 / 7500 | acc. = 11.89\n","[Evaluation] step 6155 / 7500 | acc. = 11.89\n","[Evaluation] step 6160 / 7500 | acc. = 11.89\n","[Evaluation] step 6165 / 7500 | acc. = 11.88\n","[Evaluation] step 6170 / 7500 | acc. = 11.89\n","[Evaluation] step 6175 / 7500 | acc. = 11.90\n","[Evaluation] step 6180 / 7500 | acc. = 11.89\n","[Evaluation] step 6185 / 7500 | acc. = 11.89\n","[Evaluation] step 6190 / 7500 | acc. = 11.90\n","[Evaluation] step 6195 / 7500 | acc. = 11.89\n","[Evaluation] step 6200 / 7500 | acc. = 11.90\n","[Evaluation] step 6205 / 7500 | acc. = 11.90\n","[Evaluation] step 6210 / 7500 | acc. = 11.89\n","[Evaluation] step 6215 / 7500 | acc. = 11.89\n","[Evaluation] step 6220 / 7500 | acc. = 11.89\n","[Evaluation] step 6225 / 7500 | acc. = 11.89\n","[Evaluation] step 6230 / 7500 | acc. = 11.88\n","[Evaluation] step 6235 / 7500 | acc. = 11.88\n","[Evaluation] step 6240 / 7500 | acc. = 11.89\n","[Evaluation] step 6245 / 7500 | acc. = 11.88\n","[Evaluation] step 6250 / 7500 | acc. = 11.88\n","[Evaluation] step 6255 / 7500 | acc. = 11.89\n","[Evaluation] step 6260 / 7500 | acc. = 11.88\n","[Evaluation] step 6265 / 7500 | acc. = 11.89\n","[Evaluation] step 6270 / 7500 | acc. = 11.89\n","[Evaluation] step 6275 / 7500 | acc. = 11.89\n","[Evaluation] step 6280 / 7500 | acc. = 11.88\n","[Evaluation] step 6285 / 7500 | acc. = 11.88\n","[Evaluation] step 6290 / 7500 | acc. = 11.88\n","[Evaluation] step 6295 / 7500 | acc. = 11.88\n","[Evaluation] step 6300 / 7500 | acc. = 11.88\n","[Evaluation] step 6305 / 7500 | acc. = 11.88\n","[Evaluation] step 6310 / 7500 | acc. = 11.88\n","[Evaluation] step 6315 / 7500 | acc. = 11.88\n","[Evaluation] step 6320 / 7500 | acc. = 11.88\n","[Evaluation] step 6325 / 7500 | acc. = 11.87\n","[Evaluation] step 6330 / 7500 | acc. = 11.87\n","[Evaluation] step 6335 / 7500 | acc. = 11.88\n","[Evaluation] step 6340 / 7500 | acc. = 11.88\n","[Evaluation] step 6345 / 7500 | acc. = 11.88\n","[Evaluation] step 6350 / 7500 | acc. = 11.88\n","[Evaluation] step 6355 / 7500 | acc. = 11.88\n","[Evaluation] step 6360 / 7500 | acc. = 11.88\n","[Evaluation] step 6365 / 7500 | acc. = 11.88\n","[Evaluation] step 6370 / 7500 | acc. = 11.87\n","[Evaluation] step 6375 / 7500 | acc. = 11.87\n","[Evaluation] step 6380 / 7500 | acc. = 11.88\n","[Evaluation] step 6385 / 7500 | acc. = 11.88\n","[Evaluation] step 6390 / 7500 | acc. = 11.89\n","[Evaluation] step 6395 / 7500 | acc. = 11.89\n","[Evaluation] step 6400 / 7500 | acc. = 11.89\n","[Evaluation] step 6405 / 7500 | acc. = 11.89\n","[Evaluation] step 6410 / 7500 | acc. = 11.88\n","[Evaluation] step 6415 / 7500 | acc. = 11.89\n","[Evaluation] step 6420 / 7500 | acc. = 11.89\n","[Evaluation] step 6425 / 7500 | acc. = 11.89\n","[Evaluation] step 6430 / 7500 | acc. = 11.89\n","[Evaluation] step 6435 / 7500 | acc. = 11.89\n","[Evaluation] step 6440 / 7500 | acc. = 11.88\n","[Evaluation] step 6445 / 7500 | acc. = 11.89\n","[Evaluation] step 6450 / 7500 | acc. = 11.88\n","[Evaluation] step 6455 / 7500 | acc. = 11.88\n","[Evaluation] step 6460 / 7500 | acc. = 11.88\n","[Evaluation] step 6465 / 7500 | acc. = 11.87\n","[Evaluation] step 6470 / 7500 | acc. = 11.87\n","[Evaluation] step 6475 / 7500 | acc. = 11.87\n","[Evaluation] step 6480 / 7500 | acc. = 11.87\n","[Evaluation] step 6485 / 7500 | acc. = 11.87\n","[Evaluation] step 6490 / 7500 | acc. = 11.86\n","[Evaluation] step 6495 / 7500 | acc. = 11.87\n","[Evaluation] step 6500 / 7500 | acc. = 11.87\n","[Evaluation] step 6505 / 7500 | acc. = 11.87\n","[Evaluation] step 6510 / 7500 | acc. = 11.86\n","[Evaluation] step 6515 / 7500 | acc. = 11.87\n","[Evaluation] step 6520 / 7500 | acc. = 11.87\n","[Evaluation] step 6525 / 7500 | acc. = 11.87\n","[Evaluation] step 6530 / 7500 | acc. = 11.87\n","[Evaluation] step 6535 / 7500 | acc. = 11.88\n","[Evaluation] step 6540 / 7500 | acc. = 11.88\n","[Evaluation] step 6545 / 7500 | acc. = 11.88\n","[Evaluation] step 6550 / 7500 | acc. = 11.88\n","[Evaluation] step 6555 / 7500 | acc. = 11.87\n","[Evaluation] step 6560 / 7500 | acc. = 11.87\n","[Evaluation] step 6565 / 7500 | acc. = 11.88\n","[Evaluation] step 6570 / 7500 | acc. = 11.88\n","[Evaluation] step 6575 / 7500 | acc. = 11.87\n","[Evaluation] step 6580 / 7500 | acc. = 11.88\n","[Evaluation] step 6585 / 7500 | acc. = 11.88\n","[Evaluation] step 6590 / 7500 | acc. = 11.89\n","[Evaluation] step 6595 / 7500 | acc. = 11.90\n","[Evaluation] step 6600 / 7500 | acc. = 11.90\n","[Evaluation] step 6605 / 7500 | acc. = 11.90\n","[Evaluation] step 6610 / 7500 | acc. = 11.89\n","[Evaluation] step 6615 / 7500 | acc. = 11.89\n","[Evaluation] step 6620 / 7500 | acc. = 11.89\n","[Evaluation] step 6625 / 7500 | acc. = 11.89\n","[Evaluation] step 6630 / 7500 | acc. = 11.89\n","[Evaluation] step 6635 / 7500 | acc. = 11.89\n","[Evaluation] step 6640 / 7500 | acc. = 11.89\n","[Evaluation] step 6645 / 7500 | acc. = 11.88\n","[Evaluation] step 6650 / 7500 | acc. = 11.88\n","[Evaluation] step 6655 / 7500 | acc. = 11.89\n","[Evaluation] step 6660 / 7500 | acc. = 11.89\n","[Evaluation] step 6665 / 7500 | acc. = 11.89\n","[Evaluation] step 6670 / 7500 | acc. = 11.90\n","[Evaluation] step 6675 / 7500 | acc. = 11.89\n","[Evaluation] step 6680 / 7500 | acc. = 11.89\n","[Evaluation] step 6685 / 7500 | acc. = 11.90\n","[Evaluation] step 6690 / 7500 | acc. = 11.90\n","[Evaluation] step 6695 / 7500 | acc. = 11.90\n","[Evaluation] step 6700 / 7500 | acc. = 11.90\n","[Evaluation] step 6705 / 7500 | acc. = 11.89\n","[Evaluation] step 6710 / 7500 | acc. = 11.90\n","[Evaluation] step 6715 / 7500 | acc. = 11.90\n","[Evaluation] step 6720 / 7500 | acc. = 11.90\n","[Evaluation] step 6725 / 7500 | acc. = 11.90\n","[Evaluation] step 6730 / 7500 | acc. = 11.90\n","[Evaluation] step 6735 / 7500 | acc. = 11.90\n","[Evaluation] step 6740 / 7500 | acc. = 11.90\n","[Evaluation] step 6745 / 7500 | acc. = 11.90\n","[Evaluation] step 6750 / 7500 | acc. = 11.90\n","[Evaluation] step 6755 / 7500 | acc. = 11.91\n","[Evaluation] step 6760 / 7500 | acc. = 11.91\n","[Evaluation] step 6765 / 7500 | acc. = 11.90\n","[Evaluation] step 6770 / 7500 | acc. = 11.91\n","[Evaluation] step 6775 / 7500 | acc. = 11.91\n","[Evaluation] step 6780 / 7500 | acc. = 11.91\n","[Evaluation] step 6785 / 7500 | acc. = 11.90\n","[Evaluation] step 6790 / 7500 | acc. = 11.91\n","[Evaluation] step 6795 / 7500 | acc. = 11.91\n","[Evaluation] step 6800 / 7500 | acc. = 11.91\n","[Evaluation] step 6805 / 7500 | acc. = 11.92\n","[Evaluation] step 6810 / 7500 | acc. = 11.92\n","[Evaluation] step 6815 / 7500 | acc. = 11.92\n","[Evaluation] step 6820 / 7500 | acc. = 11.92\n","[Evaluation] step 6825 / 7500 | acc. = 11.93\n","[Evaluation] step 6830 / 7500 | acc. = 11.93\n","[Evaluation] step 6835 / 7500 | acc. = 11.94\n","[Evaluation] step 6840 / 7500 | acc. = 11.94\n","[Evaluation] step 6845 / 7500 | acc. = 11.94\n","[Evaluation] step 6850 / 7500 | acc. = 11.93\n","[Evaluation] step 6855 / 7500 | acc. = 11.93\n","[Evaluation] step 6860 / 7500 | acc. = 11.93\n","[Evaluation] step 6865 / 7500 | acc. = 11.93\n","[Evaluation] step 6870 / 7500 | acc. = 11.94\n","[Evaluation] step 6875 / 7500 | acc. = 11.93\n","[Evaluation] step 6880 / 7500 | acc. = 11.93\n","[Evaluation] step 6885 / 7500 | acc. = 11.93\n","[Evaluation] step 6890 / 7500 | acc. = 11.93\n","[Evaluation] step 6895 / 7500 | acc. = 11.93\n","[Evaluation] step 6900 / 7500 | acc. = 11.94\n","[Evaluation] step 6905 / 7500 | acc. = 11.93\n","[Evaluation] step 6910 / 7500 | acc. = 11.93\n","[Evaluation] step 6915 / 7500 | acc. = 11.93\n","[Evaluation] step 6920 / 7500 | acc. = 11.93\n","[Evaluation] step 6925 / 7500 | acc. = 11.93\n","[Evaluation] step 6930 / 7500 | acc. = 11.93\n","[Evaluation] step 6935 / 7500 | acc. = 11.93\n","[Evaluation] step 6940 / 7500 | acc. = 11.93\n","[Evaluation] step 6945 / 7500 | acc. = 11.93\n","[Evaluation] step 6950 / 7500 | acc. = 11.93\n","[Evaluation] step 6955 / 7500 | acc. = 11.93\n","[Evaluation] step 6960 / 7500 | acc. = 11.92\n","[Evaluation] step 6965 / 7500 | acc. = 11.93\n","[Evaluation] step 6970 / 7500 | acc. = 11.93\n","[Evaluation] step 6975 / 7500 | acc. = 11.93\n","[Evaluation] step 6980 / 7500 | acc. = 11.93\n","[Evaluation] step 6985 / 7500 | acc. = 11.94\n","[Evaluation] step 6990 / 7500 | acc. = 11.93\n","[Evaluation] step 6995 / 7500 | acc. = 11.94\n","[Evaluation] step 7000 / 7500 | acc. = 11.93\n","[Evaluation] step 7005 / 7500 | acc. = 11.94\n","[Evaluation] step 7010 / 7500 | acc. = 11.94\n","[Evaluation] step 7015 / 7500 | acc. = 11.94\n","[Evaluation] step 7020 / 7500 | acc. = 11.95\n","[Evaluation] step 7025 / 7500 | acc. = 11.94\n","[Evaluation] step 7030 / 7500 | acc. = 11.94\n","[Evaluation] step 7035 / 7500 | acc. = 11.94\n","[Evaluation] step 7040 / 7500 | acc. = 11.94\n","[Evaluation] step 7045 / 7500 | acc. = 11.94\n","[Evaluation] step 7050 / 7500 | acc. = 11.94\n","[Evaluation] step 7055 / 7500 | acc. = 11.95\n","[Evaluation] step 7060 / 7500 | acc. = 11.94\n","[Evaluation] step 7065 / 7500 | acc. = 11.95\n","[Evaluation] step 7070 / 7500 | acc. = 11.95\n","[Evaluation] step 7075 / 7500 | acc. = 11.95\n","[Evaluation] step 7080 / 7500 | acc. = 11.95\n","[Evaluation] step 7085 / 7500 | acc. = 11.94\n","[Evaluation] step 7090 / 7500 | acc. = 11.94\n","[Evaluation] step 7095 / 7500 | acc. = 11.95\n","[Evaluation] step 7100 / 7500 | acc. = 11.95\n","[Evaluation] step 7105 / 7500 | acc. = 11.95\n","[Evaluation] step 7110 / 7500 | acc. = 11.95\n","[Evaluation] step 7115 / 7500 | acc. = 11.95\n","[Evaluation] step 7120 / 7500 | acc. = 11.95\n","[Evaluation] step 7125 / 7500 | acc. = 11.95\n","[Evaluation] step 7130 / 7500 | acc. = 11.95\n","[Evaluation] step 7135 / 7500 | acc. = 11.95\n","[Evaluation] step 7140 / 7500 | acc. = 11.94\n","[Evaluation] step 7145 / 7500 | acc. = 11.94\n","[Evaluation] step 7150 / 7500 | acc. = 11.94\n","[Evaluation] step 7155 / 7500 | acc. = 11.94\n","[Evaluation] step 7160 / 7500 | acc. = 11.94\n","[Evaluation] step 7165 / 7500 | acc. = 11.94\n","[Evaluation] step 7170 / 7500 | acc. = 11.94\n","[Evaluation] step 7175 / 7500 | acc. = 11.94\n","[Evaluation] step 7180 / 7500 | acc. = 11.94\n","[Evaluation] step 7185 / 7500 | acc. = 11.94\n","[Evaluation] step 7190 / 7500 | acc. = 11.93\n","[Evaluation] step 7195 / 7500 | acc. = 11.93\n","[Evaluation] step 7200 / 7500 | acc. = 11.93\n","[Evaluation] step 7205 / 7500 | acc. = 11.93\n","[Evaluation] step 7210 / 7500 | acc. = 11.93\n","[Evaluation] step 7215 / 7500 | acc. = 11.92\n","[Evaluation] step 7220 / 7500 | acc. = 11.93\n","[Evaluation] step 7225 / 7500 | acc. = 11.93\n","[Evaluation] step 7230 / 7500 | acc. = 11.93\n","[Evaluation] step 7235 / 7500 | acc. = 11.93\n","[Evaluation] step 7240 / 7500 | acc. = 11.93\n","[Evaluation] step 7245 / 7500 | acc. = 11.92\n","[Evaluation] step 7250 / 7500 | acc. = 11.92\n","[Evaluation] step 7255 / 7500 | acc. = 11.92\n","[Evaluation] step 7260 / 7500 | acc. = 11.93\n","[Evaluation] step 7265 / 7500 | acc. = 11.92\n","[Evaluation] step 7270 / 7500 | acc. = 11.92\n","[Evaluation] step 7275 / 7500 | acc. = 11.92\n","[Evaluation] step 7280 / 7500 | acc. = 11.92\n","[Evaluation] step 7285 / 7500 | acc. = 11.92\n","[Evaluation] step 7290 / 7500 | acc. = 11.92\n","[Evaluation] step 7295 / 7500 | acc. = 11.92\n","[Evaluation] step 7300 / 7500 | acc. = 11.93\n","[Evaluation] step 7305 / 7500 | acc. = 11.92\n","[Evaluation] step 7310 / 7500 | acc. = 11.92\n","[Evaluation] step 7315 / 7500 | acc. = 11.93\n","[Evaluation] step 7320 / 7500 | acc. = 11.93\n","[Evaluation] step 7325 / 7500 | acc. = 11.93\n","[Evaluation] step 7330 / 7500 | acc. = 11.93\n","[Evaluation] step 7335 / 7500 | acc. = 11.93\n","[Evaluation] step 7340 / 7500 | acc. = 11.92\n","[Evaluation] step 7345 / 7500 | acc. = 11.93\n","[Evaluation] step 7350 / 7500 | acc. = 11.93\n","[Evaluation] step 7355 / 7500 | acc. = 11.93\n","[Evaluation] step 7360 / 7500 | acc. = 11.92\n","[Evaluation] step 7365 / 7500 | acc. = 11.92\n","[Evaluation] step 7370 / 7500 | acc. = 11.92\n","[Evaluation] step 7375 / 7500 | acc. = 11.92\n","[Evaluation] step 7380 / 7500 | acc. = 11.91\n","[Evaluation] step 7385 / 7500 | acc. = 11.91\n","[Evaluation] step 7390 / 7500 | acc. = 11.91\n","[Evaluation] step 7395 / 7500 | acc. = 11.91\n","[Evaluation] step 7400 / 7500 | acc. = 11.91\n","[Evaluation] step 7405 / 7500 | acc. = 11.91\n","[Evaluation] step 7410 / 7500 | acc. = 11.91\n","[Evaluation] step 7415 / 7500 | acc. = 11.91\n","[Evaluation] step 7420 / 7500 | acc. = 11.92\n","[Evaluation] step 7425 / 7500 | acc. = 11.92\n","[Evaluation] step 7430 / 7500 | acc. = 11.92\n","[Evaluation] step 7435 / 7500 | acc. = 11.92\n","[Evaluation] step 7440 / 7500 | acc. = 11.92\n","[Evaluation] step 7445 / 7500 | acc. = 11.92\n","[Evaluation] step 7450 / 7500 | acc. = 11.92\n","[Evaluation] step 7455 / 7500 | acc. = 11.92\n","[Evaluation] step 7460 / 7500 | acc. = 11.92\n","[Evaluation] step 7465 / 7500 | acc. = 11.92\n","[Evaluation] step 7470 / 7500 | acc. = 11.92\n","[Evaluation] step 7475 / 7500 | acc. = 11.92\n","[Evaluation] step 7480 / 7500 | acc. = 11.92\n","[Evaluation] step 7485 / 7500 | acc. = 11.92\n","[Evaluation] step 7490 / 7500 | acc. = 11.92\n","[Evaluation] step 7495 / 7500 | acc. = 11.92\n","[Evaluation] step 7500 / 7500 | acc. = 11.92\n","\n"," --------------- Average First Failure Round --------------- \n","0.10593939393939394 / 10\n"," --------------- Overall acc. --------------- \n","11.91615888211843\n"," --------------- Overall VD acc. --------------- \n","0.0\n"," --------------- Question Prog. Acc --------------- \n","1.6800224002986708\n"," --------------- Per round Acc --------------- \n","1: 14.654838187071645 %\n","2: 7.115931615454149 %\n","3: 6.344624342450368 %\n","4: 8.471939563154349 %\n","5: 12.079667130430105 %\n","6: 12.7887722039981 %\n","7: 13.746686075309238 %\n","8: 14.829004345491834 %\n","9: 0.9827947510854365 %\n","10: 20.168642209376454 %\n","\n"," --------------- Per question Type Acc --------------- \n","seek-attr-rel-imm: 0.0 %\n","seek-attr-imm2: 8.006713469750974 %\n","count-obj-rel-early: 16.58328113949018 %\n","count-obj-exclude-early: 9.944483746761577 %\n","exist-obj-exclude-early: 40.39644207809081 %\n","seek-attr-early: 2.4501369204508814 %\n","exist-obj-rel-early: 41.03565838820896 %\n","seek-attr-rel-early: 0.0 %\n","seek-attr-imm: 0.0 %\n","exist-obj-rel-imm: 25.986838715978866 %\n","exist-obj-rel-imm2: 0.0 %\n","count-all: 0.0 %\n","exist-attribute-group: 53.674478545396894 %\n","count-other: 7.156481422975503 %\n","exist-obj-exclude-imm: 59.990172428825204 %\n","exist-attribute: 34.85970070507797 %\n","exist-other: 54.08749626066237 %\n","count-obj-rel-imm: 12.440439887157414 %\n","count-attribute: 4.12810988523587 %\n","count-obj-exclude-imm: 16.481181221161727 %\n","count-obj-rel-imm2: 0.0 %\n","count-attribute-group: 5.952107014976189 %\n","\n"," --------------- Per question Category Acc --------------- \n","seek: 2.8364202942718895 %\n","count: 10.480194517613848 %\n","exist: 40.71534506628969 %\n","\n"," --------------- Done --------------- \n","\n"]}],"source":["exe = Execution(optsQ, optsC)#***\n","exe.run('test_with_gt')"]},{"cell_type":"raw","metadata":{"tags":[]},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4223935,"sourceId":7619305,"sourceType":"datasetVersion"},{"datasetId":4438180,"sourceId":7625887,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
