{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7618624,"sourceType":"datasetVersion","datasetId":4223935}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.collect()\n\n%reset -f","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:01.792306Z","iopub.execute_input":"2024-02-13T17:26:01.793158Z","iopub.status.idle":"2024-02-13T17:26:01.890420Z","shell.execute_reply.started":"2024-02-13T17:26:01.793113Z","shell.execute_reply":"2024-02-13T17:26:01.889451Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#for cleaning the GPU ram\nimport torch \ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:01.891983Z","iopub.execute_input":"2024-02-13T17:26:01.892292Z","iopub.status.idle":"2024-02-13T17:26:06.538865Z","shell.execute_reply.started":"2024-02-13T17:26:01.892250Z","shell.execute_reply":"2024-02-13T17:26:06.537845Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# path to notebook folder, use os.path.join to concat \nimport os\nROOT_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.540199Z","iopub.execute_input":"2024-02-13T17:26:06.540659Z","iopub.status.idle":"2024-02-13T17:26:06.545842Z","shell.execute_reply.started":"2024-02-13T17:26:06.540629Z","shell.execute_reply":"2024-02-13T17:26:06.544684Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **This notebook is created for the original NSVD project.** \n### There are 4 main folders in this project as following:\n1. Preprocess_dialogs \n2. program generator\n3. executor \n4. data foldes\nAmong these folders we are going to bring number 2 to 3. ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[]}},{"cell_type":"markdown","source":"## **Following codes belong to different .py files inside program_genarator**","metadata":{"execution":{"iopub.execute_input":"2023-12-21T13:17:18.103208Z","iopub.status.busy":"2023-12-21T13:17:18.102753Z","iopub.status.idle":"2023-12-21T13:17:18.110990Z","shell.execute_reply":"2023-12-21T13:17:18.109505Z","shell.execute_reply.started":"2023-12-21T13:17:18.103172Z"}}},{"cell_type":"markdown","source":"## **clevrDialog_dataset.py**","metadata":{}},{"cell_type":"code","source":"# /kaggle/input/nsvd-dataset/caption","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.549029Z","iopub.execute_input":"2024-02-13T17:26:06.549334Z","iopub.status.idle":"2024-02-13T17:26:06.559318Z","shell.execute_reply.started":"2024-02-13T17:26:06.549307Z","shell.execute_reply":"2024-02-13T17:26:06.558407Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport json\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.560745Z","iopub.execute_input":"2024-02-13T17:26:06.561104Z","iopub.status.idle":"2024-02-13T17:26:06.742576Z","shell.execute_reply.started":"2024-02-13T17:26:06.561073Z","shell.execute_reply":"2024-02-13T17:26:06.741775Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def invertDict(_dict):\n    return {v: k for k, v in _dict.items()}","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.744022Z","iopub.execute_input":"2024-02-13T17:26:06.744383Z","iopub.status.idle":"2024-02-13T17:26:06.749448Z","shell.execute_reply.started":"2024-02-13T17:26:06.744346Z","shell.execute_reply":"2024-02-13T17:26:06.748486Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ClevrDialogDataset(Dataset):\n    def __init__(self, dataPath, vocabPath, split, indStart=0, indEnd=-1):\n        super(ClevrDialogDataset, self).__init__()\n        self.data = h5py.File(dataPath, \"r\")\n        with open(vocabPath, \"r\") as f:\n            self.vocab = json.load(f)\n        self.vocab[\"idx_text_to_token\"] = invertDict(self.vocab[\"text_token_to_idx\"])\n        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n        self.vocab[\"idx_prog_to_token\"] = invertDict(self.vocab[\"prog_token_to_idx\"])\n        self.lenVocabText = len(self.vocab[\"text_token_to_idx\"])\n        self.lenVocabProg = len(self.vocab[\"prog_token_to_idx\"])\n\n        self.split = split\n        self.indStart = indStart\n        self.indEnd = indEnd\n        self.maxSamples = indEnd - indStart\n        self.maxLenProg = 6\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.750665Z","iopub.execute_input":"2024-02-13T17:26:06.750965Z","iopub.status.idle":"2024-02-13T17:26:06.761230Z","shell.execute_reply.started":"2024-02-13T17:26:06.750934Z","shell.execute_reply":"2024-02-13T17:26:06.760241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ClevrDialogCaptionDataset(ClevrDialogDataset):\n    def __init__(self, dataPath, vocabPath, split, name, indStart=0, indEnd=-1):\n        super(ClevrDialogCaptionDataset, self).__init__(dataPath, vocabPath, split, indStart=indStart, indEnd=indEnd)\n        self.captions = torch.LongTensor(np.asarray(self.data[\"captions\"], dtype=np.int64)[indStart: indEnd])\n        self.captionsPrgs = torch.LongTensor(np.asarray(self.data[\"captionProgs\"], dtype=np.int64)[indStart: indEnd])\n        self.name = name\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        assert idx < len(self)\n        caption = self.captions[idx][:16]\n        captionPrg = self.captionsPrgs[idx]\n        return caption, captionPrg","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.762538Z","iopub.execute_input":"2024-02-13T17:26:06.763336Z","iopub.status.idle":"2024-02-13T17:26:06.772171Z","shell.execute_reply.started":"2024-02-13T17:26:06.763308Z","shell.execute_reply":"2024-02-13T17:26:06.771119Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## **models.py**","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.773493Z","iopub.execute_input":"2024-02-13T17:26:06.773836Z","iopub.status.idle":"2024-02-13T17:26:06.788831Z","shell.execute_reply.started":"2024-02-13T17:26:06.773803Z","shell.execute_reply":"2024-02-13T17:26:06.787679Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class FC(nn.Module):\n    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n        super(FC, self).__init__()\n        self.dropout_r = dropout_r\n        self.use_relu = use_relu\n\n        self.linear = nn.Linear(in_size, out_size)\n\n        if use_relu:\n            self.relu = nn.ReLU(inplace=True)\n\n        if dropout_r > 0:\n            self.dropout = nn.Dropout(dropout_r)\n\n    def forward(self, x):\n        x = self.linear(x)\n\n        if self.use_relu:\n            x = self.relu(x)\n\n        if self.dropout_r > 0:\n            x = self.dropout(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.794234Z","iopub.execute_input":"2024-02-13T17:26:06.794592Z","iopub.status.idle":"2024-02-13T17:26:06.803382Z","shell.execute_reply.started":"2024-02-13T17:26:06.794556Z","shell.execute_reply":"2024-02-13T17:26:06.802330Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n        super(MLP, self).__init__()\n\n        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n        self.linear = nn.Linear(mid_size, out_size)\n\n    def forward(self, x):\n        return self.linear(self.fc(x))","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.804536Z","iopub.execute_input":"2024-02-13T17:26:06.804828Z","iopub.status.idle":"2024-02-13T17:26:06.815009Z","shell.execute_reply.started":"2024-02-13T17:26:06.804801Z","shell.execute_reply":"2024-02-13T17:26:06.813629Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, size, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.eps = eps\n\n        self.a_2 = nn.Parameter(torch.ones(size))\n        self.b_2 = nn.Parameter(torch.zeros(size))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.816754Z","iopub.execute_input":"2024-02-13T17:26:06.817182Z","iopub.status.idle":"2024-02-13T17:26:06.826497Z","shell.execute_reply.started":"2024-02-13T17:26:06.817148Z","shell.execute_reply":"2024-02-13T17:26:06.825374Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MHAtt(nn.Module):\n    def __init__(self, opts):\n        super(MHAtt, self).__init__()\n        self.opts = opts\n\n        self.linear_v = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_k = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_q = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n        self.linear_merge = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n\n        self.dropout = nn.Dropout(opts.dropout)\n\n    def forward(self, v, k, q, mask):\n        n_batches = q.size(0)\n\n        v = self.linear_v(v).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        k = self.linear_k(k).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        q = self.linear_q(q).view(\n            n_batches,\n            -1,\n            self.opts.multiHead,\n            self.opts.hiddenSizeHead\n        ).transpose(1, 2)\n\n        atted = self.att(v, k, q, mask)\n        atted = atted.transpose(1, 2).contiguous().view(\n            n_batches,\n            -1,\n            self.opts.hiddenDim\n        )\n\n        atted = self.linear_merge(atted)\n\n        return atted\n\n    def att(self, value, key, query, mask):\n        d_k = query.size(-1)\n\n        scores = torch.matmul(\n            query, key.transpose(-2, -1)\n        ) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask, -1e9)\n\n        att_map = F.softmax(scores, dim=-1)\n        att_map = self.dropout(att_map)\n\n        return torch.matmul(att_map, value)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.830665Z","iopub.execute_input":"2024-02-13T17:26:06.831097Z","iopub.status.idle":"2024-02-13T17:26:06.852436Z","shell.execute_reply.started":"2024-02-13T17:26:06.831058Z","shell.execute_reply":"2024-02-13T17:26:06.851309Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, opts):\n        super(FFN, self).__init__()\n\n        self.mlp = MLP(\n            in_size=opts.hiddenDim,\n            mid_size=opts.FeedForwardSize,\n            out_size=opts.hiddenDim,\n            dropout_r=opts.dropout,\n            use_relu=True\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.855680Z","iopub.execute_input":"2024-02-13T17:26:06.856333Z","iopub.status.idle":"2024-02-13T17:26:06.869229Z","shell.execute_reply.started":"2024-02-13T17:26:06.856300Z","shell.execute_reply":"2024-02-13T17:26:06.868132Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class SA(nn.Module):\n    def __init__(self, opts):\n        super(SA, self).__init__()\n        self.mhatt = MHAtt(opts)\n        self.ffn = FFN(opts)\n\n        self.dropout1 = nn.Dropout(opts.dropout)\n        self.norm1 = LayerNorm(opts.hiddenDim)\n\n        self.dropout2 = nn.Dropout(opts.dropout)\n        self.norm2 = LayerNorm(opts.hiddenDim)\n\n    def forward(self, x, x_mask):\n        x = self.norm1(x + self.dropout1(\n            self.mhatt(x, x, x, x_mask)\n        ))\n\n        x = self.norm2(x + self.dropout2(\n            self.ffn(x)\n        ))\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.870897Z","iopub.execute_input":"2024-02-13T17:26:06.871258Z","iopub.status.idle":"2024-02-13T17:26:06.880963Z","shell.execute_reply.started":"2024-02-13T17:26:06.871221Z","shell.execute_reply":"2024-02-13T17:26:06.879961Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class AttFlat(nn.Module):\n    def __init__(self, opts):\n        super(AttFlat, self).__init__()\n        self.opts = opts\n\n        self.mlp = MLP(\n            in_size=opts.hiddenDim,\n            mid_size=opts.FlatMLPSize,\n            out_size=opts.FlatGlimpses,\n            dropout_r=opts.dropout,\n            use_relu=True\n        )\n        # FLAT_GLIMPSES = 1\n        self.linear_merge = nn.Linear(\n            opts.hiddenDim * opts.FlatGlimpses,\n            opts.FlatOutSize\n        )\n\n    def forward(self, x, x_mask):\n        att = self.mlp(x)\n        att = att.masked_fill(\n            x_mask.squeeze(1).squeeze(1).unsqueeze(2),\n            -1e9\n        )\n        att = F.softmax(att, dim=1)\n\n        att_list = []\n        for i in range(self.opts.FlatGlimpses):\n            att_list.append(\n                torch.sum(att[:, :, i: i + 1] * x, dim=1)\n            )\n\n        x_atted = torch.cat(att_list, dim=1)\n        x_atted = self.linear_merge(x_atted)\n\n        return x_atted","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.882548Z","iopub.execute_input":"2024-02-13T17:26:06.883403Z","iopub.status.idle":"2024-02-13T17:26:06.895873Z","shell.execute_reply.started":"2024-02-13T17:26:06.883365Z","shell.execute_reply":"2024-02-13T17:26:06.894877Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CaptionEncoder(nn.Module):\n    def __init__(self, opts, textVocabSize):\n        super(CaptionEncoder, self).__init__()\n        self.embedding = nn.Embedding(textVocabSize, opts.embedDim)\n        bidirectional = opts.bidirectional > 0\n        self.lstmC = nn.LSTM(\n            input_size=opts.embedDim,\n            hidden_size=opts.hiddenDim,\n            num_layers=opts.numLayers,\n            batch_first=True,\n            bidirectional=bidirectional\n        )\n        if bidirectional:\n            opts.hiddenDim *= 2\n            opts.hiddenSizeHead *= 2\n            opts.FlatOutSize *= 2\n\n        self.attCap = nn.ModuleList([SA(opts) for _ in range(opts.layers)])\n        self.attFlatCap = AttFlat(opts)\n        self.fc = nn.Linear(opts.hiddenDim, opts.hiddenDim)\n\n    def forward(self, cap, hist=None):\n        capMask = self.make_mask(cap.unsqueeze(2))\n        cap = self.embedding(cap)\n        cap, (_, _) = self.lstmC(cap)\n        capO = cap.detach().clone()\n\n        for attC in self.attCap:\n            cap = attC(cap, capMask)\n        # (batchSize, 512)\n        cap = self.attFlatCap(cap, capMask)\n        encOut = self.fc(cap)\n        return encOut, capO\n    \n    # Masking\n    def make_mask(self, feature):\n        return (torch.sum(\n            torch.abs(feature),\n            dim=-1\n        ) == 0).unsqueeze(1).unsqueeze(2)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.897536Z","iopub.execute_input":"2024-02-13T17:26:06.898219Z","iopub.status.idle":"2024-02-13T17:26:06.911540Z","shell.execute_reply.started":"2024-02-13T17:26:06.898186Z","shell.execute_reply":"2024-02-13T17:26:06.910562Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from itertools import chain # needed for preprocessing the output of the local decode function -> added by Sepi","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.913175Z","iopub.execute_input":"2024-02-13T17:26:06.913509Z","iopub.status.idle":"2024-02-13T17:26:06.928039Z","shell.execute_reply.started":"2024-02-13T17:26:06.913467Z","shell.execute_reply":"2024-02-13T17:26:06.926933Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, opts, progVocabSize, maxLen, startID=1, endID=2):\n        super(Decoder, self).__init__()\n        self.numLayers = opts.numLayers\n        self.bidirectional = opts.bidirectional > 0\n        self.maxLen = maxLen\n        self.startID = startID\n        self.endID = endID\n\n        self.embedding = nn.Embedding(progVocabSize, opts.embedDim)\n        self.lstmProg = nn.LSTM(\n            input_size=opts.embedDim,\n            hidden_size=2*opts.hiddenDim if self.bidirectional else opts.hiddenDim,\n            num_layers=opts.numLayers,\n            batch_first=True,\n            # bidirectional=self.bidirectional,\n        )\n        hiddenDim = opts.hiddenDim\n        if self.bidirectional:\n            hiddenDim *= 2\n\n        self.fcAtt = nn.Linear(2*hiddenDim, hiddenDim)\n        self.fcOut = nn.Linear(hiddenDim, progVocabSize)\n\n    def initPrgHidden(self, encOut):\n        hidden = [encOut for _ in range(self.numLayers)]\n        hidden = torch.stack(hidden, 0).contiguous()\n        return hidden, hidden\n\n    def forwardStep(self, prog, progH, questO):\n        #**********************************************our error relates to this prog cause in our case it is not acting as tensor anymore.\n        batchSize = prog.size(0)\n        inputDim = questO.size(1)\n        prog = self.embedding(prog)\n        outProg, progH = self.lstmProg(prog, progH)\n\n        att = torch.bmm(outProg, questO.transpose(1, 2))\n        att = F.softmax(att.view(-1, inputDim), 1).view(batchSize, -1, inputDim)\n        context = torch.bmm(att, questO)\n        # (batchSize, progLength, hiddenDim)\n        out = F.tanh(self.fcAtt(torch.cat([outProg, context], dim=-1)))\n\n        # (batchSize, progLength, progVocabSize)\n        out = self.fcOut(out)\n        predSoftmax = F.log_softmax(out, 2)\n        return predSoftmax, progH\n\n    def forward(self, prog, encOut, questO):\n        progH = self.initPrgHidden(encOut)\n        predSoftmax, progH = self.forwardStep(prog, progH, questO)\n\n        return predSoftmax, progH\n\n    def sample(self, encOut, questO):\n        batchSize = encOut.size(0)\n        cudaFlag = encOut.is_cuda\n        progH = self.initPrgHidden(encOut)\n        # prog = progCopy[:, 0:3]\n        prog = torch.LongTensor(batchSize, 1).fill_(self.startID)\n        # prog = torch.cat((progStart, progEnd), -1)\n        if cudaFlag:\n            prog = prog.cuda()\n        outputLogProbs = []\n        outputTokens = []\n     \n\n        def decode(i, output):\n            tokens = output.topk(1, dim=-1)[1].view(batchSize, -1)\n            #print(\"This is inside of the decode local function and this is the tockens=\", tokens)\n            return tokens\n\n        for i in range(self.maxLen):\n            predSoftmax, progH = self.forwardStep(prog, progH, questO)\n            #print(\"inside of the for loop inside of the decoder.sample and it is iteration:\", i)\n           # print(\"-----------------------------------------\")\n            #print(\"predSoftmax which is the output of the forwardStep is equal to =\", predSoftmax)\n            #print(\"-----------------------------------------\")\n            prog = decode(i, predSoftmax)\n            prog_flat = list(chain(*prog))\n            flat_list = [item.item() for item in prog_flat]\n           # print(flat_list)\n           # print(\"-----------------------------------------\")\n           # print(\"The prog which is the output of the decode local functipon is equal to=\", flat_list)\n            #print(\"-----------------------------------------\")\n        #****************************************my modification\n            outputTokens.append(flat_list)#new\n       # print(\"lets check what is inside outputTocken\", outputTokens)    \n       # print(\"-----------------------------------------\")\n        return outputTokens, outputLogProbs\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.929738Z","iopub.execute_input":"2024-02-13T17:26:06.930784Z","iopub.status.idle":"2024-02-13T17:26:06.952663Z","shell.execute_reply.started":"2024-02-13T17:26:06.930744Z","shell.execute_reply":"2024-02-13T17:26:06.951534Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class SeqToSeqC(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(SeqToSeqC, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, cap, prog):\n        encOut, capO = self.encoder(cap)\n        predSoftmax, progHC = self.decoder(prog, encOut, capO)\n        return predSoftmax, progHC\n   \n    def sample(self, cap):\n        with torch.no_grad():\n            encOut, capO = self.encoder(cap)\n        outputTokens, outputLogProbs = self.decoder.sample(encOut, capO)\n\n        outputTokens_t = [[row[i] for row in outputTokens] for i in range(len(outputTokens[0]))]\n        #return outputTokens\n        return outputTokens_t","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.954070Z","iopub.execute_input":"2024-02-13T17:26:06.954401Z","iopub.status.idle":"2024-02-13T17:26:06.969239Z","shell.execute_reply.started":"2024-02-13T17:26:06.954375Z","shell.execute_reply":"2024-02-13T17:26:06.968046Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## **optim.py**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as Optim","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.970855Z","iopub.execute_input":"2024-02-13T17:26:06.971221Z","iopub.status.idle":"2024-02-13T17:26:06.983572Z","shell.execute_reply.started":"2024-02-13T17:26:06.971195Z","shell.execute_reply":"2024-02-13T17:26:06.982353Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class WarmupOptimizer(object):\n    def __init__(self, lr_base, optimizer, data_size, batch_size):\n        self.optimizer = optimizer\n        self._step = 0\n        self.lr_base = lr_base\n        self._rate = 0\n        self.data_size = data_size\n        self.batch_size = batch_size\n\n    def step(self):\n        self._step += 1\n\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n\n        self.optimizer.step()\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def rate(self, step=None):\n        if step is None:\n            step = self._step\n\n        if step <= int(self.data_size / self.batch_size * 1):\n            r = self.lr_base * 1/2.\n        else:\n            r = self.lr_base\n\n        return r\n\n\ndef get_optim(opts, model, data_size, lr_base=None):\n    if lr_base is None:\n        lr_base = opts.lr\n\n    if opts.optim == 'adam':\n        optim = Optim.Adam(\n                filter(lambda p: p.requires_grad, model.parameters()),\n                lr=0,\n                betas=opts.betas,\n                eps=opts.eps,\n\n            )\n    elif opts.optim == 'rmsprop':\n        optim = Optim.RMSprop(\n                filter(lambda p: p.requires_grad, model.parameters()),\n                lr=0,\n                eps=opts.eps,\n                weight_decay=opts.weight_decay\n            )\n    else:\n        raise ValueError('{} optimizer is not supported'.fromat(opts.optim))\n    return WarmupOptimizer(\n        lr_base,\n        optim,\n        data_size,\n        opts.batch_size\n    )\n\ndef adjust_lr(optim, decay_r):\n    optim.lr_base *= decay_r\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:06.984678Z","iopub.execute_input":"2024-02-13T17:26:06.985050Z","iopub.status.idle":"2024-02-13T17:26:07.003191Z","shell.execute_reply.started":"2024-02-13T17:26:06.985011Z","shell.execute_reply":"2024-02-13T17:26:07.002396Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## **option_caption_parser.py**","metadata":{}},{"cell_type":"code","source":"import argparse\nimport os\nimport torch\n#import utils_m","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:07.005249Z","iopub.execute_input":"2024-02-13T17:26:07.006678Z","iopub.status.idle":"2024-02-13T17:26:07.024783Z","shell.execute_reply.started":"2024-02-13T17:26:07.006645Z","shell.execute_reply":"2024-02-13T17:26:07.023598Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#TOTAL_ITER = 5000\nTOTAL_ITER = 5000\nVALID_EVE = 1000\n#VALID_EVE = 1000","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:07.026482Z","iopub.execute_input":"2024-02-13T17:26:07.026834Z","iopub.status.idle":"2024-02-13T17:26:07.036079Z","shell.execute_reply.started":"2024-02-13T17:26:07.026802Z","shell.execute_reply":"2024-02-13T17:26:07.034938Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Options_c():#changed optiopn class as Option_c to differentiate it with the one belong to question\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument(\n            '--mode',\n            default=\"train\",\n            # required=True,\n            type=str,\n            choices=['train', 'test'],\n            help='The mode of the experiment')\n\n        self.parser.add_argument(\n            '--run_dir',\n            default=\"kaggle/working\",\n            # required=True,\n            type=str,\n            help='The experiment directory')\n\n        self.parser.add_argument(\n            '--load_checkpoint_path',\n            default=None,\n            type=str,\n            help='The path the the pretrained CaptionNet')\n\n        self.parser.add_argument(\n            '--res_path',\n            default=\"kaggle/working/res.txt\",#***\n            # required=True,\n            type=str,\n            help='Path where to log the predicted caption programs')\n\n        self.parser.add_argument(\n            '--gpu_ids',\n            default='0',\n            type=str,\n            help='Id of the gpu to be used')\n\n        self.parser.add_argument(\n            '--seed',\n            default=42,\n            type=int,\n            help='The seed used in training')\n\n        self.parser.add_argument(\n            '--dataPathTr',\n            # default=\"caption_small/tr_cap_s.h5\", #***try with smaller dataset\n            # required=True,\n            default = '/kaggle/input/Small_Tr_Val_Test_Final/cap_tr_half.h5',\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed training data')\n\n        self.parser.add_argument(\n            '--dataPathVal',\n            #default=\"/kaggle/input/nsvd-dataset/caption/caption_val.h5\",\n            # default=\"caption_small/val_cap_s.h5\",#***try with smaller dataset\n            default = '/kaggle/input/Small_Tr_Val_Test_Final/cap_val_half.h5',\n            # required=True,\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed validation data')\n\n        self.parser.add_argument(\n            '--dataPathTest',\n            # required=True,\n            #default=\"/kaggle/input/nsvd-dataset/caption/caption_test.h5\",\n            # default=\"caption_small/test_cap_s.h5\",#***try with smaller dataset\n            default ='/kaggle/input/Small_Tr_Val_Test_Final/cap_test_75000.h5',\n            type=str,\n            help='Path to the h5 file of the Clevr-Dialog preprocessed test data')\n\n        self.parser.add_argument(\n            '--vocabPath',\n            # default=\"caption/vocab_output_caption.json\",\n            default = '/kaggle/input/caption/vocab_output_caption.json',\n\n            # required=True,\n            type=str,\n            help='Path to the generated vocabulary')\n\n        self.parser.add_argument(\n            '--batch_size',\n            default=64,\n            type=int,\n            help='Batch size')\n\n        self.parser.add_argument(\n            '--num_workers',\n            default=0,\n            type=int,\n            help='Number of workers for loading')\n\n        self.parser.add_argument(\n            '--num_iters',\n            #default=5000,\n            default=TOTAL_ITER,\n            type=int,\n            help='Total number of iterations')\n\n        self.parser.add_argument(\n            '--display_every',\n            default=5,\n            type=int,\n            help='Display training information every N iterations')\n\n        self.parser.add_argument(\n            '--debug_every',\n            default=100,\n            type=int,\n            help='Display debug message every N iterations')\n\n        self.parser.add_argument(\n            '--validate_every',\n            default=VALID_EVE,\n            type=int,\n            help='Validate every N iterations')\n\n        self.parser.add_argument(\n            '--shuffle_data',\n            default=1,\n            type=int,\n            help='Activate to shuffle the training data')\n\n        self.parser.add_argument(\n            '--optim',\n            default='adam',\n            type=str,\n            help='The name of the optimizer to be used')\n\n        self.parser.add_argument(\n            '--lr',\n            default=1e-3,\n            type=float,\n            help='Base learning rate')\n\n        self.parser.add_argument(\n            '--betas',\n            default='0.9, 0.98',\n            type=str,\n            help='Adam optimizer\\'s betas')\n\n        self.parser.add_argument(\n            '--eps',\n            default='1e-9',\n            type=float,\n            help='Adam optimizer\\'s epsilon')\n\n        self.parser.add_argument(\n            '--lr_decay_marks',\n            default='50000, 55000',\n            type=str,\n            help='Learing rate decay marks')\n\n        self.parser.add_argument(\n            '--lr_decay_factor',\n            default=0.5,\n            type=float,\n            help='Learning rate decay factor')\n\n        self.parser.add_argument(\n            '--weight_decay',\n            default=1e-6,\n            type=float,\n            help='Weight decay')\n\n        self.parser.add_argument(\n            '--embedDim',\n            default=300,\n            type=int,\n            help='Embedding dimension')\n\n        self.parser.add_argument(\n            '--hiddenDim',\n            default=512,\n            type=int,\n            help='LSTM hidden dimension')\n\n        self.parser.add_argument(\n            '--numLayers',\n            default=2,\n            type=int,\n            help='Number of hidden LSTM layers')\n\n        self.parser.add_argument(\n            '--dropout',\n            default=0.1,\n            type=float,\n            help='Dropout value')\n\n        self.parser.add_argument(\n            '--multiHead',\n            default=8,\n            type=int,\n            help='Number of attention heads')\n\n        self.parser.add_argument(\n            '--hiddenSizeHead',\n            default=64,\n            type=int,\n            help='Dimension of each attention head')\n\n        self.parser.add_argument(\n            '--FeedForwardSize',\n            default=2048,\n            type=int,\n            help='Dimension of the feed forward layer')\n\n        self.parser.add_argument(\n            '--FlatMLPSize',\n            default=512,\n            type=int,\n            help='MLP flatten size')\n\n        self.parser.add_argument(\n            '--FlatGlimpses',\n            default=1,\n            type=int,\n            help='Number of flatten glimpses')\n\n        self.parser.add_argument(\n            '--FlatOutSize',\n            default=512,\n            type=int,\n            help='Final attention reduction dimension')\n\n        self.parser.add_argument(\n            '--layers',\n            default=6,\n            type=int,\n            help='Number of self attention layers')\n\n        self.parser.add_argument(\n            '--bidirectional',\n            default=1,\n            type=int,\n            help='Activate to use bidirectional LSTMs')\n\n        self.initialized = True\n\n    def parse(self):\n        # initialize parser\n        if not self.initialized:\n            self.initialize()\n       # self.opts = self.parser.parse_args()\n        self.opts, unknown = self.parser.parse_known_args()#this is added by me to fix the error of command line arguments.\n\n        # parse gpu id list\n        str_gpu_ids = self.opts.gpu_ids.split(',')\n        self.opts.gpu_ids = []\n        for str_id in str_gpu_ids:\n            if str_id.isdigit() and int(str_id) >= 0:\n                self.opts.gpu_ids.append(int(str_id))\n        if len(self.opts.gpu_ids) > 0 and torch.cuda.is_available():\n            print('\\n[INFO] Using {} CUDA device(s) ...'.format(len(self.opts.gpu_ids)))\n        else:\n            print('\\n[INFO] Using cpu ...')\n            self.opts.gpu_ids = []\n\n        # parse the optimizer's betas and lr decay marks\n        self.opts.betas = [float(beta) for beta in self.opts.betas.split(',')]\n        lr_decay_marks = [int(m) for m in self.opts.lr_decay_marks.split(',')]\n        for i in range(1, len(lr_decay_marks)):\n            assert lr_decay_marks[i] > lr_decay_marks[i-1]\n        self.opts.lr_decay_marks = lr_decay_marks\n\n        # print and save options\n        args = vars(self.opts)\n        print('\\n ' + 30*'-' + 'Opts' + 30*'-')\n        for k, v in args.items():\n            print('%s: %s' % (str(k), str(v)))\n\n        if not os.path.isdir(self.opts.run_dir):\n            os.makedirs(self.opts.run_dir)\n        filename = 'opts_c.txt'\n        file_path = os.path.join(self.opts.run_dir, filename)\n        with open(file_path, 'wt') as fout:\n            fout.write('| options\\n')\n            for k, v in sorted(args.items()):\n                fout.write('%s: %s\\n' % (str(k), str(v)))\n        return self.opts\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:07.037768Z","iopub.execute_input":"2024-02-13T17:26:07.039784Z","iopub.status.idle":"2024-02-13T17:26:07.082283Z","shell.execute_reply.started":"2024-02-13T17:26:07.039747Z","shell.execute_reply":"2024-02-13T17:26:07.081316Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## **train_caption_parser.py**","metadata":{}},{"cell_type":"code","source":"#from clevrDialog_dataset import ClevrDialogCaptionDataset\n#from models import SeqToSeqC, CaptionEncoder, Decoder\n#from optim import get_optim, adjust_lr\n#from options_caption_parser import Options\nimport os, json, torch, pickle, copy, time\nimport numpy as np\nimport torch.nn as nn\nimport torch.utils.data as Data\nfrom tensorboardX import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:07.083539Z","iopub.execute_input":"2024-02-13T17:26:07.084163Z","iopub.status.idle":"2024-02-13T17:26:13.534746Z","shell.execute_reply.started":"2024-02-13T17:26:07.084135Z","shell.execute_reply":"2024-02-13T17:26:13.533638Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import sys\nclass Execution:\n    def __init__(self, opts):\n        self.opts = opts\n\n        self.loss_fn = torch.nn.NLLLoss().cuda()\n        print(\"[INFO] Loading dataset ...\")\n\n        self.dataset_tr = ClevrDialogCaptionDataset(\n            opts.dataPathTr, opts.vocabPath, \"train\", \"Captions Tr\")\n\n        self.dataset_val = ClevrDialogCaptionDataset(\n            opts.dataPathVal, opts.vocabPath, \"val\", \"Captions Val\")\n\n        self.dataset_test = ClevrDialogCaptionDataset(\n           opts.dataPathTest, opts.vocabPath, \"test\", \"Captions Test\")\n\n        tb_path = os.path.join(opts.run_dir, \"tb_logdir\")\n        if not os.path.isdir(tb_path):\n            os.makedirs(tb_path)\n\n        self.ckpt_path = os.path.join(opts.run_dir, \"ckpt_dir\")\n        if not os.path.isdir(self.ckpt_path):\n            os.makedirs(self.ckpt_path)\n\n        self.writer = SummaryWriter(tb_path)\n        self.iter_val = 0\n        self.bestValAcc = float(\"-inf\")\n        self.bestValIter = -1\n\n    def constructNet(self, lenVocabText, lenVocabProg, maxLenProg, ):\n        decoder = Decoder(self.opts, lenVocabProg, maxLenProg)\n        encoder = CaptionEncoder(self.opts, lenVocabText)\n        net = SeqToSeqC(encoder, decoder)\n        return net\n\n    def train(self, dataset, dataset_val=None):\n        # Obtain needed information\n        lenVocabText = dataset.lenVocabText\n        lenVocabProg = dataset.lenVocabProg\n        maxLenProg = dataset.maxLenProg\n        net = self.constructNet(lenVocabText, lenVocabProg, maxLenProg)\n\n        net.cuda()\n        net.train()\n\n        # Define the multi-gpu training if needed\n        if len(self.opts.gpu_ids) > 1:\n            net = nn.DataParallel(net, device_ids=self.opts.gpu_ids)\n\n        # Load checkpoint if resume training\n        if self.opts.load_checkpoint_path is not None:\n            print(\"[INFO] Resume trainig from ckpt {} ...\".format(\n                self.opts.load_checkpoint_path\n            ))\n\n            # Load the network parameters\n            ckpt = torch.load(self.opts.load_checkpoint_path)\n            print(\"[INFO] Checkpoint successfully loaded ...\")\n            net.load_state_dict(ckpt['state_dict'])\n\n            # Load the optimizer paramters\n            optim = get_optim(self.opts, net, len(dataset), lr_base=ckpt['lr_base'])\n            optim.optimizer.load_state_dict(ckpt['optimizer'])\n\n        else:\n            optim = get_optim(self.opts, net, len(dataset))\n        _iter = 0\n        epoch = 0\n\n        # Define dataloader\n        dataloader = Data.DataLoader(\n            dataset,\n            batch_size=self.opts.batch_size,\n            shuffle=self.opts.shuffle_data,\n            num_workers=self.opts.num_workers,\n        )\n        _iterCur = 0\n        _totalCur = len(dataloader)\n        # Training loop\n        while _iter < self.opts.num_iters:\n            # Learning Rate Decay\n            if _iter in self.opts.lr_decay_marks:\n                adjust_lr(optim, self.opts.lr_decay_factor)\n\n            time_start = time.time()\n            # Iteration\n            for caption, captionPrg in dataloader:\n                if _iter >= self.opts.num_iters:\n                    break\n                caption = caption.cuda()\n                captionPrg = captionPrg.cuda()\n                captionPrgTarget = captionPrg.clone()\n                optim.zero_grad()\n\n                predSoftmax, _ = net(caption, captionPrg)\n\n                loss = self.loss_fn(\n                    predSoftmax[:, :-1, :].contiguous().view(-1, predSoftmax.size(2)),\n                    captionPrgTarget[:, 1:].contiguous().view(-1))\n                loss.backward()\n\n                # logging\n                self.writer.add_scalar(\n                    'train/loss',\n                    loss.cpu().data.numpy(),\n                    global_step=_iter)\n\n                self.writer.add_scalar(\n                    'train/lr',\n                    optim._rate,\n                    global_step=_iter)\n                if _iter % self.opts.display_every == 0:\n                    print(\"\\r[CLEVR-Dialog - %s (%d/%4d)][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" % (\n                            dataset.name,\n                            _iterCur,\n                            _totalCur,\n                            epoch,\n                            _iter,\n                            self.opts.num_iters,\n                            loss.cpu().data.numpy(),\n                            optim._rate,\n                        ), end='          ')\n                optim.step()\n                _iter += 1\n                _iterCur += 1\n\n                if _iter % self.opts.validate_every == 0:\n                    if dataset_val is not None:\n                        valAcc = self.eval(\n                            net,\n                            dataset_val,\n                            valid=True,\n                        )\n                        if valAcc > self.bestValAcc:\n                            self.bestValAcc = valAcc\n                            self.bestValIter = _iter\n\n                            print(\"[INFO] Checkpointing model @ iter {}\".format(_iter))\n                            state = {\n                                'state_dict': net.state_dict(),\n                                'optimizer': optim.optimizer.state_dict(),\n                                'lr_base': optim.lr_base,\n                                'optim': optim.lr_base,\n                                'last_iter': _iter,\n                                'last_epoch': epoch,\n                            }\n                            # checkpointing\n                            torch.save(\n                                state,\n                                os.path.join(self.ckpt_path, 'ckpt_iter' + str(_iter) + '.pkl')\n                            )\n                    else:\n                        print(\"[INFO] No validation dataset available\")\n\n            time_end = time.time()\n            print('Finished epoch in {}s'.format(int(time_end-time_start)))\n            epoch += 1\n\n        print(\"[INFO] Training done. Best model had val acc. {} @ iter {}...\".format(self.bestValAcc, self.bestValIter))\n\n    # Evaluation\n    def eval(self, net, dataset, valid=False):\n        net = net.eval()\n        data_size = len(dataset)\n        dataloader = Data.DataLoader(\n            dataset,\n            batch_size=self.opts.batch_size,\n            shuffle=False,\n            num_workers=self.opts.num_workers,\n            pin_memory=False\n        )\n        allPredictedProgs = []\n        numAllProg = 0\n        falsePred = 0\n        for step, (caption, captionPrg) in enumerate(dataloader):\n            print(\"\\rEvaluation: [step %4d/%4d]\" % (\n                step,\n                int(data_size / self.opts.batch_size),\n            ), end='          ')\n            sys.stdout.flush()#my shit***************************\n            caption = caption.cuda()\n            captionPrg = captionPrg.cuda()\n          \n            tokens = net.sample(caption)\n            targetProgs = decodeProg(captionPrg, dataset.vocab[\"idx_prog_to_token\"], target=True)\n            predProgs = decodeProg(tokens, dataset.vocab[\"idx_prog_to_token\"])\n            predProgs = [sublist for sublist in predProgs if sublist]\n            allPredictedProgs.extend(list(map(lambda s: \"( {} ( {} ) ) \\n\".format(s[0], \", \".join(s[1:])), predProgs)))\n                              \n                                \n            numAllProg += len(targetProgs)\n            for targetProg, predProg in zip(targetProgs, predProgs):\n                mainMod = targetProg[0] == predProg[0]\n                sameLength = len(targetProg) == len(predProg)\n                sameArgs = False\n                if sameLength:\n                    sameArgs = True\n                    for argTarget in targetProg[1:]:\n                        if argTarget not in predProg[1:]:\n                            sameArgs = False\n                            break\n\n                if not (mainMod and sameArgs):\n                    falsePred += 1\n        val_acc = (1 - (falsePred / numAllProg)) * 100.0\n        print(\"Acc: {}\".format(val_acc))\n        net = net.train()\n        if not valid:\n            with open(self.opts.res_path, \"w\") as f:\n                f.writelines(allPredictedProgs)\n            print(\"[INFO] Predicted caption programs logged into {}\".format(self.opts.res_path))\n        return val_acc\n\n    def run(self, run_mode):\n        self.set_seed(self.opts.seed)\n        if run_mode == 'train':\n            self.train(self.dataset_tr, self.dataset_val)\n\n        elif run_mode == 'test':\n            lenVocabText = self.dataset_test.lenVocabText\n            lenVocabProg = self.dataset_test.lenVocabProg\n            maxLenProg = self.dataset_test.maxLenProg\n            net = self.constructNet(lenVocabText, lenVocabProg, maxLenProg)\n\n            print('Loading ckpt {}'.format(self.opts.load_checkpoint_path))\n            state_dict = torch.load(self.opts.load_checkpoint_path)['state_dict']\n            net.load_state_dict(state_dict)\n            net.cuda()\n            self.eval(net, self.dataset_test)\n\n        else:\n            exit(-1)\n\n    def set_seed(self, seed):\n        \"\"\"Sets the seed for reproducibility.\n        Args:\n            seed (int): The seed used\n        \"\"\"\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        np.random.seed(seed)\n        print('[INFO] Seed set to {}...'.format(seed))\n\n\ndef decodeProg(tokens, prgIdxToToken, target=False):\n    \n    if (target == True):\n        tokensBatch = tokens.tolist()\n    else:\n        tokensBatch = tokens\n    #print(\"want to see what happens to tokens in decodeProg\", tokensBatch)\n    progsBatch = []\n    for tokens in tokensBatch:\n        #print(\"tokens inside the first for loop in decodeProg\", tokens)\n        prog = []\n        for tok in tokens:\n            if tok == 2:  # <END> has index 2\n                break\n            \n            prog.append(prgIdxToToken.get(tok))\n          \n           \n        if target:\n            #print(\"tuye if\")\n            prog = prog[1:]\n        progsBatch.append(prog)\n    return progsBatch\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:13.536407Z","iopub.execute_input":"2024-02-13T17:26:13.537015Z","iopub.status.idle":"2024-02-13T17:26:13.582030Z","shell.execute_reply.started":"2024-02-13T17:26:13.536981Z","shell.execute_reply":"2024-02-13T17:26:13.580948Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.execute_input":"2023-12-26T15:48:45.283904Z","iopub.status.busy":"2023-12-26T15:48:45.283515Z","iopub.status.idle":"2023-12-26T15:48:45.292927Z","shell.execute_reply":"2023-12-26T15:48:45.291671Z","shell.execute_reply.started":"2023-12-26T15:48:45.283874Z"}}},{"cell_type":"code","source":"##### #__name__ == \"__main__\":\nopts = Options_c().parse()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:13.587917Z","iopub.execute_input":"2024-02-13T17:26:13.588631Z","iopub.status.idle":"2024-02-13T17:26:13.651040Z","shell.execute_reply.started":"2024-02-13T17:26:13.588594Z","shell.execute_reply":"2024-02-13T17:26:13.649963Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\n[INFO] Using 1 CUDA device(s) ...\n\n ------------------------------Opts------------------------------\nmode: train\nrun_dir: kaggle/working\nload_checkpoint_path: None\nres_path: kaggle/working/res.txt\ngpu_ids: [0]\nseed: 42\ndataPathTr: /kaggle/input/Small_Tr_Val_Test_Final/cap_tr_half.h5\ndataPathVal: /kaggle/input/Small_Tr_Val_Test_Final/cap_val_half.h5\ndataPathTest: /kaggle/input/Small_Tr_Val_Test_Final/cap_test_75000.h5\nvocabPath: /kaggle/input/caption/vocab_output_caption.json\nbatch_size: 64\nnum_workers: 0\nnum_iters: 5000\ndisplay_every: 5\ndebug_every: 100\nvalidate_every: 1000\nshuffle_data: 1\noptim: adam\nlr: 0.001\nbetas: [0.9, 0.98]\neps: 1e-09\nlr_decay_marks: [50000, 55000]\nlr_decay_factor: 0.5\nweight_decay: 1e-06\nembedDim: 300\nhiddenDim: 512\nnumLayers: 2\ndropout: 0.1\nmultiHead: 8\nhiddenSizeHead: 64\nFeedForwardSize: 2048\nFlatMLPSize: 512\nFlatGlimpses: 1\nFlatOutSize: 512\nlayers: 6\nbidirectional: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"exe = Execution(opts)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:13.652283Z","iopub.execute_input":"2024-02-13T17:26:13.652642Z","iopub.status.idle":"2024-02-13T17:26:13.987182Z","shell.execute_reply.started":"2024-02-13T17:26:13.652606Z","shell.execute_reply":"2024-02-13T17:26:13.985945Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[INFO] Loading dataset ...\n","output_type":"stream"}]},{"cell_type":"code","source":"exe.run(opts.mode)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:26:13.988403Z","iopub.execute_input":"2024-02-13T17:26:13.988695Z","iopub.status.idle":"2024-02-13T17:50:55.027291Z","shell.execute_reply.started":"2024-02-13T17:26:13.988670Z","shell.execute_reply":"2024-02-13T17:50:55.026136Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[INFO] Seed set to 42...\nEvaluation: [step    1/   1]          ][epoch  0][iter  995/5000] loss: 0.5769, lr: 5.00e-04          Acc: 75.0\n[INFO] Checkpointing model @ iter 1000\nEvaluation: [step    1/   1]          )][epoch  0][iter 1995/5000] loss: 0.5720, lr: 5.00e-04          Acc: 75.80645161290323\n[INFO] Checkpointing model @ iter 2000\n[CLEVR-Dialog - Captions Tr (2730/2733)][epoch  0][iter 2730/5000] loss: 0.5663, lr: 5.00e-04          Finished epoch in 805s\nEvaluation: [step    1/   1]          )][epoch  1][iter 2995/5000] loss: 0.5704, lr: 1.00e-03          Acc: 72.58064516129032\nEvaluation: [step    1/   1]          )][epoch  1][iter 3995/5000] loss: 0.5614, lr: 1.00e-03          Acc: 75.0\nEvaluation: [step    1/   1]          )][epoch  1][iter 4995/5000] loss: 0.5499, lr: 1.00e-03          Acc: 75.80645161290323\nFinished epoch in 671s\n[INFO] Training done. Best model had val acc. 75.80645161290323 @ iter 2000...\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"[INFO] Done ...\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T17:50:55.028815Z","iopub.execute_input":"2024-02-13T17:50:55.029371Z","iopub.status.idle":"2024-02-13T17:50:55.035083Z","shell.execute_reply.started":"2024-02-13T17:50:55.029342Z","shell.execute_reply":"2024-02-13T17:50:55.034095Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[INFO] Done ...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}